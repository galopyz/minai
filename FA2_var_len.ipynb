{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954f5b46",
   "metadata": {},
   "source": [
    "# How does FA2 `flash_attn_varlen_func` handle masks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759f3e4",
   "metadata": {},
   "source": [
    "In this notebook, we want to know whether packed sequences passed to `flash_attn_varlen_func` are independent from each other. Confirming the issue: https://github.com/Dao-AILab/flash-attention/issues/654."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2edb84a",
   "metadata": {},
   "source": [
    "If masks are applied correctly, we would get the same result whether we use a packed sequence with `flash_attn_varlen_func` or use multiple sequences with `flash_attn_func`. We won't use any padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ad8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *\n",
    "from fastcore.all import *\n",
    "\n",
    "import torch\n",
    "from flash_attn import flash_attn_func, flash_attn_varlen_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d12a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df39e0f",
   "metadata": {},
   "source": [
    "Here are the doc strings for `flash_attn_func` and `flash_attn_varlen_func`. What are the differences? \n",
    "\n",
    "1. **Sequence handling**:\n",
    "   - `flash_attn_func`: Processes regular batched sequences where each sequence has the same length (padded if necessary)\n",
    "   - `flash_attn_varlen_func`: Handles variable-length sequences packed into a single tensor, avoiding computation on padding tokens\n",
    "\n",
    "2. **Input format**:\n",
    "   - `flash_attn_func`: Takes inputs in shape [batch_size, seq_len, num_heads, head_dim]\n",
    "   - `flash_attn_varlen_func`: Takes inputs in shape [total_tokens, num_heads, head_dim] where total_tokens is the sum of all sequence lengths\n",
    "\n",
    "3. **Additional parameters for `flash_attn_varlen_func`**:\n",
    "   - Requires `cu_seqlens` (cumulative sequence lengths) to mark sequence boundaries\n",
    "   - Requires `max_seqlen` to know the maximum sequence length in the batch\n",
    "\n",
    "4. **Efficiency**:\n",
    "   - `flash_attn_varlen_func` is more memory-efficient for batches with varying sequence lengths\n",
    "   - `flash_attn_func` is simpler to use when all sequences have the same length\n",
    "\n",
    "5. **Use cases**:\n",
    "   - `flash_attn_func`: Better for training with fixed sequence lengths or when using padding\n",
    "   - `flash_attn_varlen_func`: Better for inference or when handling many sequences of different lengths\n",
    "\n",
    "Both functions implement the same core attention algorithm with the same optimizations, but `flash_attn_varlen_func` adds the ability to handle variable-length sequences more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414f5192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### flash_attn_func\n",
       "\n",
       ">      flash_attn_func (q, k, v, dropout_p=0.0, softmax_scale=None,\n",
       ">                       causal=False, window_size=(-1, -1), softcap=0.0,\n",
       ">                       alibi_slopes=None, deterministic=False,\n",
       ">                       return_attn_probs=False)\n",
       "\n",
       "*dropout_p should be set to 0.0 during evaluation\n",
       "Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\n",
       "than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n",
       "For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n",
       "0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n",
       "\n",
       "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
       "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
       "    1 1 1 1 0\n",
       "    1 1 1 1 1\n",
       "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
       "    0 0\n",
       "    0 0\n",
       "    0 0\n",
       "    1 0\n",
       "    1 1\n",
       "If the row of the mask is all zero, the output will be zero.\n",
       "\n",
       "If window_size != (-1, -1), implements sliding window local attention. Query at position i\n",
       "will only attend to keys between\n",
       "[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n",
       "\n",
       "Arguments:\n",
       "    q: (batch_size, seqlen, nheads, headdim)\n",
       "    k: (batch_size, seqlen, nheads_k, headdim)\n",
       "    v: (batch_size, seqlen, nheads_k, headdim)\n",
       "    dropout_p: float. Dropout probability.\n",
       "    softmax_scale: float. The scaling of QK^T before applying softmax.\n",
       "        Default to 1 / sqrt(headdim).\n",
       "    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n",
       "    window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n",
       "    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n",
       "        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n",
       "        is added to the attention score of query i and key j.\n",
       "    deterministic: bool. Whether to use the deterministic implementation of the backward pass,\n",
       "        which is slightly slower and uses more memory. The forward pass is always deterministic.\n",
       "    return_attn_probs: bool. Whether to return the attention probabilities. This option is for\n",
       "       testing only. The returned probabilities are not guaranteed to be correct\n",
       "       (they might not have the right scaling).\n",
       "Return:\n",
       "    out: (batch_size, seqlen, nheads, headdim).\n",
       "    softmax_lse [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen). The\n",
       "        logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax\n",
       "        normalization factor).\n",
       "    S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).\n",
       "        The output of softmax (possibly with different scaling). It also encodes the dropout\n",
       "        pattern (negative means that location was dropped, nonnegative means it was kept).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| q |  |  |  |\n",
       "| k |  |  |  |\n",
       "| v |  |  |  |\n",
       "| dropout_p | float | 0.0 |  |\n",
       "| softmax_scale | NoneType | None |  |\n",
       "| causal | bool | False |  |\n",
       "| window_size | tuple | (-1, -1) | -1 means infinite context window |\n",
       "| softcap | float | 0.0 | 0.0 means deactivated |\n",
       "| alibi_slopes | NoneType | None |  |\n",
       "| deterministic | bool | False |  |\n",
       "| return_attn_probs | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### flash_attn_func\n",
       "\n",
       ">      flash_attn_func (q, k, v, dropout_p=0.0, softmax_scale=None,\n",
       ">                       causal=False, window_size=(-1, -1), softcap=0.0,\n",
       ">                       alibi_slopes=None, deterministic=False,\n",
       ">                       return_attn_probs=False)\n",
       "\n",
       "*dropout_p should be set to 0.0 during evaluation\n",
       "Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\n",
       "than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n",
       "For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n",
       "0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n",
       "\n",
       "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
       "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
       "    1 1 1 1 0\n",
       "    1 1 1 1 1\n",
       "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
       "    0 0\n",
       "    0 0\n",
       "    0 0\n",
       "    1 0\n",
       "    1 1\n",
       "If the row of the mask is all zero, the output will be zero.\n",
       "\n",
       "If window_size != (-1, -1), implements sliding window local attention. Query at position i\n",
       "will only attend to keys between\n",
       "[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n",
       "\n",
       "Arguments:\n",
       "    q: (batch_size, seqlen, nheads, headdim)\n",
       "    k: (batch_size, seqlen, nheads_k, headdim)\n",
       "    v: (batch_size, seqlen, nheads_k, headdim)\n",
       "    dropout_p: float. Dropout probability.\n",
       "    softmax_scale: float. The scaling of QK^T before applying softmax.\n",
       "        Default to 1 / sqrt(headdim).\n",
       "    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n",
       "    window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n",
       "    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n",
       "        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n",
       "        is added to the attention score of query i and key j.\n",
       "    deterministic: bool. Whether to use the deterministic implementation of the backward pass,\n",
       "        which is slightly slower and uses more memory. The forward pass is always deterministic.\n",
       "    return_attn_probs: bool. Whether to return the attention probabilities. This option is for\n",
       "       testing only. The returned probabilities are not guaranteed to be correct\n",
       "       (they might not have the right scaling).\n",
       "Return:\n",
       "    out: (batch_size, seqlen, nheads, headdim).\n",
       "    softmax_lse [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen). The\n",
       "        logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax\n",
       "        normalization factor).\n",
       "    S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).\n",
       "        The output of softmax (possibly with different scaling). It also encodes the dropout\n",
       "        pattern (negative means that location was dropped, nonnegative means it was kept).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| q |  |  |  |\n",
       "| k |  |  |  |\n",
       "| v |  |  |  |\n",
       "| dropout_p | float | 0.0 |  |\n",
       "| softmax_scale | NoneType | None |  |\n",
       "| causal | bool | False |  |\n",
       "| window_size | tuple | (-1, -1) | -1 means infinite context window |\n",
       "| softcap | float | 0.0 | 0.0 means deactivated |\n",
       "| alibi_slopes | NoneType | None |  |\n",
       "| deterministic | bool | False |  |\n",
       "| return_attn_probs | bool | False |  |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(flash_attn_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d582180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### flash_attn_varlen_func\n",
       "\n",
       ">      flash_attn_varlen_func (q, k, v, cu_seqlens_q, cu_seqlens_k,\n",
       ">                              max_seqlen_q, max_seqlen_k, dropout_p=0.0,\n",
       ">                              softmax_scale=None, causal=False,\n",
       ">                              window_size=(-1, -1), softcap=0.0,\n",
       ">                              alibi_slopes=None, deterministic=False,\n",
       ">                              return_attn_probs=False, block_table=None)\n",
       "\n",
       "*dropout_p should be set to 0.0 during evaluation\n",
       "Supports multi-query and grouped-query attention (MQA/GQA) by passing in K, V with fewer heads\n",
       "than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n",
       "For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n",
       "0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n",
       "\n",
       "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
       "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
       "    1 1 1 1 0\n",
       "    1 1 1 1 1\n",
       "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
       "    0 0\n",
       "    0 0\n",
       "    0 0\n",
       "    1 0\n",
       "    1 1\n",
       "If the row of the mask is all zero, the output will be zero.\n",
       "\n",
       "If window_size != (-1, -1), implements sliding window local attention. Query at position i\n",
       "will only attend to keys between\n",
       "[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n",
       "\n",
       "Arguments:\n",
       "    q: (total_q, nheads, headdim), where total_q = total number of query tokens in the batch.\n",
       "    k: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.\n",
       "    v: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.\n",
       "    cu_seqlens_q: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
       "       of the sequences in the batch, used to index into q.\n",
       "    cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
       "       of the sequences in the batch, used to index into kv.\n",
       "    max_seqlen_q: int. Maximum query sequence length in the batch.\n",
       "    max_seqlen_k: int. Maximum key sequence length in the batch.\n",
       "    dropout_p: float. Dropout probability.\n",
       "    softmax_scale: float. The scaling of QK^T before applying softmax.\n",
       "        Default to 1 / sqrt(headdim).\n",
       "    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n",
       "    window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n",
       "    softcap: float. Anything > 0 activates softcapping attention.\n",
       "    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n",
       "        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n",
       "        is added to the attention score of query i and key j.\n",
       "    deterministic: bool. Whether to use the deterministic implementation of the backward pass,\n",
       "        which is slightly slower and uses more memory. The forward pass is always deterministic.\n",
       "    return_attn_probs: bool. Whether to return the attention probabilities. This option is for\n",
       "       testing only. The returned probabilities are not guaranteed to be correct\n",
       "       (they might not have the right scaling).\n",
       "Return:\n",
       "    out: (total, nheads, headdim).\n",
       "    softmax_lse [optional, if return_attn_probs=True]: (nheads, total_q_seqlen). The\n",
       "        logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax\n",
       "        normalization factor).\n",
       "    S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).\n",
       "        The output of softmax (possibly with different scaling). It also encodes the dropout\n",
       "        pattern (negative means that location was dropped, nonnegative means it was kept).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| q |  |  |  |\n",
       "| k |  |  |  |\n",
       "| v |  |  |  |\n",
       "| cu_seqlens_q |  |  |  |\n",
       "| cu_seqlens_k |  |  |  |\n",
       "| max_seqlen_q |  |  |  |\n",
       "| max_seqlen_k |  |  |  |\n",
       "| dropout_p | float | 0.0 |  |\n",
       "| softmax_scale | NoneType | None |  |\n",
       "| causal | bool | False |  |\n",
       "| window_size | tuple | (-1, -1) | -1 means infinite context window |\n",
       "| softcap | float | 0.0 | 0.0 means deactivated |\n",
       "| alibi_slopes | NoneType | None |  |\n",
       "| deterministic | bool | False |  |\n",
       "| return_attn_probs | bool | False |  |\n",
       "| block_table | NoneType | None |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### flash_attn_varlen_func\n",
       "\n",
       ">      flash_attn_varlen_func (q, k, v, cu_seqlens_q, cu_seqlens_k,\n",
       ">                              max_seqlen_q, max_seqlen_k, dropout_p=0.0,\n",
       ">                              softmax_scale=None, causal=False,\n",
       ">                              window_size=(-1, -1), softcap=0.0,\n",
       ">                              alibi_slopes=None, deterministic=False,\n",
       ">                              return_attn_probs=False, block_table=None)\n",
       "\n",
       "*dropout_p should be set to 0.0 during evaluation\n",
       "Supports multi-query and grouped-query attention (MQA/GQA) by passing in K, V with fewer heads\n",
       "than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\n",
       "For example, if Q has 6 heads and K, V have 2 heads, head 0, 1, 2 of Q will attention to head\n",
       "0 of K, V, and head 3, 4, 5 of Q will attention to head 1 of K, V.\n",
       "\n",
       "If causal=True, the causal mask is aligned to the bottom right corner of the attention matrix.\n",
       "For example, if seqlen_q = 2 and seqlen_k = 5, the causal mask (1 = keep, 0 = masked out) is:\n",
       "    1 1 1 1 0\n",
       "    1 1 1 1 1\n",
       "If seqlen_q = 5 and seqlen_k = 2, the causal mask is:\n",
       "    0 0\n",
       "    0 0\n",
       "    0 0\n",
       "    1 0\n",
       "    1 1\n",
       "If the row of the mask is all zero, the output will be zero.\n",
       "\n",
       "If window_size != (-1, -1), implements sliding window local attention. Query at position i\n",
       "will only attend to keys between\n",
       "[i + seqlen_k - seqlen_q - window_size[0], i + seqlen_k - seqlen_q + window_size[1]] inclusive.\n",
       "\n",
       "Arguments:\n",
       "    q: (total_q, nheads, headdim), where total_q = total number of query tokens in the batch.\n",
       "    k: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.\n",
       "    v: (total_k, nheads_k, headdim), where total_k = total number of key tokens in the batch.\n",
       "    cu_seqlens_q: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
       "       of the sequences in the batch, used to index into q.\n",
       "    cu_seqlens_k: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
       "       of the sequences in the batch, used to index into kv.\n",
       "    max_seqlen_q: int. Maximum query sequence length in the batch.\n",
       "    max_seqlen_k: int. Maximum key sequence length in the batch.\n",
       "    dropout_p: float. Dropout probability.\n",
       "    softmax_scale: float. The scaling of QK^T before applying softmax.\n",
       "        Default to 1 / sqrt(headdim).\n",
       "    causal: bool. Whether to apply causal attention mask (e.g., for auto-regressive modeling).\n",
       "    window_size: (left, right). If not (-1, -1), implements sliding window local attention.\n",
       "    softcap: float. Anything > 0 activates softcapping attention.\n",
       "    alibi_slopes: (nheads,) or (batch_size, nheads), fp32. A bias of\n",
       "        (-alibi_slope * |i + seqlen_k - seqlen_q - j|)\n",
       "        is added to the attention score of query i and key j.\n",
       "    deterministic: bool. Whether to use the deterministic implementation of the backward pass,\n",
       "        which is slightly slower and uses more memory. The forward pass is always deterministic.\n",
       "    return_attn_probs: bool. Whether to return the attention probabilities. This option is for\n",
       "       testing only. The returned probabilities are not guaranteed to be correct\n",
       "       (they might not have the right scaling).\n",
       "Return:\n",
       "    out: (total, nheads, headdim).\n",
       "    softmax_lse [optional, if return_attn_probs=True]: (nheads, total_q_seqlen). The\n",
       "        logsumexp of each row of the matrix QK^T * scaling (e.g., log of the softmax\n",
       "        normalization factor).\n",
       "    S_dmask [optional, if return_attn_probs=True]: (batch_size, nheads, seqlen, seqlen).\n",
       "        The output of softmax (possibly with different scaling). It also encodes the dropout\n",
       "        pattern (negative means that location was dropped, nonnegative means it was kept).*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| q |  |  |  |\n",
       "| k |  |  |  |\n",
       "| v |  |  |  |\n",
       "| cu_seqlens_q |  |  |  |\n",
       "| cu_seqlens_k |  |  |  |\n",
       "| max_seqlen_q |  |  |  |\n",
       "| max_seqlen_k |  |  |  |\n",
       "| dropout_p | float | 0.0 |  |\n",
       "| softmax_scale | NoneType | None |  |\n",
       "| causal | bool | False |  |\n",
       "| window_size | tuple | (-1, -1) | -1 means infinite context window |\n",
       "| softcap | float | 0.0 | 0.0 means deactivated |\n",
       "| alibi_slopes | NoneType | None |  |\n",
       "| deterministic | bool | False |  |\n",
       "| return_attn_probs | bool | False |  |\n",
       "| block_table | NoneType | None |  |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(flash_attn_varlen_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f390f",
   "metadata": {},
   "source": [
    "## `flash_attn_func`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6239696",
   "metadata": {},
   "source": [
    "So, the question is, do they produce the same result? We will use `flash_attn_func` on two sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bfe4232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Setup example data: 2 sequences\n",
    "batch_size, n_heads, head_dim = 1, 1, 8\n",
    "seq_lens = [5, 3]  # First sequence has 5 tokens, second has 3\n",
    "\n",
    "# Create query, key, value tensors for each sequence\n",
    "q1 = torch.randn(1, seq_lens[0], n_heads, head_dim, device=device, dtype=dtype)\n",
    "k1 = torch.rand_like(q1)\n",
    "v1 = torch.rand_like(q1)\n",
    "q1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec100b8",
   "metadata": {},
   "source": [
    "The shape is `bs, seq_lens, n_heads, head_dim` for q, k, and v."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8db1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2 = torch.randn(1, seq_lens[1], n_heads, head_dim, device=device, dtype=dtype)\n",
    "k2 = torch.rand_like(q2)\n",
    "v2 = torch.rand_like(q2)\n",
    "q2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987251d5",
   "metadata": {},
   "source": [
    "Let's make changes to q, k, and v so that we can have different q,k,v values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a14f83e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0.1943,  2.1562, -0.1719,  0.8477, -1.9219,  0.6523, -0.6484,\n",
       "            -0.8164]],\n",
       " \n",
       "          [[ 0.5273, -1.2734, -1.6641, -0.3027, -0.0928,  0.1992, -1.1172,\n",
       "             1.8594]],\n",
       " \n",
       "          [[-0.7148,  0.6875,  0.7969, -0.0334,  1.4922, -0.5156, -0.2539,\n",
       "             1.4766]],\n",
       " \n",
       "          [[-0.3262, -1.1562,  2.3594, -0.6914,  0.1836, -1.1797, -1.8047,\n",
       "            -1.5781]],\n",
       " \n",
       "          [[ 0.8398,  1.4219,  0.6484,  0.4258, -1.5859,  0.6211,  1.6875,\n",
       "            -0.6641]]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[[1.9844, 1.1250, 1.5625, 1.5234, 1.7500, 1.5938, 1.9688, 1.8984]],\n",
       " \n",
       "          [[1.7734, 1.6719, 1.5469, 1.5078, 1.5938, 1.2578, 1.8906, 1.1875]],\n",
       " \n",
       "          [[1.7188, 1.1797, 1.9844, 1.6250, 1.4844, 1.7344, 1.2734, 1.9062]],\n",
       " \n",
       "          [[1.6562, 1.6484, 1.3281, 1.0234, 1.1406, 1.0625, 1.4141, 1.5781]],\n",
       " \n",
       "          [[1.8203, 1.6953, 1.5781, 1.0703, 1.7344, 1.5156, 1.6875, 1.3516]]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       " \n",
       "          [[1.3438, 2.0000, 1.5547, 1.2500, 1.9297, 1.2812, 1.8438, 1.5156]],\n",
       " \n",
       "          [[2.1406, 2.6875, 2.5938, 2.8750, 2.4062, 2.7812, 2.3750, 2.1719]],\n",
       " \n",
       "          [[3.3281, 3.4219, 3.1094, 3.9844, 3.6250, 3.6250, 3.5781, 3.7812]],\n",
       " \n",
       "          [[4.0312, 4.0312, 4.1562, 4.7188, 4.5000, 4.9688, 4.7188, 4.7500]]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(seq_lens[0]):\n",
    "    k1[0, i, 0] = k1[0, i, 0] + 1\n",
    "    v1[0, i, 0] = v1[0, i, 0] + i\n",
    "q1, k1, v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d01b535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.6992, -1.8672, -0.8828, -1.6641, -0.4316,  0.9492,  0.6602,\n",
       "             0.0447]],\n",
       " \n",
       "          [[ 0.5703,  2.1875, -0.2471, -1.3828,  0.0603, -0.2432,  1.3203,\n",
       "             0.5195]],\n",
       " \n",
       "          [[-0.6094,  0.1001, -0.8945, -0.9375, -0.2656,  1.5312,  0.5586,\n",
       "            -0.9453]]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[[1.3281, 2.0000, 1.0703, 1.2891, 1.5000, 1.1406, 1.5781, 1.8281]],\n",
       " \n",
       "          [[1.4219, 1.1172, 1.2734, 1.5469, 1.9844, 1.1797, 1.5547, 1.2969]],\n",
       " \n",
       "          [[1.3828, 1.4062, 1.5625, 1.8203, 1.0156, 1.0625, 1.9609, 1.1719]]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       " \n",
       "          [[1.2109, 1.7266, 1.9688, 1.1953, 1.4375, 1.0703, 1.4688, 1.9062]],\n",
       " \n",
       "          [[2.2500, 2.7188, 2.9219, 2.6562, 2.2969, 2.4219, 2.1250, 2.6562]]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(seq_lens[1]):\n",
    "    k2[0, i, 0] = k2[0, i, 0] + 1\n",
    "    v2[0, i, 0] = v2[0, i, 0] + i\n",
    "q2, k2, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a0feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       "\n",
       "         [[0.9922, 0.7930, 1.0000, 0.6211, 1.1406, 0.5625, 1.2266, 0.5898]],\n",
       "\n",
       "         [[1.4688, 1.6484, 1.6641, 1.5469, 1.7031, 1.4844, 1.7344, 1.2969]],\n",
       "\n",
       "         [[2.0469, 2.3125, 2.1719, 2.3594, 2.3438, 2.2188, 2.3281, 2.1094]],\n",
       "\n",
       "         [[2.3125, 2.4688, 2.4062, 2.5625, 2.6406, 2.5312, 2.6875, 2.4688]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = flash_attn_func(q1, k1, v1, causal=True)\n",
    "out2 = flash_attn_func(q2, k2, v2, causal=True)\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f035772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       "\n",
       "         [[0.5703, 0.7148, 1.0000, 0.5039, 0.4863, 0.4707, 0.5703, 0.5820]],\n",
       "\n",
       "         [[1.2422, 1.5547, 1.8125, 1.3359, 1.2578, 1.2188, 1.2422, 1.5078]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee45b80",
   "metadata": {},
   "source": [
    "## `flash_attn_varlen_func`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4bcaa4",
   "metadata": {},
   "source": [
    "To use `flash_attn_varlen_func`, we have to pack the sequences, find `cu_seqlens` (cumulative sequence lengths), and `max_seq_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b100f657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 5, 8], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup for packed sequences\n",
    "batch_size, n_heads, head_dim = 1, 1, 8\n",
    "seq_lens = [5, 3]\n",
    "total_tokens = sum(seq_lens)\n",
    "\n",
    "# Create cumulative sequence lengths tensor\n",
    "cu_seqlens = torch.tensor([0, seq_lens[0], seq_lens[0] + seq_lens[1]], dtype=torch.int32, device=device) # has to be int32\n",
    "cu_seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe18dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(seq_lens)\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7059e10e",
   "metadata": {},
   "source": [
    "We no longer have batch size in the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ac4065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 8]), torch.Size([8, 1, 8]), torch.Size([8, 1, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.cat([q1, q2], dim=1).squeeze(0)\n",
    "k = torch.cat([k1, k2], dim=1).squeeze(0)\n",
    "v = torch.cat([v1, v2], dim=1).squeeze(0)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54637a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1943,  2.1562, -0.1719,  0.8477, -1.9219,  0.6523, -0.6484,\n",
       "           -0.8164]],\n",
       " \n",
       "         [[ 0.5273, -1.2734, -1.6641, -0.3027, -0.0928,  0.1992, -1.1172,\n",
       "            1.8594]],\n",
       " \n",
       "         [[-0.7148,  0.6875,  0.7969, -0.0334,  1.4922, -0.5156, -0.2539,\n",
       "            1.4766]],\n",
       " \n",
       "         [[-0.3262, -1.1562,  2.3594, -0.6914,  0.1836, -1.1797, -1.8047,\n",
       "           -1.5781]],\n",
       " \n",
       "         [[ 0.8398,  1.4219,  0.6484,  0.4258, -1.5859,  0.6211,  1.6875,\n",
       "           -0.6641]],\n",
       " \n",
       "         [[-0.6992, -1.8672, -0.8828, -1.6641, -0.4316,  0.9492,  0.6602,\n",
       "            0.0447]],\n",
       " \n",
       "         [[ 0.5703,  2.1875, -0.2471, -1.3828,  0.0603, -0.2432,  1.3203,\n",
       "            0.5195]],\n",
       " \n",
       "         [[-0.6094,  0.1001, -0.8945, -0.9375, -0.2656,  1.5312,  0.5586,\n",
       "           -0.9453]]], device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[1.9844, 1.1250, 1.5625, 1.5234, 1.7500, 1.5938, 1.9688, 1.8984]],\n",
       " \n",
       "         [[1.7734, 1.6719, 1.5469, 1.5078, 1.5938, 1.2578, 1.8906, 1.1875]],\n",
       " \n",
       "         [[1.7188, 1.1797, 1.9844, 1.6250, 1.4844, 1.7344, 1.2734, 1.9062]],\n",
       " \n",
       "         [[1.6562, 1.6484, 1.3281, 1.0234, 1.1406, 1.0625, 1.4141, 1.5781]],\n",
       " \n",
       "         [[1.8203, 1.6953, 1.5781, 1.0703, 1.7344, 1.5156, 1.6875, 1.3516]],\n",
       " \n",
       "         [[1.3281, 2.0000, 1.0703, 1.2891, 1.5000, 1.1406, 1.5781, 1.8281]],\n",
       " \n",
       "         [[1.4219, 1.1172, 1.2734, 1.5469, 1.9844, 1.1797, 1.5547, 1.2969]],\n",
       " \n",
       "         [[1.3828, 1.4062, 1.5625, 1.8203, 1.0156, 1.0625, 1.9609, 1.1719]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " tensor([[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       " \n",
       "         [[1.3438, 2.0000, 1.5547, 1.2500, 1.9297, 1.2812, 1.8438, 1.5156]],\n",
       " \n",
       "         [[2.1406, 2.6875, 2.5938, 2.8750, 2.4062, 2.7812, 2.3750, 2.1719]],\n",
       " \n",
       "         [[3.3281, 3.4219, 3.1094, 3.9844, 3.6250, 3.6250, 3.5781, 3.7812]],\n",
       " \n",
       "         [[4.0312, 4.0312, 4.1562, 4.7188, 4.5000, 4.9688, 4.7188, 4.7500]],\n",
       " \n",
       "         [[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       " \n",
       "         [[1.2109, 1.7266, 1.9688, 1.1953, 1.4375, 1.0703, 1.4688, 1.9062]],\n",
       " \n",
       "         [[2.2500, 2.7188, 2.9219, 2.6562, 2.2969, 2.4219, 2.1250, 2.6562]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b897f161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       "\n",
       "        [[0.9922, 0.7930, 1.0000, 0.6211, 1.1406, 0.5625, 1.2266, 0.5898]],\n",
       "\n",
       "        [[1.4688, 1.6484, 1.6641, 1.5469, 1.7031, 1.4844, 1.7344, 1.2969]],\n",
       "\n",
       "        [[2.0469, 2.3125, 2.1719, 2.3594, 2.3438, 2.2188, 2.3281, 2.1094]],\n",
       "\n",
       "        [[2.3125, 2.4688, 2.4062, 2.5625, 2.6406, 2.5312, 2.6875, 2.4688]],\n",
       "\n",
       "        [[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       "\n",
       "        [[0.5703, 0.7148, 1.0000, 0.5039, 0.4863, 0.4707, 0.5703, 0.5820]],\n",
       "\n",
       "        [[1.2422, 1.5547, 1.8125, 1.3359, 1.2578, 1.2188, 1.2422, 1.5078]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varlen_out = flash_attn_varlen_func(q, k, v, cu_seqlens, cu_seqlens, \n",
    "                            max_seq_len, max_seq_len, 0.0, causal=True)\n",
    "varlen_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda8230",
   "metadata": {},
   "source": [
    "Now let's compare it with results from `flash_attn_func`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "950b2f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 1, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5145ea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 8])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cabc8",
   "metadata": {},
   "source": [
    "We match the shape first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "484bafd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_out = torch.cat([out1, out2], dim=1).squeeze(0)\n",
    "packed_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44821b52",
   "metadata": {},
   "source": [
    "We can create a small function to check equality of two tensors. Under the hood, we use `test_eq` from fastcore with some changes to dtype and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cf25f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_eq_cuda(a, b, rtol=1e-5, atol=1e-8):\n",
    "    \"Test equality of tensors, handling CUDA tensors and special dtypes\"\n",
    "    a_cpu = a.cpu() if hasattr(a, 'cpu') else a\n",
    "    b_cpu = b.cpu() if hasattr(b, 'cpu') else b\n",
    "    \n",
    "    # Convert to float32 if needed\n",
    "    if hasattr(a_cpu, 'dtype') and str(a_cpu.dtype) == 'bfloat16':\n",
    "        a_cpu = a_cpu.float()\n",
    "    if hasattr(b_cpu, 'dtype') and str(b_cpu.dtype) == 'bfloat16':\n",
    "        b_cpu = b_cpu.float()\n",
    "    \n",
    "    # Use torch's isclose for tensor comparison instead of test_eq\n",
    "    if hasattr(a_cpu, 'shape') and hasattr(b_cpu, 'shape'):\n",
    "        assert a_cpu.shape == b_cpu.shape, f\"Shapes don't match: {a_cpu.shape} vs {b_cpu.shape}\"\n",
    "        assert torch.allclose(a_cpu, b_cpu, rtol=rtol, atol=atol), f\"Tensors not close enough:\\n{a_cpu}\\n{b_cpu}\"\n",
    "    else:\n",
    "        test_eq(a_cpu, b_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a76e96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_cuda(varlen_out, packed_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b76ca8",
   "metadata": {},
   "source": [
    "Test passed! Here are the outputs from each approaches. We can just take a look at them as they are small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8574ad29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       "\n",
       "        [[0.9922, 0.7930, 1.0000, 0.6211, 1.1406, 0.5625, 1.2266, 0.5898]],\n",
       "\n",
       "        [[1.4688, 1.6484, 1.6641, 1.5469, 1.7031, 1.4844, 1.7344, 1.2969]],\n",
       "\n",
       "        [[2.0469, 2.3125, 2.1719, 2.3594, 2.3438, 2.2188, 2.3281, 2.1094]],\n",
       "\n",
       "        [[2.3125, 2.4688, 2.4062, 2.5625, 2.6406, 2.5312, 2.6875, 2.4688]],\n",
       "\n",
       "        [[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       "\n",
       "        [[0.5703, 0.7148, 1.0000, 0.5039, 0.4863, 0.4707, 0.5703, 0.5820]],\n",
       "\n",
       "        [[1.2422, 1.5547, 1.8125, 1.3359, 1.2578, 1.2188, 1.2422, 1.5078]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varlen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69bdb3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8242, 0.2129, 0.7305, 0.3203, 0.7539, 0.2129, 0.9297, 0.1445]],\n",
       "\n",
       "        [[0.9922, 0.7930, 1.0000, 0.6211, 1.1406, 0.5625, 1.2266, 0.5898]],\n",
       "\n",
       "        [[1.4688, 1.6484, 1.6641, 1.5469, 1.7031, 1.4844, 1.7344, 1.2969]],\n",
       "\n",
       "        [[2.0469, 2.3125, 2.1719, 2.3594, 2.3438, 2.2188, 2.3281, 2.1094]],\n",
       "\n",
       "        [[2.3125, 2.4688, 2.4062, 2.5625, 2.6406, 2.5312, 2.6875, 2.4688]],\n",
       "\n",
       "        [[0.3145, 0.3066, 0.6133, 0.2256, 0.1050, 0.2305, 0.2070, 0.0510]],\n",
       "\n",
       "        [[0.5703, 0.7148, 1.0000, 0.5039, 0.4863, 0.4707, 0.5703, 0.5820]],\n",
       "\n",
       "        [[1.2422, 1.5547, 1.8125, 1.3359, 1.2578, 1.2188, 1.2422, 1.5078]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4866ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
