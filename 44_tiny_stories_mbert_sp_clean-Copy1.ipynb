{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbf607",
   "metadata": {},
   "source": [
    "Using \n",
    "- [x] sdpa \n",
    "- [x] optimizer\n",
    "  - [x] AdamW\n",
    "  - [x] StableAdamW\n",
    "- [x] torch.compile \n",
    "- [x] MixedPrecision()\n",
    "- [ ] **lr_sched using # of tokens.**\n",
    "- [x] Double layer norm\n",
    "- [ ] Using a custom tokenizer.\n",
    "  - [x] Use minBPE\n",
    "  - [ ] Huggingface Tokenizer\n",
    "- [x] GLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c75570",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- [ ] modern bert sequence packing + FA2\n",
    "  - [x] pad token\n",
    "  - [ ] `__length__`: Use token length\n",
    "- [ ] Gradient accumulation or microbatch.\n",
    "- [ ] W&B logging.\n",
    "- [ ] Train a tokenizer using huggingface.\n",
    "- [ ] Implement positional embedding with `cu_seqlens`.\n",
    "  - [x] Absolute\n",
    "  - [ ] Rope\n",
    "- [x] **Generate text.**\n",
    "- [ ] Cross entropy loss\n",
    "  - [ ] [CCE loss](https://github.com/apple/ml-cross-entropy)\n",
    "  - [ ] Liger kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967927c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6938ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbed3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home()/'git/minai/TinyStories_All_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a959b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257, 2365, 1597]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "# tokenizer.train(txt_raw, vocab_size=3000)\n",
    "\n",
    "tokenizer.load((path/\"tok3k_regex.model\").name) # loads the model back from disk\n",
    "tokenizer.encode(\"hello world\") # string -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ca6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_toks(txt, toker): return toker.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a08231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks_to_txt(toks, toker): return toker.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8452255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21197"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_txts = 21197\n",
    "num_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d73f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=\"\\n\\n\\n\"\n",
    "ctx_len = 1024\n",
    "bs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d68efb",
   "metadata": {},
   "source": [
    "We create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86bf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    d = {}\n",
    "    d['input_ids'] = [tokenizer.encode(t, allowed_special={\"<|endoftext|>\"})+[0] for t in b['text']]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d5014bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[377, 708, 360, 365, 1208, 266, 1228, 458, 46, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = ds.with_transform(transforms)\n",
    "tds['train'][0]['input_ids'][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed48361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e59606a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21197\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_subset = tds['train'].select(range(num_txts))\n",
    "trn_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c607d",
   "metadata": {},
   "source": [
    "### Sequence packer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd3464e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "from composer.core.types import Batch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "import math\n",
    "from composer.core import Time\n",
    "\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __len__(self): return self.src_dataloader_len // 4\n",
    "    # use tokens.\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences for causal attention.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                labels = np.full_like(batch, self.pad_token_id)\n",
    "                labels[:, :-1] = batch[:, 1:]\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            yield yieldval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "529c59c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trn_dl = DataLoader(trn_subset, batch_size=bs, collate_fn=lambda x: x)\n",
    "len(trn_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f89e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = next(iter(trn_dl))\n",
    "len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95469fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pseq_len= 1024\n",
    "\n",
    "trn_d = {\n",
    "    \"src_iterable\": trn_dl,\n",
    "    \"src_batch_size\": bs,\n",
    "    \"src_max_seq_len\": 1,\n",
    "    \"out_batch_size\": bs,\n",
    "    \"out_pseq_len\": out_pseq_len,\n",
    "    \"buffer_size\": int(bs * 10),\n",
    "    \"pad_token_id\": 0,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "    \"world_size\": 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "617230cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[516, 327,  44,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         [763, 401, 282,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0]]),\n",
       " 'labels': tensor([[327,  44, 258,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         [401, 282, 258,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0]]),\n",
       " 'cu_seqlens': [tensor([   0,  170,  360,  574,  792,  959, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  175,  349,  540,  739,  895, 1019, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  221,  352,  589,  777,  913, 1010, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  208,  403,  546,  711,  961, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  108,  273,  426,  560,  732,  871, 1015, 1024],\n",
       "         dtype=torch.int32),\n",
       "  tensor([   0,  140,  335,  500,  712,  846,  970, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  214,  457,  650,  822,  972, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  144,  427,  663,  879, 1018, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  211,  367,  554,  786,  988, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  197,  367,  588,  839,  974, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  207,  464,  622,  907, 1002, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  260,  421,  746,  977, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  191,  403,  584,  835,  992, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  243,  472,  710,  897, 1005, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  210,  379,  582,  845, 1008, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  208,  516,  795,  984, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  251,  490,  786,  930, 1008, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  240,  400,  617,  847, 1015, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  252,  502,  651,  859,  993, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  317,  538,  770, 1003, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  300,  573,  747,  951, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  280,  517,  793, 1022, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  225,  558,  738,  950, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  181,  451,  644,  890, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  363,  508,  718,  914, 1019, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  255,  515,  688,  895, 1023, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  193,  507,  775, 1016, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  232,  424,  678,  855,  999, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  225,  502,  706, 1005, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  235,  456,  639,  895, 1011, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  242,  592,  792,  975, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  251,  499,  748,  958, 1024], dtype=torch.int32)],\n",
       " 'max_seqlen': [218,\n",
       "  199,\n",
       "  237,\n",
       "  250,\n",
       "  172,\n",
       "  212,\n",
       "  243,\n",
       "  283,\n",
       "  232,\n",
       "  251,\n",
       "  285,\n",
       "  325,\n",
       "  251,\n",
       "  243,\n",
       "  263,\n",
       "  308,\n",
       "  296,\n",
       "  240,\n",
       "  252,\n",
       "  317,\n",
       "  300,\n",
       "  280,\n",
       "  333,\n",
       "  270,\n",
       "  363,\n",
       "  260,\n",
       "  314,\n",
       "  254,\n",
       "  299,\n",
       "  256,\n",
       "  350,\n",
       "  251]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_trn_dl = CausalSequencePacker(**trn_d)\n",
    "sq_batch = next(iter(sq_trn_dl))\n",
    "sq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcab36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subset = trn_subset = tds['validation'].select(range(len(tds['validation'])//10))\n",
    "val_dl = DataLoader(val_subset, batch_size=bs, collate_fn=lambda x: x)\n",
    "\n",
    "val_d = {\n",
    "    \"src_iterable\": val_dl,\n",
    "    \"src_batch_size\": bs,\n",
    "    \"src_max_seq_len\": 1,\n",
    "    \"out_batch_size\": bs,\n",
    "    \"out_pseq_len\": out_pseq_len,\n",
    "    \"buffer_size\": int(bs * 10),\n",
    "    \"pad_token_id\": 0,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}\n",
    "sq_val_dl = CausalSequencePacker(**val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fddca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CausalSequencePacker at 0x707b7b71f0b0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders(sq_trn_dl, sq_val_dl)\n",
    "dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ac3e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c1bf0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[516, 327,  44,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [763, 438, 258,  ...,   0,   0,   0],\n",
       "         [763, 401, 282,  ...,   0,   0,   0],\n",
       "         [763, 438, 258,  ...,   0,   0,   0]]),\n",
       " 'labels': tensor([[327,  44, 258,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         ...,\n",
       "         [438, 258, 397,  ...,   0,   0,   0],\n",
       "         [401, 282, 258,  ...,   0,   0,   0],\n",
       "         [438, 258, 397,  ...,   0,   0,   0]]),\n",
       " 'cu_seqlens': [tensor([   0,  170,  360,  574,  792,  959, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  175,  349,  540,  739,  895, 1019, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  221,  352,  589,  777,  913, 1010, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  208,  403,  546,  711,  961, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  108,  273,  426,  560,  732,  871, 1015, 1024],\n",
       "         dtype=torch.int32),\n",
       "  tensor([   0,  140,  335,  500,  712,  846,  970, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  214,  457,  650,  822,  972, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  144,  427,  663,  879, 1018, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  211,  367,  554,  786,  988, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  197,  367,  588,  839,  974, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  207,  464,  622,  907, 1002, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  260,  421,  746,  977, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  191,  403,  584,  835,  992, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  243,  472,  710,  897, 1005, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  210,  379,  582,  845, 1008, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  208,  516,  795,  984, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  251,  490,  786,  930, 1008, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  240,  400,  617,  847, 1015, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  252,  502,  651,  859,  993, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  317,  538,  770, 1003, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  300,  573,  747,  951, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  280,  517,  793, 1022, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  225,  558,  738,  950, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  181,  451,  644,  890, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  363,  508,  718,  914, 1019, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  255,  515,  688,  895, 1023, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  193,  507,  775, 1016, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  232,  424,  678,  855,  999, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  225,  502,  706, 1005, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  235,  456,  639,  895, 1011, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  242,  592,  792,  975, 1024], dtype=torch.int32),\n",
       "  tensor([   0,  251,  499,  748,  958, 1024], dtype=torch.int32)],\n",
       " 'max_seqlen': [218,\n",
       "  199,\n",
       "  237,\n",
       "  250,\n",
       "  172,\n",
       "  212,\n",
       "  243,\n",
       "  283,\n",
       "  232,\n",
       "  251,\n",
       "  285,\n",
       "  325,\n",
       "  251,\n",
       "  243,\n",
       "  263,\n",
       "  308,\n",
       "  296,\n",
       "  240,\n",
       "  252,\n",
       "  317,\n",
       "  300,\n",
       "  280,\n",
       "  333,\n",
       "  270,\n",
       "  363,\n",
       "  260,\n",
       "  314,\n",
       "  254,\n",
       "  299,\n",
       "  256,\n",
       "  350,\n",
       "  251]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf3dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 1024]) torch.Size([32, 1024])\n",
      "1 torch.Size([32, 1024]) torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dls.train):\n",
    "    print(i, batch['input_ids'].shape, batch['labels'].shape)\n",
    "    if i == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### FlashCausalAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55c0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    Flash Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with Flash Attention implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - Check if Flash Attention is available (FLASH_ATTN_AVAILABLE)\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [total_seq_len, hidden_dim].\n",
    "            cu_seqlens: Cumulative sequence lengths tensor of shape [batch_size + 1]\n",
    "                    Used instead of an attention mask for both masking and\n",
    "                    variable-length sequences. Example:\n",
    "                        cu_seqlens = torch.tensor([0, 10, 30, 60])\n",
    "                    This means there are three sequences in the batch:\n",
    "                        - First sequence has 10 tokens\n",
    "                        - Second sequence has 20 tokens\n",
    "                        - Third sequence has 30 tokens\n",
    "            max_seqlen: Maximum sequence length in the batch. In the example above,\n",
    "                        the maximum sequence length is 30.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [total_seq_len, hidden_dim] after attention.\n",
    "        \"\"\"    \n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            raise ImportError(\"Flash Attention is not available. Please install it with `pip install flash-attn`\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x) # [batch_size, seq_len, d_out]\n",
    "\n",
    "        k_reshaped = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        q_reshaped = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v_reshaped = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        output = flash_attn_varlen_func(\n",
    "            q_reshaped,\n",
    "            k_reshaped,\n",
    "            v_reshaped,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead43c6",
   "metadata": {},
   "source": [
    "Using a mokey patched version for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2c9878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FlashCausalAttentionBlock(nn.Module):\n",
    "#     \"\"\"Monkey patching\"\"\"\n",
    "#     def __init__(self, hidden_dim, num_heads, dropout = 0.0): super().__init__()\n",
    "    \n",
    "#     def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cf8210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH_ATTN_AVAILABLE = False\n",
    "try:\n",
    "    from flash_attn import flash_attn_varlen_func\n",
    "\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Flash Attention is not available\n",
    "    pass\n",
    "\n",
    "# with torch.no_grad(), torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "#     mha = FlashCausalAttentionBlock(hidden_dim=out_pseq_len, num_heads=1).to(def_device)\n",
    "#     output = mha(out_batches['input_ids'].to(torch.float32).to(def_device), out_batches['cu_seqlens'][0].to(def_device), out_batches['max_seqlen'][0])\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02ace140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashCausalAttentionBlock(\n",
       "  (Wq): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (Wk): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (Wv): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (Wo): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = FlashCausalAttentionBlock(hidden_dim=out_pseq_len, num_heads=2).to(def_device)\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4d523b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sq_batch['cu_seqlens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc9891a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.tensor([t[-1] for t in sq_batch['cu_seqlens'][:-1]], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb5a2a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sq_batch['cu_seqlens'][0][-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d07df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.full((len(sq_batch['cu_seqlens']),), sq_batch['cu_seqlens'][0][-1].item(), dtype=torch.int32, device=def_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "718d2f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 377,  708,  360,  365, 1208,  266, 1228,  458,   46,    0,  763,  438,\n",
       "         258,  397,   44,  401,  282,  258,  390,  528])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_batch['input_ids'][0][160:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f9b09eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' happy because they had shared and worked together.\\x00Once upon a time, there was a little car'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(sq_batch['input_ids'][0][160:180], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e76617ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,  170,  360,  574,  792,  959, 1024], dtype=torch.int32),\n",
       " tensor([   0,  175,  349,  540,  739,  895, 1019, 1024], dtype=torch.int32),\n",
       " tensor([   0,  221,  352,  589,  777,  913, 1010, 1024], dtype=torch.int32),\n",
       " tensor([   0,  208,  403,  546,  711,  961, 1024], dtype=torch.int32),\n",
       " tensor([   0,  108,  273,  426,  560,  732,  871, 1015, 1024],\n",
       "        dtype=torch.int32),\n",
       " tensor([   0,  140,  335,  500,  712,  846,  970, 1024], dtype=torch.int32),\n",
       " tensor([   0,  214,  457,  650,  822,  972, 1024], dtype=torch.int32),\n",
       " tensor([   0,  144,  427,  663,  879, 1018, 1024], dtype=torch.int32),\n",
       " tensor([   0,  211,  367,  554,  786,  988, 1024], dtype=torch.int32),\n",
       " tensor([   0,  197,  367,  588,  839,  974, 1024], dtype=torch.int32),\n",
       " tensor([   0,  207,  464,  622,  907, 1002, 1024], dtype=torch.int32),\n",
       " tensor([   0,  260,  421,  746,  977, 1024], dtype=torch.int32),\n",
       " tensor([   0,  191,  403,  584,  835,  992, 1024], dtype=torch.int32),\n",
       " tensor([   0,  243,  472,  710,  897, 1005, 1024], dtype=torch.int32),\n",
       " tensor([   0,  210,  379,  582,  845, 1008, 1024], dtype=torch.int32),\n",
       " tensor([   0,  208,  516,  795,  984, 1024], dtype=torch.int32),\n",
       " tensor([   0,  251,  490,  786,  930, 1008, 1024], dtype=torch.int32),\n",
       " tensor([   0,  240,  400,  617,  847, 1015, 1024], dtype=torch.int32),\n",
       " tensor([   0,  252,  502,  651,  859,  993, 1024], dtype=torch.int32),\n",
       " tensor([   0,  317,  538,  770, 1003, 1024], dtype=torch.int32),\n",
       " tensor([   0,  300,  573,  747,  951, 1024], dtype=torch.int32),\n",
       " tensor([   0,  280,  517,  793, 1022, 1024], dtype=torch.int32),\n",
       " tensor([   0,  225,  558,  738,  950, 1024], dtype=torch.int32),\n",
       " tensor([   0,  181,  451,  644,  890, 1024], dtype=torch.int32),\n",
       " tensor([   0,  363,  508,  718,  914, 1019, 1024], dtype=torch.int32),\n",
       " tensor([   0,  255,  515,  688,  895, 1023, 1024], dtype=torch.int32),\n",
       " tensor([   0,  193,  507,  775, 1016, 1024], dtype=torch.int32),\n",
       " tensor([   0,  232,  424,  678,  855,  999, 1024], dtype=torch.int32),\n",
       " tensor([   0,  225,  502,  706, 1005, 1024], dtype=torch.int32),\n",
       " tensor([   0,  235,  456,  639,  895, 1011, 1024], dtype=torch.int32),\n",
       " tensor([   0,  242,  592,  792,  975, 1024], dtype=torch.int32),\n",
       " tensor([   0,  251,  499,  748,  958, 1024], dtype=torch.int32)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_batch['cu_seqlens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ddcba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   170,   360,   574,   792,   959,  1024,  1199,  1373,  1564,\n",
       "         1763,  1919,  2043,  2048,  2269,  2400,  2637,  2825,  2961,  3058,\n",
       "         3072,  3280,  3475,  3618,  3783,  4033,  4096,  4204,  4369,  4522,\n",
       "         4656,  4828,  4967,  5111,  5120,  5260,  5455,  5620,  5832,  5966,\n",
       "         6090,  6144,  6358,  6601,  6794,  6966,  7116,  7168,  7312,  7595,\n",
       "         7831,  8047,  8186,  8192,  8403,  8559,  8746,  8978,  9180,  9216,\n",
       "         9413,  9583,  9804, 10055, 10190, 10240, 10447, 10704, 10862, 11147,\n",
       "        11242, 11264, 11524, 11685, 12010, 12241, 12288, 12479, 12691, 12872,\n",
       "        13123, 13280, 13312, 13555, 13784, 14022, 14209, 14317, 14336, 14546,\n",
       "        14715, 14918, 15181, 15344, 15360, 15568, 15876, 16155, 16344, 16384,\n",
       "        16635, 16874, 17170, 17314, 17392, 17408, 17648, 17808, 18025, 18255,\n",
       "        18423, 18432, 18684, 18934, 19083, 19291, 19425, 19456, 19773, 19994,\n",
       "        20226, 20459, 20480, 20780, 21053, 21227, 21431, 21504, 21784, 22021,\n",
       "        22297, 22526, 22528, 22753, 23086, 23266, 23478, 23552, 23733, 24003,\n",
       "        24196, 24442, 24576, 24939, 25084, 25294, 25490, 25595, 25600, 25855,\n",
       "        26115, 26288, 26495, 26623, 26624, 26817, 27131, 27399, 27640, 27648,\n",
       "        27880, 28072, 28326, 28503, 28647, 28672, 28897, 29174, 29378, 29677,\n",
       "        29696, 29931, 30152, 30335, 30591, 30707, 30720, 30962, 31312, 31512,\n",
       "        31695, 31744, 31995, 32243, 32492, 32702, 32768], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_cu_seqlens(sq_batch['cu_seqlens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23c6d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_cu_seqlens(cu_seqlens):\n",
    "    \"Combine multiple tensors into cumulative sequence\"\n",
    "#     return cu_seqlens[0]\n",
    "    dtype = cu_seqlens[0].dtype\n",
    "    lengths = torch.tensor([t[-1] for t in cu_seqlens[:-1]], dtype=dtype).to(def_device)\n",
    "    offsets = torch.cat([torch.tensor([0], dtype=dtype, device=def_device), torch.cumsum(lengths, dim=0)]).to(dtype)\n",
    "    return torch.cat([t[:-1] if i < len(cu_seqlens)-1 else t for i, t in enumerate(cu_seqlens)]).to(def_device) + offsets.repeat_interleave(torch.tensor([t.numel()-1 if i < len(cu_seqlens)-1 else t.numel() for i, t in enumerate(cu_seqlens)]).to(def_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b1755d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate(model,toks, 10, ctx_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89622e98",
   "metadata": {},
   "source": [
    "GeGLU > Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim, bias=False)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae0f91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "    The Gated Linear Unit has two parallel linear transforms: one for the gate and one for the value.\n",
    "    Apply the activation only to the gate, then multiply elementwise with the value, followed by a\n",
    "    final linear projection and optional dropout.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        intermediate_dim: int,\n",
    "        act: nn.Module = nn.GELU,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a GLU.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            intermediate_dim: Dimension of each intermediate branch\n",
    "                              Often set to 2/3 * 4 * hidden_dim to maintain similar parameter\n",
    "                              count to a standard MLP with 4x expansion\n",
    "            activation: Activation function to use, defaults to GELU\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.Wg = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.act = act\n",
    "        self.Wo = nn.Linear(intermediate_dim, hidden_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        gate = self.act(self.Wg(x))\n",
    "        val = self.Wv(x)\n",
    "        out = self.Wo(gate * val)\n",
    "        return self.do(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "        self.ln4 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = FlashCausalAttentionBlock(hidden_dim=emb_dim, num_heads=n_head, dropout=drop_out)\n",
    "        self.ff = GLU(emb_dim, int(emb_dim*ff_mult), act=act)\n",
    "        self.emb_dim = emb_dim\n",
    "    \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        x_shape = x.shape\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        # reshape for flash attention\n",
    "        x = x.view(-1, self.emb_dim)\n",
    "        x = self.mha(x, cu_seqlens, max_seqlen)  # Need to pass (x: Tensor, cu_seqlens: Tensor, max_seqlen: int) \n",
    "        x = x.view(*x_shape)  # x.shape:(total_seq_len, hidden_dim)\n",
    "        x = self.ln2(x) \n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln3(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.ln4(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2b6f3",
   "metadata": {},
   "source": [
    "**TODO: create `pos_emb` using `cu_seqlens`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "133be1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pos_ids(seqlens, max_len=None, device=def_device):\n",
    "    if max_len is None: max_len = seqlens[-1].item()\n",
    "    pos_ids = torch.zeros(max_len, dtype=torch.long, device=device)\n",
    "    \n",
    "    for start, end in zip(seqlens[:-1], seqlens[1:]):\n",
    "        pos_ids[start:end] = torch.arange(end - start)\n",
    "    \n",
    "    return pos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "154e980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   170,   360,   574,   792,   959,  1024,  1199,  1373,  1564,\n",
       "         1763,  1919,  2043,  2048,  2269,  2400,  2637,  2825,  2961,  3058,\n",
       "         3072,  3280,  3475,  3618,  3783,  4033,  4096,  4204,  4369,  4522,\n",
       "         4656,  4828,  4967,  5111,  5120,  5260,  5455,  5620,  5832,  5966,\n",
       "         6090,  6144,  6358,  6601,  6794,  6966,  7116,  7168,  7312,  7595,\n",
       "         7831,  8047,  8186,  8192,  8403,  8559,  8746,  8978,  9180,  9216,\n",
       "         9413,  9583,  9804, 10055, 10190, 10240, 10447, 10704, 10862, 11147,\n",
       "        11242, 11264, 11524, 11685, 12010, 12241, 12288, 12479, 12691, 12872,\n",
       "        13123, 13280, 13312, 13555, 13784, 14022, 14209, 14317, 14336, 14546,\n",
       "        14715, 14918, 15181, 15344, 15360, 15568, 15876, 16155, 16344, 16384,\n",
       "        16635, 16874, 17170, 17314, 17392, 17408, 17648, 17808, 18025, 18255,\n",
       "        18423, 18432, 18684, 18934, 19083, 19291, 19425, 19456, 19773, 19994,\n",
       "        20226, 20459, 20480, 20780, 21053, 21227, 21431, 21504, 21784, 22021,\n",
       "        22297, 22526, 22528, 22753, 23086, 23266, 23478, 23552, 23733, 24003,\n",
       "        24196, 24442, 24576, 24939, 25084, 25294, 25490, 25595, 25600, 25855,\n",
       "        26115, 26288, 26495, 26623, 26624, 26817, 27131, 27399, 27640, 27648,\n",
       "        27880, 28072, 28326, 28503, 28647, 28672, 28897, 29174, 29378, 29677,\n",
       "        29696, 29931, 30152, 30335, 30591, 30707, 30720, 30962, 31312, 31512,\n",
       "        31695, 31744, 31995, 32243, 32492, 32702, 32768], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens = combine_cu_seqlens(sq_batch['cu_seqlens'])\n",
    "cu_seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0f21fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([187])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70b452e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 32, 32768)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_len,bs, ctx_len*bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95eedc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([180, 181, 182, 183, 184, 185, 186, 187, 188, 189,   0,   1,   2,   3,\n",
       "          4,   5,   6,   7,   8,   9], device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pos_ids(cu_seqlens, ctx_len*bs)[350:370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "02825299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pos_ids(cu_seqlens, ctx_len*bs).view(bs, ctx_len).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b837e7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['ctx_len'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.ModuleList(\n",
    "            [TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias'], cfg['act']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        cu_seqlens = combine_cu_seqlens(cu_seqlens)\n",
    "        max_seqlen = max(max_seqlen)\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)).view(bs, seq_len)\n",
    "        import pdb; pdb.set_trace()\n",
    "        x = self.do(tok + pos)\n",
    "        for tb in self.tb:\n",
    "            x = tb(x, cu_seqlens, max_seqlen) \n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "64d3e9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[119, 104, 296, 430, 368, 63]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_to_toks('what is up?', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "48cfc7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[119, 104, 296, 430, 368]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_to_toks('what is up', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecb7c13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_txt([119], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d4c8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:].to(def_device)\n",
    "        with torch.no_grad(), torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "            logits = model(idx_cond, torch.tensor([[0, idx_cond.shape[1]]], dtype=torch.int), [idx_cond.shape[1]])   # Model takes additional parameters\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "75e22280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 60,250\n",
      "Total size: 0.23 MB\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 3008,\n",
    "    'emb_dim': 8,\n",
    "    'ctx_len': 1024,\n",
    "    'n_head': 2,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 2/3 * 4,\n",
    "    'qkv_bias': False,\n",
    "    'act': nn.GELU(),   # activation function\n",
    "}\n",
    "model = GPTModel(cfg).to(def_device)\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c392c9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[119, 104, 296, 430, 368,  63]], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = text_to_token_ids('what is up?', tokenizer).to(def_device)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d9e259fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ceb49",
   "metadata": {},
   "source": [
    "**TODO: Add logging on weights and bias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "11a3eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = {\n",
    "#     'n_tb': 1,    # num transformer blocks\n",
    "#     'vocab_sz': 3008,\n",
    "#     'emb_dim': 8,\n",
    "#     'ctx_len': 1024,\n",
    "#     'n_head': 2,\n",
    "#     'drop_out': 0,\n",
    "#     'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "#     'ff_mult': 2/3 * 4,\n",
    "#     'qkv_bias': False,\n",
    "#     'act': nn.GELU(),   # activation function\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    \"\"\"Using `learn.lbl` and `learn.inps`\"\"\"\n",
    "    def after_batch(self, learn):\n",
    "        for m in self.metrics.values(): m.update(learn.preds, learn.lbl)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(learn.inps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1ed5a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# v can also be a list isntead of a tensor.\n",
    "from collections.abc import Mapping\n",
    "\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    if isinstance(x, Mapping): return {k:to_device(v, device) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return type(x)(to_device(o, device) for o in x)\n",
    "    return x\n",
    "\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9eada510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "# Supports multiple `inp_nm` and `lbl_nm`\n",
    "def _get_inp(b, n_inp, inp_nm):\n",
    "    if inp_nm is None: return b[:n_inp]\n",
    "    if isinstance(inp_nm, (list, tuple)): return [b[k] for k in inp_nm]\n",
    "    return [b[inp_nm]]\n",
    "\n",
    "def _get_lbl(b, n_inp, lbl_nm):\n",
    "    if lbl_nm is None: return b[:n_inp]\n",
    "    if isinstance(lbl_nm, (list, tuple)): return [b[k] for k in lbl_nm]\n",
    "    return [b[lbl_nm]]\n",
    "\n",
    "def _get_preds(b, preds_nm):\n",
    "    return b if preds_nm is None else getattr(b, preds_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b9636646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1, inp_nm=None, lbl_nm=None, preds_nm=None):\n",
    "        self.n_inp,self.inp_nm,self.lbl_nm,self.preds_nm = n_inp,inp_nm,lbl_nm,preds_nm\n",
    "\n",
    "    def predict(self, learn):\n",
    "        inps = _get_inp(learn.batch, self.n_inp, self.inp_nm)\n",
    "        learn.preds = learn.model(*inps)\n",
    "\n",
    "    def get_loss(self, learn):\n",
    "        lbls = _get_lbl(learn.batch, self.n_inp, self.lbl_nm)\n",
    "        preds = _get_preds(learn.preds, self.preds_nm)\n",
    "        learn.loss = learn.loss_func(preds, *lbls)\n",
    "\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99d81cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, dtype=torch.bfloat16):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def before_fit(self, learn): self.scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=self.dtype)\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "        \n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79e3c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LLMTrainCB(TrainCB):\n",
    "    \"\"\"First label is chosen and flattened for LLM training.\n",
    "       Added learn.inps, learn.preds, and learn.lbl\n",
    "       Don't need separate loss fn.\"\"\"\n",
    "    \n",
    "    def predict(self, learn):\n",
    "        learn.inps = _get_inp(learn.batch, self.n_inp, self.inp_nm)\n",
    "        learn.preds = learn.model(*learn.inps).flatten(0, 1)\n",
    "    \n",
    "    def get_loss(self, learn):\n",
    "        learn.lbl = _get_lbl(learn.batch, self.n_inp, self.lbl_nm)[0].flatten() # flattened\n",
    "        learn.loss = learn.loss_func(learn.preds, learn.lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 22,    # num transformer blocks\n",
    "    'vocab_sz': 3008,\n",
    "    'emb_dim': 384,\n",
    "    'ctx_len': ctx_len,\n",
    "    'n_head': 12,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 2/3 * 4,\n",
    "    'qkv_bias': False,\n",
    "    'act': nn.GELU(),   # activation function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d72d6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 41,790,400\n",
      "Total size: 159.42 MB\n"
     ]
    }
   ],
   "source": [
    "# model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "model = GPTModel(cfg).to(def_device)\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), \n",
    "       LLMTrainCB(inp_nm=['input_ids', 'cu_seqlens', 'max_seqlen'], lbl_nm='labels'),\n",
    "       ProgressCB(), DeviceCB()]  \n",
    "learn = Learner(model, dls, loss_func=F.cross_entropy, cbs=cbs, opt_func=opt)\n",
    "# with torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "#     learn.lr_find()\n",
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b8295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b1f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = 1e-3, 2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "# sched = partial(lr_scheduler.CosineAnnealingLR, last_epoch=2, T_max=tmax)\n",
    "xtra = [BatchSchedCB(sched)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "330caa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch._dynamo.config.capture_scalar_outputs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "639085e4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/165 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 1024]' is invalid for input of size 12582912",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# learn = Learner(model, dls, loss_func=F.cross_entropy, cbs=cbs, opt_func=opt)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:264\u001b[39m, in \u001b[36mLearner.fit\u001b[39m\u001b[34m(self, n_epochs, train, valid, cbs, lr)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lr = \u001b[38;5;28mself\u001b[39m.lr\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_func: \u001b[38;5;28mself\u001b[39m.opt = \u001b[38;5;28mself\u001b[39m.opt_func(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m cbs: \u001b[38;5;28mself\u001b[39m.cbs.remove(cb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:252\u001b[39m, in \u001b[36mLearner._fit\u001b[39m\u001b[34m(self, train, valid)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_sz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.train_dl = CycleDL(\u001b[38;5;28mself\u001b[39m.train_dl, \u001b[38;5;28mself\u001b[39m.epoch_sz)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epochs:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m train: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mone_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(): \u001b[38;5;28mself\u001b[39m.one_epoch(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:245\u001b[39m, in \u001b[36mLearner.one_epoch\u001b[39m\u001b[34m(self, training)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train(training)\n\u001b[32m    244\u001b[39m \u001b[38;5;28mself\u001b[39m.dl = \u001b[38;5;28mself\u001b[39m.train_dl \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dls.valid\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:240\u001b[39m, in \u001b[36mLearner._one_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter,\u001b[38;5;28mself\u001b[39m.batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.dl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:227\u001b[39m, in \u001b[36mLearner._one_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m'\u001b[39m\u001b[33mafter_predict\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_loss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:272\u001b[39m, in \u001b[36mLearner.callback\u001b[39m\u001b[34m(self, method_nm)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcallback\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_nm): \u001b[43mrun_cbs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_nm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:189\u001b[39m, in \u001b[36mrun_cbs\u001b[39m\u001b[34m(cbs, method_nm, learn)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cbs, key=attrgetter(\u001b[33m'\u001b[39m\u001b[33morder\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m    188\u001b[39m     method = \u001b[38;5;28mgetattr\u001b[39m(cb, method_nm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mLLMTrainCB.predict\u001b[39m\u001b[34m(self, learn)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn):\n\u001b[32m      8\u001b[39m     learn.inps = _get_inp(learn.batch, \u001b[38;5;28mself\u001b[39m.n_inp, \u001b[38;5;28mself\u001b[39m.inp_nm)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     learn.preds = \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, x, cu_seqlens, max_seqlen)\u001b[39m\n\u001b[32m     16\u001b[39m bs, seq_len = x.shape\n\u001b[32m     17\u001b[39m tok = \u001b[38;5;28mself\u001b[39m.token_emb(x)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m pos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_pos_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdef_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdb\u001b[39;00m; pdb.set_trace()\n\u001b[32m     20\u001b[39m x = \u001b[38;5;28mself\u001b[39m.do(tok + pos)\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[32, 1024]' is invalid for input of size 12582912"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "# model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "model = GPTModel(cfg).to(def_device)\n",
    "# cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(plot=True), DeviceCB(), MixedPrecision()]\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), \n",
    "       LLMTrainCB(inp_nm=['input_ids', 'cu_seqlens', 'max_seqlen'], lbl_nm='labels'),\n",
    "       ProgressCB(plot=True), DeviceCB()] \n",
    "learn = Learner(model, dls, loss_func=F.cross_entropy, cbs=cbs+xtra, opt_func=opt)\n",
    "# learn = Learner(model, dls, loss_func=F.cross_entropy, cbs=cbs, opt_func=opt)\n",
    "with torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "    learn.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "da0f2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/tmp/ipykernel_15264/1692340423.py\u001b[39m(\u001b[92m18\u001b[39m)\u001b[36mforward\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     16\u001b[39m         bs, seq_len = x.shape\n",
      "\u001b[32m     17\u001b[39m         tok = self.token_emb(x)\n",
      "\u001b[32m---> 18\u001b[39m         pos = self.pos_emb(create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)).view(bs, seq_len)\n",
      "\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m pdb; pdb.set_trace()\n",
      "\u001b[32m     20\u001b[39m         x = self.do(tok + pos)\n",
      "\n",
      "ipdb> tok.shape\n",
      "torch.Size([32, 1024, 384])\n",
      "ipdb> create_pos_ids(cu_seqlens, seq_len*bs, device=def_device).shape\n",
      "torch.Size([32768])\n",
      "ipdb> bs*seq_len\n",
      "32768\n",
      "ipdb> x.numel()\n",
      "32768\n",
      "ipdb> cu_seqlens.shape\n",
      "torch.Size([187])\n",
      "ipdb> cu_seqlens\n",
      "tensor([    0,   170,   360,   574,   792,   959,  1024,  1199,  1373,  1564,\n",
      "         1763,  1919,  2043,  2048,  2269,  2400,  2637,  2825,  2961,  3058,\n",
      "         3072,  3280,  3475,  3618,  3783,  4033,  4096,  4204,  4369,  4522,\n",
      "         4656,  4828,  4967,  5111,  5120,  5260,  5455,  5620,  5832,  5966,\n",
      "         6090,  6144,  6358,  6601,  6794,  6966,  7116,  7168,  7312,  7595,\n",
      "         7831,  8047,  8186,  8192,  8403,  8559,  8746,  8978,  9180,  9216,\n",
      "         9413,  9583,  9804, 10055, 10190, 10240, 10447, 10704, 10862, 11147,\n",
      "        11242, 11264, 11524, 11685, 12010, 12241, 12288, 12479, 12691, 12872,\n",
      "        13123, 13280, 13312, 13555, 13784, 14022, 14209, 14317, 14336, 14546,\n",
      "        14715, 14918, 15181, 15344, 15360, 15568, 15876, 16155, 16344, 16384,\n",
      "        16635, 16874, 17170, 17314, 17392, 17408, 17648, 17808, 18025, 18255,\n",
      "        18423, 18432, 18684, 18934, 19083, 19291, 19425, 19456, 19773, 19994,\n",
      "        20226, 20459, 20480, 20780, 21053, 21227, 21431, 21504, 21784, 22021,\n",
      "        22297, 22526, 22528, 22753, 23086, 23266, 23478, 23552, 23733, 24003,\n",
      "        24196, 24442, 24576, 24939, 25084, 25294, 25490, 25595, 25600, 25855,\n",
      "        26115, 26288, 26495, 26623, 26624, 26817, 27131, 27399, 27640, 27648,\n",
      "        27880, 28072, 28326, 28503, 28647, 28672, 28897, 29174, 29378, 29677,\n",
      "        29696, 29931, 30152, 30335, 30591, 30707, 30720, 30962, 31312, 31512,\n",
      "        31695, 31744, 31995, 32243, 32492, 32702, 32768], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "ipdb> self.pos_emb(create_pos_ids(cu_seqlens, seq_len*bs, device=def_device))\n",
      "tensor([[ 0.5257, -0.1543, -0.1253,  ...,  1.0849, -1.0517, -1.0436],\n",
      "        [-0.9570, -1.4837,  1.0775,  ..., -0.6471, -2.0126, -0.0602],\n",
      "        [ 0.3773, -0.9810, -1.7022,  ..., -0.8495,  0.0073, -0.6884],\n",
      "        ...,\n",
      "        [-1.6342, -1.8965, -0.0252,  ..., -1.9618, -1.1964,  0.6218],\n",
      "        [ 0.4785, -0.0731,  1.1489,  ...,  0.6393,  0.0165, -2.5976],\n",
      "        [-0.2802, -0.3428,  0.2441,  ...,  0.7152,  0.0653, -0.5062]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n",
      "ipdb> self.pos_emb(create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)).shape\n",
      "torch.Size([32768, 384])\n",
      "ipdb> self.pos_emb(create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)).view(bs, seq_len, -1).shape\n",
      "torch.Size([32, 1024, 384])\n",
      "ipdb> create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)\n",
      "tensor([ 0,  1,  2,  ..., 63, 64, 65], device='cuda:0')\n",
      "ipdb> x\n",
      "tensor([[516, 327,  44,  ...,   0,   0,   0],\n",
      "        [763, 438, 258,  ...,   0,   0,   0],\n",
      "        [763, 438, 258,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [763, 438, 258,  ...,   0,   0,   0],\n",
      "        [763, 401, 282,  ...,   0,   0,   0],\n",
      "        [763, 438, 258,  ...,   0,   0,   0]], device='cuda:0')\n",
      "ipdb> x.shape\n",
      "torch.Size([32, 1024])\n",
      "ipdb> cu_seqlens\n",
      "tensor([    0,   170,   360,   574,   792,   959,  1024,  1199,  1373,  1564,\n",
      "         1763,  1919,  2043,  2048,  2269,  2400,  2637,  2825,  2961,  3058,\n",
      "         3072,  3280,  3475,  3618,  3783,  4033,  4096,  4204,  4369,  4522,\n",
      "         4656,  4828,  4967,  5111,  5120,  5260,  5455,  5620,  5832,  5966,\n",
      "         6090,  6144,  6358,  6601,  6794,  6966,  7116,  7168,  7312,  7595,\n",
      "         7831,  8047,  8186,  8192,  8403,  8559,  8746,  8978,  9180,  9216,\n",
      "         9413,  9583,  9804, 10055, 10190, 10240, 10447, 10704, 10862, 11147,\n",
      "        11242, 11264, 11524, 11685, 12010, 12241, 12288, 12479, 12691, 12872,\n",
      "        13123, 13280, 13312, 13555, 13784, 14022, 14209, 14317, 14336, 14546,\n",
      "        14715, 14918, 15181, 15344, 15360, 15568, 15876, 16155, 16344, 16384,\n",
      "        16635, 16874, 17170, 17314, 17392, 17408, 17648, 17808, 18025, 18255,\n",
      "        18423, 18432, 18684, 18934, 19083, 19291, 19425, 19456, 19773, 19994,\n",
      "        20226, 20459, 20480, 20780, 21053, 21227, 21431, 21504, 21784, 22021,\n",
      "        22297, 22526, 22528, 22753, 23086, 23266, 23478, 23552, 23733, 24003,\n",
      "        24196, 24442, 24576, 24939, 25084, 25294, 25490, 25595, 25600, 25855,\n",
      "        26115, 26288, 26495, 26623, 26624, 26817, 27131, 27399, 27640, 27648,\n",
      "        27880, 28072, 28326, 28503, 28647, 28672, 28897, 29174, 29378, 29677,\n",
      "        29696, 29931, 30152, 30335, 30591, 30707, 30720, 30962, 31312, 31512,\n",
      "        31695, 31744, 31995, 32243, 32492, 32702, 32768], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "ipdb> create_pos_ids(cu_seqlens, seq_len*bs, device=def_device)[160:180]\n",
      "tensor([160, 161, 162, 163, 164, 165, 166, 167, 168, 169,   0,   1,   2,   3,\n",
      "          4,   5,   6,   7,   8,   9], device='cuda:0')\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b000fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(706, 7)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train), len(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could some an His him from him from him from him from him when they took being being an But this when him from being him about some an Her no again from an But him from him again from being him from being an One again from being no again again from him again again again from him again again again again from him again again again again again again again from some an So some an When no.\" Then how when they took him from him when they took an Then they took being an I again again again from him again again again again again from some an I again again again again again from him again from him from him from some an But him from him from being what some being an But him as being scared again from him again again from him inside again again again again again again from an But him again again.\" Then this from my again again again from its an Sam.\" Then they ran some what\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "model.eval()\n",
    "token_ids = generate(\n",
    "    model=model.eval(),\n",
    "    idx=text_to_token_ids(\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\", tokenizer).to(def_device),\n",
    "    max_new_tokens=180,\n",
    "    context_size=cfg[\"ctx_len\"],\n",
    "    top_k=25,\n",
    "    temperature=1.3\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f52dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97909f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "1  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "2  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "3  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "4  Once upon a time, there lived a bunny in a fie...         NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('evaluation_prompts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, model, tokenizer, max_tokens=180, context_size=cfg[\"ctx_len\"], \n",
    "                top_k=25, temperature=1.3):\n",
    "    # Tokenize the prompt\n",
    "    toks = text_to_token_ids(row['prompt'], tokenizer)\n",
    "    \n",
    "    # Generate completion\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=toks.to(def_device),\n",
    "        max_new_tokens=max_tokens,\n",
    "        context_size=context_size,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract only the generated part (not the original prompt)\n",
    "    completion = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['completion'] = df.apply(lambda row: process_row(row, model, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b414bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"0401_init.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12510bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"0401_init.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. minbpe\n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019726f",
   "metadata": {},
   "source": [
    "Use triton cross entropy loss or compile nn.crosstropyloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a80a",
   "metadata": {},
   "source": [
    "Add view(-1,...) before flash attention and remove view(-1,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01d2c9f",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What is a good padding token?\n",
    "- How to use dictionary?\n",
    "- `View` in MHA\n",
    "- What does `src_max_seq_len` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72152b7",
   "metadata": {},
   "source": [
    "Tokenizer train with number fo merges and optionally add count."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
