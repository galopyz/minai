{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import sys, gc, traceback, math, typing, random, numpy as np\n",
    "from collections.abc import Mapping\n",
    "from copy import copy\n",
    "from time import time\n",
    "from itertools import zip_longest\n",
    "from functools import partial, wraps\n",
    "from operator import attrgetter, itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import fastcore.all as fc\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from fastprogress.core import format_time\n",
    "\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import default_collate\n",
    "\n",
    "from torcheval.metrics import Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "try: from accelerate import Accelerator\n",
    "except: Accelerator=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the core functionality of the training framework developed interactively in the most recent FastAI course ('Impractical Deep Learning for Coders'). It is built on top of ideas from the fastai library, but is more flexible, and simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def set_seed(seed, deterministic=False):\n",
    "    torch.use_deterministic_algorithms(deterministic)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import os\n",
    "\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:8'\n",
    "set_seed(42, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn = ds['train']\n",
    "val = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset needs to match the PyTorch dataset interface, which is very simple. It needs to have a `__len__` method, and a `__getitem__` method. The `__getitem__` method typically returns a tuple of (x, y) where x is the input and y is the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Dataset():\n",
    "    \"Simple dataset that combines two collections\"\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset can optionally include transformation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TfmDataset(Dataset):\n",
    "    \"Dataset subclass that transforms items\"\n",
    "    def __init__(self, x, y, tfm_x=None, tfm_y=None):\n",
    "        super().__init__(x,y)\n",
    "        self.tfm_x,self.tfm_y = tfm_x,tfm_y\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        x,y = self.x[i],self.y[i]\n",
    "        return self.tfm_x(x) if self.tfm_x else x, self.tfm_y(y) if self.tfm_y else y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TfmDataset(trn['image'], trn['label'], tfm_x=to_tensor)\n",
    "val_ds = TfmDataset(val['image'], val['label'], tfm_x=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    \"Convert train and validation datasets to data loaders\"\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1024\n",
    "trn_dl,val_dl = get_dls(trn_ds, val_ds, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.mean(), xb.std(), xb.min(), xb.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface datasets typically return dictionaries, so here's a collate function to handle them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def collate_dict(ds):\n",
    "    get = itemgetter(*ds.features)\n",
    "    def _f(b): return get(default_collate(b))\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can transform these dict-style datasets using `with_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    b['image'] = [to_tensor(o)*2-1 for o in b['image']]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = ds.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DataLoaders:\n",
    "    \"Convert a `DatasetDict` into a pair of `DataLoader`s\"\n",
    "    def __init__(self, *dls): self.train,self.valid = dls[:2]\n",
    "\n",
    "    @classmethod\n",
    "    def from_dd(cls, dd, batch_size, as_tuple=True, **kwargs):\n",
    "        f = collate_dict(dd['train'])\n",
    "        return cls(*get_dls(*dd.values(), bs=batch_size, collate_fn=f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate the same `DataLoader`s as before using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_dd(tds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.train))\n",
    "xb.shape,yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.mean(), xb.std(), xb.min(), xb.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities for Displaying Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often work with images - this section has some convenient utilities for displaying them. At some point I'll add some examples :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(plt.Axes.imshow)\n",
    "def show_image(im, ax=None, figsize=None, title=None, noframe=True, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    if fc.hasattrs(im, ('cpu','permute','detach')):\n",
    "        im = im.detach().cpu()\n",
    "        if len(im.shape)==3 and im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=np.array(im)\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, **kwargs)\n",
    "    if title is not None: ax.set_title(title)\n",
    "    ax.set_xticks([]) \n",
    "    ax.set_yticks([]) \n",
    "    if noframe: ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(plt.subplots, keep=True)\n",
    "def subplots(\n",
    "    nrows:int=1, # Number of rows in returned axes grid\n",
    "    ncols:int=1, # Number of columns in returned axes grid\n",
    "    figsize:tuple=None, # Width, height in inches of the returned figure\n",
    "    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n",
    "    suptitle:str=None, # Title to be set to returned figure\n",
    "    **kwargs\n",
    "): # fig and axs\n",
    "    \"A figure and set of subplots to display images of `imsize` inches\"\n",
    "    if figsize is None: figsize=(ncols*imsize, nrows*imsize)\n",
    "    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    if suptitle is not None: fig.suptitle(suptitle)\n",
    "    if nrows*ncols==1: ax = np.array([ax])\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(subplots)\n",
    "def get_grid(\n",
    "    n:int, # Number of axes\n",
    "    nrows:int=None, # Number of rows, defaulting to `int(math.sqrt(n))`\n",
    "    ncols:int=None, # Number of columns, defaulting to `ceil(n/rows)`\n",
    "    title:str=None, # If passed, title set to the figure\n",
    "    weight:str='bold', # Title font weight\n",
    "    size:int=14, # Title font size\n",
    "    **kwargs,\n",
    "): # fig and axs\n",
    "    \"Return a grid of `n` axes, `rows` by `cols`\"\n",
    "    if nrows: ncols = ncols or int(np.floor(n/nrows))\n",
    "    elif ncols: nrows = nrows or int(np.ceil(n/ncols))\n",
    "    else:\n",
    "        nrows = int(math.sqrt(n))\n",
    "        ncols = int(np.floor(n/nrows))\n",
    "    fig,axs = subplots(nrows, ncols, **kwargs)\n",
    "    for i in range(n, nrows*ncols): axs.flat[i].set_axis_off()\n",
    "    if title is not None: fig.suptitle(title, weight=weight, size=size)\n",
    "    return fig,axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.delegates(subplots)\n",
    "def show_images(ims:list, # Images to show\n",
    "                nrows:typing.Union[int, None]=None, # Number of rows in grid\n",
    "                ncols:typing.Union[int, None]=None, # Number of columns in grid (auto-calculated if None)\n",
    "                titles:typing.Union[list, None]=None, # Optional list of titles for each image\n",
    "                **kwargs):\n",
    "    \"Show all images `ims` as subplots with `rows` using `titles`\"\n",
    "    axs = get_grid(len(ims), nrows, ncols, **kwargs)[1].flat\n",
    "    for im,t,ax in zip_longest(ims, [] if titles is None else titles, axs): show_image(im, ax=ax, title=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'gray_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = fc.nested_attr(dls, 'train.dataset.features')\n",
    "names = feat['label'].names\n",
    "titles = [names[i] for i in yb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(xb[:9], titles=titles[:9], figsize=(3,4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience functions related to device management. Note that if you do `from minai import *`  then def_device will be defined and will be used as the default in functions like to_device() unless you specify otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}\n",
    "    return type(x)(to_device(o, device) for o in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(x):\n",
    "    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}\n",
    "    if isinstance(x, list): return [to_cpu(o) for o in x]\n",
    "    if isinstance(x, tuple): return tuple(to_cpu(list(x)))\n",
    "    return x.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def collate_device(b): return to_device(default_collate(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Learner "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of miniai is the Learner class. It binds together model, dataloaders, loss function and so on. The goal is to handle everything required for training a model while still providing complete control over any of the steps involved. This is done by leaning heavily on callbacks. \n",
    "\n",
    "It may be instructive here to look at how a single batch is processed (code from the Learner definition below):\n",
    "\n",
    "```python\n",
    "@with_cbs('batch')\n",
    "def _one_batch(self):\n",
    "    self.predict()\n",
    "    self.callback('after_predict')\n",
    "    self.get_loss()\n",
    "    self.callback('after_loss')\n",
    "    if self.training:\n",
    "        self.backward()\n",
    "        self.callback('after_backward')\n",
    "        self.step()\n",
    "        self.callback('after_step')\n",
    "        self.zero_grad()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@with_cbs('batch')` decorator means that any of the models callbacks that define a `before_batch` or `after_batch` method will have said method called when appropriate. This allows you to do things like pre-process a batch of data before the model sees it, or log the loss after the batch has been processed. Within the `_one_batch` method, five special functions are called (predict, get_loss, backward, step, zero_grad). These are the five steps that are required for training a model. There are additional points where callbacks can be called, for example after the loss has been calculated but before the model has been updated. \n",
    "\n",
    "The default Learner doesn't even define these methods - instead, it looks to see if they are defined in any of its callbacks. If you're not planning on doing anything fancy, then TrainCB is all you need. It defines all of the methods above. Alternatively, you can use `TrainLearner` which is a subclass of Learner that defines all of the methods above.\n",
    "\n",
    "What's the point? These choices mean that if you're just fitting a basic model then you can use TrainLearner or TrainCB and you don't need to worry about any of the details. BUT if you do want to go in and add something fancy, you now have that option. Need a custom step to modify gradients before they are applied? No problem - re-define the step method. Need to make sure everything is on the right device before calling the model? No problem - check out DeviceCB to see how easy it is to do this! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CancelFitException(Exception): pass\n",
    "class CancelBatchException(Exception): pass\n",
    "class CancelEpochException(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner class and friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CycleDL():\n",
    "    def __init__(self, items, sz=None):\n",
    "        self.items = items\n",
    "        self.sz = len(items) if sz is None else sz\n",
    "        self.it = None\n",
    "\n",
    "    def __len__(self): return len(self.items) if self.sz is None else self.sz\n",
    "    def __iter__(self):\n",
    "        if self.it is None: self.it = cycle(iter(self.items))\n",
    "        for i in range(self.sz): yield next(self.it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CycleDL(range(10), 3)\n",
    "[list(d) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Learner():\n",
    "    def __init__(self, model, dls=(0,), loss_func=F.mse_loss, lr=0.1, cbs=None, opt_func=optim.SGD, epoch_sz=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        fc.store_attr()\n",
    "\n",
    "    @with_cbs('batch')\n",
    "    def _one_batch(self):\n",
    "        self.predict()\n",
    "        self.callback('after_predict')\n",
    "        self.get_loss()\n",
    "        self.callback('after_loss')\n",
    "        if self.training:\n",
    "            self.backward()\n",
    "            self.callback('after_backward')\n",
    "            self.step()\n",
    "            self.callback('after_step')\n",
    "            self.zero_grad()\n",
    "\n",
    "    @with_cbs('epoch')\n",
    "    def _one_epoch(self):\n",
    "        for self.iter,self.batch in enumerate(self.dl): self._one_batch()\n",
    "\n",
    "    def one_epoch(self, training):\n",
    "        self.model.train(training)\n",
    "        self.dl = self.train_dl if training else self.dls.valid\n",
    "        self._one_epoch()\n",
    "\n",
    "    @with_cbs('fit')\n",
    "    def _fit(self, train, valid):\n",
    "        self.train_dl = self.dls.train\n",
    "        if self.epoch_sz is not None: self.train_dl = CycleDL(self.train_dl, self.epoch_sz)\n",
    "        for self.epoch in self.epochs:\n",
    "            if train: self.one_epoch(True)\n",
    "            if valid:\n",
    "                with torch.inference_mode(): self.one_epoch(False)\n",
    "\n",
    "    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):\n",
    "        cbs = fc.L(cbs)\n",
    "        self.cbs += cbs\n",
    "        try:\n",
    "            self.n_epochs = n_epochs\n",
    "            self.epochs = range(n_epochs)\n",
    "            if lr is None: lr = self.lr\n",
    "            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)\n",
    "            self._fit(train, valid)\n",
    "        finally:\n",
    "            for cb in cbs: self.cbs.remove(cb)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)\n",
    "        raise AttributeError(name)\n",
    "\n",
    "    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)\n",
    "    \n",
    "    @property\n",
    "    def training(self): return self.model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _get_inp(b, n_inp, inp_nm):\n",
    "    if inp_nm is not None: return [b[inp_nm]]\n",
    "    return b[:n_inp]\n",
    "\n",
    "def _get_lbl(b, n_inp, lbl_nm):\n",
    "    if lbl_nm is not None: return [b[lbl_nm]]\n",
    "    return b[n_inp:]\n",
    "\n",
    "def _get_preds(b, preds_nm):\n",
    "    return b if preds_nm is None else getattr(b, preds_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainLearner(Learner):\n",
    "    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=torch.optim.SGD, epoch_sz=None,\n",
    "                 n_inp=1, inp_nm=None, lbl_nm=None, preds_nm=None):\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func=opt_func, epoch_sz=epoch_sz)\n",
    "        self.n_inp,self.inp_nm,self.lbl_nm,self.preds_nm = n_inp,inp_nm,lbl_nm,preds_nm\n",
    "\n",
    "    def predict(self):\n",
    "        inps = _get_inp(self.batch, self.n_inp, self.inp_nm)\n",
    "        self.preds = self.model(*inps)\n",
    "\n",
    "    def get_loss(self):\n",
    "        lbls = _get_lbl(self.batch, self.n_inp, self.lbl_nm)\n",
    "        preds = _get_preds(self.preds, self.preds_nm)\n",
    "        self.loss = self.loss_func(preds, *lbls)\n",
    "\n",
    "    def backward(self): self.loss.backward()\n",
    "    def step(self): self.opt.step()\n",
    "    def zero_grad(self): self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1, inp_nm=None, lbl_nm=None, preds_nm=None):\n",
    "        self.n_inp = n_inp\n",
    "        self.n_inp,self.inp_nm,self.lbl_nm,self.preds_nm = n_inp,inp_nm,lbl_nm,preds_nm\n",
    "\n",
    "    def predict(self, learn):\n",
    "        inps = _get_inp(learn.batch, self.n_inp, self.inp_nm)\n",
    "        learn.preds = learn.model(*inps)\n",
    "\n",
    "    def get_loss(self, learn):\n",
    "        lbls = _get_lbl(learn.batch, self.n_inp, self.lbl_nm)\n",
    "        preds = _get_preds(learn.preds, self.preds_nm)\n",
    "        learn.loss = learn.loss_func(preds, *lbls)\n",
    "\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MomentumLearner(TrainLearner):\n",
    "    def __init__(self, model, dls, loss_func, lr=None, cbs=None, opt_func=torch.optim.SGD, epoch_sz=None,\n",
    "                 n_inp=1, inp_nm=None, lbl_nm=None, preds_nm=None, mom=0.85):\n",
    "        self.mom = mom\n",
    "        super().__init__(model, dls, loss_func, lr, cbs, opt_func=opt_func, epoch_sz=epoch_sz, n_inp=n_inp,\n",
    "                        inp_nm=inp_nm, lbl_nm=lbl_nm, preds_nm=preds_nm)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters():\n",
    "                if p.grad is not None:\n",
    "                    p.grad.detach_()\n",
    "                    p.grad *= self.mom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeviceCB - makes sure everything is on the right device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): fc.store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you only want to run a single batch, for example when debugging. This callback allows you to do that, making use of the CancelFitException:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SingleBatchCB(Callback):\n",
    "    order = 1\n",
    "    def after_batch(self, learn): raise CancelFitException()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rely on torcheval metrics for calculating metrics. By defauly, adding a MetricsCB will track the loss. But you can feed in other metrics as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MetricsCB(Callback):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        for o in ms: metrics[type(o).__name__] = o\n",
    "        self.metrics = metrics\n",
    "        self.all_metrics = copy(metrics)\n",
    "        self.all_metrics['loss'] = self.loss = Mean()\n",
    "\n",
    "    def _log(self, d): print(d)\n",
    "    def before_fit(self, learn): learn.metrics = self\n",
    "    def before_epoch(self, learn):\n",
    "        for o in self.all_metrics.values(): o.reset()\n",
    "        self.start_time = time()\n",
    "\n",
    "    def after_epoch(self, learn):\n",
    "        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}\n",
    "        log['epoch'] = learn.epoch\n",
    "        log['train'] = 'train' if learn.model.training else 'eval'\n",
    "        log['time'] = format_time(time() - self.start_time)\n",
    "        self._log(log)\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProgressCB is a simple callback that prints out the progress of training. It's not very sophisticated, but it's useful for debugging. Any metrics being tracked via MetricsCB will be printed out as well. And you can set plot=True to get a plot of the loss during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProgressCB(Callback):\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, plot=False): self.plot = plot\n",
    "    def before_fit(self, learn):\n",
    "        learn.epochs = self.mbar = master_bar(learn.epochs)\n",
    "        self.first = True\n",
    "        if hasattr(learn, 'metrics'): learn.metrics._log = self._log\n",
    "        self.losses = []\n",
    "\n",
    "    def _log(self, d):\n",
    "        if self.first:\n",
    "            self.mbar.write(list(d), table=True)\n",
    "            self.first = False\n",
    "        self.mbar.write(list(d.values()), table=True)\n",
    "\n",
    "    def before_epoch(self, learn): learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)\n",
    "    def after_batch(self, learn):\n",
    "        learn.dl.comment = f'{learn.loss:.3f}'\n",
    "        if self.plot and hasattr(learn, 'metrics') and learn.training:\n",
    "            self.losses.append(learn.loss.item())\n",
    "            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are callbacks for all sorts of things, here are some common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [\n",
    "    TrainCB(), # Handles the core steps in the training loop\n",
    "    DeviceCB(), # Puts data and model on GPU\n",
    "    MetricsCB(accuracy=MulticlassAccuracy())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try a very basic convnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(nin,nout):\n",
    "    return [nn.Conv2d(nin, nout, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(), nn.BatchNorm2d(nout)]\n",
    "\n",
    "def get_model():\n",
    "    return nn.Sequential(*conv(1, 16), *conv(16,32), *conv(32,32),\n",
    "                         nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = partial(torch.optim.AdamW, betas=(0.9,0.95), eps=1e-5)\n",
    "learn = Learner(get_model(), dls, nn.CrossEntropyLoss(), lr=0.05, cbs=cbs, opt_func=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, cbs=[ProgressCB(plot=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CapturePreds(Callback):\n",
    "    def before_fit(self, learn): self.all_inps,self.all_preds,self.all_targs = [],[],[]\n",
    "    def after_batch(self, learn):\n",
    "        self.all_inps. append(to_cpu(learn.batch[0]))\n",
    "        self.all_preds.append(to_cpu(learn.preds))\n",
    "        self.all_targs.append(to_cpu(learn.batch[1]))\n",
    "    def after_fit(self, learn):\n",
    "        self.all_preds,self.all_targs,self.all_inps = map(torch.cat, [self.all_preds,self.all_targs,self.all_inps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "def capture_preds(self: Learner, cbs=None, inps=False):\n",
    "    cp = CapturePreds()\n",
    "    with torch.inference_mode(): self.fit(1, train=False, cbs=[cp]+fc.L(cbs))\n",
    "    res = cp.all_preds,cp.all_targs\n",
    "    if inps: res = res+(cp.all_inps,)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,targs = learn.capture_preds()\n",
    "preds.shape,targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(preds.argmax(1)==targs).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want logging more often than the end of each full pass through the data loader, set `epoch_sz` to be the number of batches you'd like to count as an \"epoch\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(get_model(), dls, nn.CrossEntropyLoss(), lr=0.05,\n",
    "                cbs=cbs, opt_func=opt, epoch_sz=10)\n",
    "learn.fit(3, cbs=[ProgressCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@fc.patch\n",
    "@fc.delegates(show_images)\n",
    "def show_image_batch(self:Learner, max_n=9, cbs=None, **kwargs):\n",
    "    self.fit(1, cbs=[SingleBatchCB()]+fc.L(cbs))\n",
    "    xb,yb = self.batch\n",
    "    feat = fc.nested_attr(self.dls, 'train.dataset.features')\n",
    "    if feat is None: titles = np.array(yb)\n",
    "    else:\n",
    "        names = feat['label'].names\n",
    "        titles = [names[i] for i in yb]\n",
    "    show_images(xb[:max_n], titles=titles[:max_n], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_image_batch(figsize=(3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful method from FastAI, the learning rate finder tries batches at ever-increasing LRs and plots the loss. It's a good way to get a sense of what LR to use for training. Note that we also patch in the `lr_find` method to the Learner class, so you can call it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.sched = ExponentialLR(learn.opt, self.gamma)\n",
    "        self.lrs,self.losses = [],[]\n",
    "        self.min = math.inf\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: raise CancelEpochException()\n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(learn.loss)\n",
    "        self.losses.append(loss)\n",
    "        if loss < self.min: self.min = loss\n",
    "        if loss > self.min*self.max_mult:\n",
    "            raise CancelFitException()\n",
    "        self.sched.step()\n",
    "\n",
    "    def cleanup_fit(self, learn):\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):\n",
    "    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(get_model(), dls, nn.CrossEntropyLoss(), lr=0.1, cbs=cbs, opt_func=opt)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RecorderCB(Callback):\n",
    "    def __init__(self, **d): self.d = d\n",
    "    def before_fit(self, learn):\n",
    "        self.recs = {k:[] for k in self.d}\n",
    "        self.pg = learn.opt.param_groups[0]\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        if not learn.training: return\n",
    "        for k,v in self.d.items():\n",
    "            self.recs[k].append(v(self))\n",
    "\n",
    "    def plot(self):\n",
    "        for k,v in self.recs.items():\n",
    "            plt.plot(v, label=k)\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BaseSchedCB(Callback):\n",
    "    def __init__(self, sched): self.sched = sched\n",
    "    def before_fit(self, learn): self.schedo = self.sched(learn.opt)\n",
    "    def _step(self, learn):\n",
    "        if learn.training: self.schedo.step()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export  \n",
    "class BatchSchedCB(BaseSchedCB):\n",
    "    def after_batch(self, learn): self._step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class EpochSchedCB(BaseSchedCB):\n",
    "    def after_epoch(self, learn): self._step(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class HasLearnCB(Callback):\n",
    "    def before_fit(self, learn): self.learn = learn \n",
    "    def after_fit(self, learn): self.learn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class MixedPrecision(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, dtype=torch.bfloat16):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def before_fit(self, learn): self.scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=self.dtype)\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "        \n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MixedPrecision(), DeviceCB(), MetricsCB(accuracy=MulticlassAccuracy())]\n",
    "learn = Learner(get_model(), dls, nn.CrossEntropyLoss(), lr=0.05, cbs=cbs, opt_func=opt)\n",
    "learn.fit(1, cbs=[ProgressCB(plot=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AccelerateCB(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.acc = Accelerator(mixed_precision=mixed_precision)\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n",
    "            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n",
    "    \n",
    "    def after_fit(self, learn): learn.model = self.acc.unwrap_model(learn.model)\n",
    "    def backward(self, learn): self.acc.backward(learn.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def append_stats(hook, mod, inp, outp):\n",
    "    if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "    acts = to_cpu(outp).float()\n",
    "    hook.stats[0].append(acts.mean())\n",
    "    hook.stats[1].append(acts.std())\n",
    "    hook.stats[2].append(acts.abs().histc(40,0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_min(h):\n",
    "    h1 = torch.stack(h.stats[2]).t().float()\n",
    "    return h1[0]/h1.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hook():\n",
    "    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Hooks(list):\n",
    "    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "    def remove(self):\n",
    "        for h in self: h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HooksCallback(Callback):\n",
    "    def __init__(self, hookfunc, mod_filter=fc.noop, on_train=True, on_valid=False, mods=None):\n",
    "        fc.store_attr()\n",
    "        super().__init__()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        if self.mods: mods=self.mods\n",
    "        else: mods = fc.filter_ex(learn.model.modules(), self.mod_filter)\n",
    "        self.hooks = Hooks(mods, partial(self._hookfunc, learn))\n",
    "\n",
    "    def _hookfunc(self, learn, *args, **kwargs):\n",
    "        if (self.on_train and learn.training) or (self.on_valid and not learn.training): self.hookfunc(*args, **kwargs)\n",
    "\n",
    "    def after_fit(self, learn): self.hooks.remove()\n",
    "    def __iter__(self): return iter(self.hooks)\n",
    "    def __len__(self): return len(self.hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Thanks to @ste for initial version of histgram plotting code\n",
    "def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ActivationStats(HooksCallback):\n",
    "    def __init__(self, mod_filter=fc.noop): super().__init__(append_stats, mod_filter)\n",
    "\n",
    "    def color_dim(self, figsize=(11,5)):\n",
    "        fig,axes = get_grid(len(self), figsize=figsize)\n",
    "        for ax,h in zip(axes.flat, self):\n",
    "            show_image(get_hist(h), ax, origin='lower')\n",
    "\n",
    "    def dead_chart(self, figsize=(11,5)):\n",
    "        fig,axes = get_grid(len(self), figsize=figsize)\n",
    "        for ax,h in zip(axes.flatten(), self):\n",
    "            ax.plot(get_min(h))\n",
    "            ax.set_ylim(0,1)\n",
    "\n",
    "    def plot_stats(self, figsize=(10,4)):\n",
    "        fig,axs = plt.subplots(1,2, figsize=figsize)\n",
    "        for h in self:\n",
    "            for i in 0,1: axs[i].plot(h.stats[i])\n",
    "        axs[0].set_title('Means')\n",
    "        axs[1].set_title('Stdevs')\n",
    "        plt.legend(fc.L.range(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astats = ActivationStats(fc.risinstance(nn.Conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(get_model(), dls, nn.CrossEntropyLoss(), lr=0.05, cbs=cbs, opt_func=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, cbs=[ProgressCB(), astats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('image', cmap='viridis')\n",
    "astats.color_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.plot_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astats.dead_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _flops(x, h, w):\n",
    "    if x.dim()<3: return x.numel()\n",
    "    if x.dim()==4: return x.numel()*h*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@fc.patch\n",
    "def summary(self:Learner):\n",
    "    res = '|Module|Input|Output|Num params|MFLOPS|\\n|--|--|--|--|--|\\n'\n",
    "    totp,totf = 0,0\n",
    "    def _f(hook, mod, inp, outp):\n",
    "        nonlocal res,totp,totf\n",
    "        nparms = sum(o.numel() for o in mod.parameters())\n",
    "        totp += nparms\n",
    "        *_,h,w = outp.shape\n",
    "        flops = sum(_flops(o, h, w) for o in mod.parameters())/1e6\n",
    "        totf += flops\n",
    "        res += f'|{type(mod).__name__}|{tuple(inp[0].shape)}|{tuple(outp.shape)}|{nparms}|{flops:.1f}|\\n'\n",
    "    with Hooks(self.model, _f) as hooks: self.fit(1, lr=1, cbs=SingleBatchCB())\n",
    "    print(f\"Tot params: {totp}; MFLOPS: {totf:.1f}\")\n",
    "    if fc.IN_NOTEBOOK:\n",
    "        from IPython.display import Markdown\n",
    "        return Markdown(res)\n",
    "    else: print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BatchTransformCB(Callback):\n",
    "    def __init__(self, tfm, on_train=True, on_val=True): fc.store_attr()\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        if (self.on_train and learn.training) or (self.on_val and not learn.training):\n",
    "            learn.batch = self.tfm(learn.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(b): return (b[0]-xmean)/xstd,b[1]\n",
    "norm = BatchTransformCB(_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x -= self.sub\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_erase1(x, pct, xm, xs, mn, mx):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    nn.init.normal_(x[:,:,stx:stx+szx,sty:sty+szy], mean=xm, std=xs)\n",
    "    x.clamp_(mn, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_erase(x, pct=0.2, min_num=0, max_num = 4):\n",
    "    xm,xs,mn,mx = x.mean(),x.std(),x.min(),x.max()\n",
    "    num = random.randint(min_num, max_num)\n",
    "    for i in range(num): _rand_erase1(x, pct, xm, xs, mn, mx)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'gray_r'\n",
    "xb,_ = next(iter(dls.train))\n",
    "xbt = xb[:9]\n",
    "rand_erase(xbt, 0.2, 3, 3)\n",
    "show_images(xbt, imsize=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandErase(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_erase(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _rand_copy1(x, pct):\n",
    "    szx = int(pct*x.shape[-2])\n",
    "    szy = int(pct*x.shape[-1])\n",
    "    stx1 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty1 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    stx2 = int(random.random()*(1-pct)*x.shape[-2])\n",
    "    sty2 = int(random.random()*(1-pct)*x.shape[-1])\n",
    "    x[:,:,stx1:stx1+szx,sty1:sty1+szy] = x[:,:,stx2:stx2+szx,sty2:sty2+szy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rand_copy(x, pct=0.2, min_num=0, max_num=4):\n",
    "    num = random.randint(min_num, max_num)\n",
    "    for i in range(num): _rand_copy1(x, pct)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbt = xb[:9]\n",
    "xb,_ = next(iter(dls.train))\n",
    "rand_copy(xbt, 0.2, 3, 3)\n",
    "show_images(xbt, imsize=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class RandCopy(nn.Module):\n",
    "    def __init__(self, pct=0.2, max_num=4):\n",
    "        super().__init__()\n",
    "        self.pct,self.max_num = pct,max_num\n",
    "    def forward(self, x): return rand_copy(x, self.pct, self.max_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_ipython_hist():\n",
    "    # Code in this function mainly copied from IPython source\n",
    "    if not 'get_ipython' in globals(): return\n",
    "    ip = get_ipython()\n",
    "    user_ns = ip.user_ns\n",
    "    ip.displayhook.flush()\n",
    "    pc = ip.displayhook.prompt_count + 1\n",
    "    for n in range(1, pc): user_ns.pop('_i'+repr(n),None)\n",
    "    user_ns.update(dict(_i='',_ii='',_iii=''))\n",
    "    hm = ip.history_manager\n",
    "    hm.input_hist_parsed[:] = [''] * pc\n",
    "    hm.input_hist_raw[:] = [''] * pc\n",
    "    hm._i = hm._ii = hm._iii = hm._i00 =  ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_tb():\n",
    "    # h/t Piotr Czapla\n",
    "    if hasattr(sys, 'last_traceback'):\n",
    "        traceback.clear_frames(sys.last_traceback)\n",
    "        delattr(sys, 'last_traceback')\n",
    "    if hasattr(sys, 'last_type'): delattr(sys, 'last_type')\n",
    "    if hasattr(sys, 'last_value'): delattr(sys, 'last_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def clean_mem():\n",
    "    clean_tb()\n",
    "    clean_ipython_hist()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import nbdev_export; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
