{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence packer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seuqnce packing for more efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modernbert Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://github.dev/AnswerDotAI/ModernBERT/blob/main/src/sequence_packer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "import math, random\n",
    "\n",
    "# from composer.core.types import Batch\n",
    "# from composer.core import Time\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens,\n",
    "        # warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            # self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "            pass\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens = None,\n",
    "        # batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens = None,\n",
    "        # batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()\n",
    "\n",
    "\n",
    "def split_packed_batch(batch: Any, microbatch_size: Union[int, float], padding_tolerance=1.0) -> Sequence:\n",
    "    # NOTE: Packed sequences are already packed into a microbatch size worth of tokens.\n",
    "    # So to correctly return a microbatch worth of data, we will simply return each item (i.e. microbatch_size 1)\n",
    "\n",
    "    num_items = batch[\"input_ids\"].shape[0]\n",
    "    split_inputs = [x.squeeze() for x in batch[\"input_ids\"].split(1)]\n",
    "    split_labels = [x.squeeze() for x in batch[\"labels\"].split(1)]\n",
    "    split_attention_masks = [x.squeeze() for x in batch[\"attention_mask\"].split(1)]\n",
    "    split_cu_seqlens = batch[\"cu_seqlens\"]\n",
    "\n",
    "    result = []\n",
    "    for i in range(num_items):\n",
    "        attention_mask = split_attention_masks[i]\n",
    "        padding_amount = 1 - (attention_mask.sum() / len(attention_mask))\n",
    "\n",
    "        if padding_amount > padding_tolerance:\n",
    "            last_non_pad = attention_mask.nonzero().max()\n",
    "            input_ids = split_inputs[i][: last_non_pad + 1]\n",
    "            labels = split_labels[i][: last_non_pad + 1]\n",
    "            cu_seqlens = split_cu_seqlens[i][:-1]\n",
    "            attention_mask = attention_mask[: last_non_pad + 1]\n",
    "        else:\n",
    "            input_ids = split_inputs[i]\n",
    "            labels = split_labels[i]\n",
    "            cu_seqlens = split_cu_seqlens[i]\n",
    "\n",
    "        result.append(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"labels\": labels,\n",
    "                \"cu_seqlens\": cu_seqlens,\n",
    "                \"max_seqlen\": batch[\"max_seqlen\"][i],\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    assert all([x[\"input_ids\"].shape[-1] == y[\"cu_seqlens\"][-1] for x, y in zip(result, result)])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_num_samples_in_packed_batch(batch) -> int:\n",
    "    # Number of sequences can be inferred from cu_seqlens arrays\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    if isinstance(cu_seqlens, torch.Tensor):\n",
    "        return cu_seqlens.size()[0] - 1\n",
    "    elif isinstance(cu_seqlens, list):\n",
    "        return sum([x.size()[0] - 1 for x in batch[\"cu_seqlens\"]])\n",
    "    else:\n",
    "        raise TypeError('Expected a batch with a \"cu_seqlens\" key of type list or Tensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "I'll show you a simple example of how to use the SequencePacker, specifically the GreedyBestFitSequencePacker implementation. Let's create a minimal working example:\n",
    "\n",
    "```python\n",
    "# First, let's create some sample data\n",
    "import random\n",
    "\n",
    "# Create a simple dataset of variable length sequences\n",
    "class SimpleDataset:\n",
    "    def __init__(self, num_samples=100, min_len=10, max_len=100, vocab_size=1000):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq_len = random.randint(min_len, max_len)\n",
    "            self.samples.append({\"input_ids\": [random.randint(1, vocab_size-1) for _ in range(seq_len)]})\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "# Create a batch sampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SimpleDataset(num_samples=100)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Create the sequence packer\n",
    "packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=dataloader,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=100,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=200,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.15,\n",
    "    suppress_masking=False\n",
    ")\n",
    "\n",
    "# Use the packer\n",
    "for i, batch in enumerate(packer):\n",
    "    if i >= 3: break  # Just show first few batches\n",
    "    \n",
    "    # Unpack the batch\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {input_ids.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in cu_seqlens]}\")\n",
    "    print(f\"  Sequences per packed sequence: {[len(s)-1 for s in cu_seqlens]}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "This example:\n",
    "\n",
    "1. Creates a simple dataset with variable length sequences\n",
    "2. Sets up a DataLoader to batch the sequences\n",
    "3. Creates a GreedyBestFitSequencePacker that:\n",
    "   - Takes batches of 8 sequences from the DataLoader\n",
    "   - Produces output batches with 2 packed sequences, each with length 200\n",
    "   - Uses a buffer size of 32 sequences\n",
    "   - Applies 15% masking for MLM training\n",
    "4. Iterates through the packed batches and prints information about each one\n",
    "\n",
    "The key benefit of the sequence packer is efficiently packing variable-length sequences into fixed-length batches, which maximizes GPU utilization for training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, num_samples=100, min_len=10, max_len=100, vocab_size=1000):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq_len = random.randint(min_len, max_len)\n",
    "            self.samples.append({\"input_ids\": [random.randint(1, vocab_size-1) for _ in range(seq_len)]})\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3,\n",
       "  24,\n",
       "  18,\n",
       "  9,\n",
       "  21,\n",
       "  7,\n",
       "  16,\n",
       "  47,\n",
       "  42,\n",
       "  33,\n",
       "  41,\n",
       "  44,\n",
       "  43,\n",
       "  4,\n",
       "  16,\n",
       "  42,\n",
       "  5,\n",
       "  11]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = SimpleDataset(num_samples=5, min_len=10, max_len=50, vocab_size=50)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 23, 21, 34, 29]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(ds[i]['input_ids']) for i in range(len(ds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[18, 34, 23, 21, 29]\n"
     ]
    }
   ],
   "source": [
    "def simple_collate(batch):\n",
    "    return batch\n",
    "\n",
    "dl = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "batch = next(iter(dl))\n",
    "print(len(batch))\n",
    "print([len(item['input_ids']) for item in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 100])\n",
      "  Labels shape: torch.Size([2, 100])\n",
      "  Cumulative sequence lengths: [[0, 21, 43, 68, 90, 100], [0, 38, 74, 99, 100]]\n",
      "  Sequences per packed sequence: [5, 4]\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 100])\n",
      "  Labels shape: torch.Size([2, 100])\n",
      "  Cumulative sequence lengths: [[0, 30, 70, 92, 100], [0, 46, 92, 100]]\n",
      "  Sequences per packed sequence: [4, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "larger_ds = SimpleDataset(num_samples=50, min_len=20, max_len=50, vocab_size=1000)\n",
    "larger_dl = DataLoader(larger_ds, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "smaller_packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,  # Match our max sequence length\n",
    "    out_batch_size=2,    # Create 2 packed sequences per batch\n",
    "    out_pseq_len=100,    # Each packed sequence can hold ~2-5 of our sequences\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.15,\n",
    "    suppress_masking=False\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(smaller_packer):\n",
    "    if i >= 2: break\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {input_ids.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in cu_seqlens]}\")\n",
    "    print(f\"  Sequences per packed sequence: {[len(s)-1 for s in cu_seqlens]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[601, 650, 344, 466,  94, 287,  43,   0,  88, 304, 625,  33, 245, 453,\n",
       "         753, 186, 158, 154, 121,   0, 444, 378, 197, 924, 330,  24, 466,   0,\n",
       "         467, 633, 200, 691,   0, 862,   0,   0, 733, 243, 552, 132, 947, 841,\n",
       "           0, 681, 512,  73,   0,  81, 262,   0, 168, 639, 122, 366, 556, 429,\n",
       "         986, 800, 170, 152, 466, 351,  42,   0, 150, 792, 696, 671, 784, 794,\n",
       "         410, 245, 995, 288,  18, 814, 867, 742, 924, 392,   0, 213,   0, 814,\n",
       "         712, 482, 177, 925, 118, 713, 130, 755, 469,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1],\n",
       "        [340, 325, 653, 336, 724, 876, 771, 572, 199, 529, 931, 658, 691,  29,\n",
       "         516, 726, 913,   0, 742, 360, 906, 390,   0, 731,   0,   0, 843,   0,\n",
       "         147, 675, 136,   0,   0, 545,   0, 597, 462, 537,  85, 872,   0,   0,\n",
       "         119,   0, 371, 407, 302, 357, 607, 180, 281, 784, 969, 582, 883, 304,\n",
       "         496, 924,   0, 738, 891,  59,   0, 660, 254,   0, 464,   0,   0, 999,\n",
       "          35, 863, 344, 471,   0, 724, 783, 499, 600, 717, 564, 253, 241, 972,\n",
       "         695, 367, 171, 719, 954,  -1,  -1,  -1,  -1,   0,   0,  -1,   0,  -1,\n",
       "          -1,  -1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100,   10, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,  762,   17, -100, -100, -100, -100,\n",
       "         -100, -100, -100,  908, -100, -100, -100, -100,  991, -100,  199,  661,\n",
       "         -100, -100, -100, -100, -100, -100,  809, -100, -100, -100,  187, -100,\n",
       "         -100,  567, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100,  512, -100, -100, -100, -100, -100, -100,  410, -100,\n",
       "         -100, -100, -100, -100, -100, -100,  489, -100,  725, -100,  653, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100,  821, -100, -100, -100, -100,  165, -100,\n",
       "          922,  221, -100,  696, -100, -100, -100,  161,   90, -100,  711, -100,\n",
       "         -100, -100, -100, -100,  535,  784, -100,  101, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   23, -100,\n",
       "         -100, -100,  119,  660, -100,  827, -100,  467,  966, -100, -100, -100,\n",
       "         -100, -100,  963, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  0,  23,  67,  93, 100], dtype=torch.int32),\n",
       " tensor([  0,  42,  89, 100], dtype=torch.int32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"cu_seqlens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "What are input_ids, labels, and cu_seqlens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Looking at the output from the SequencePacker, let me explain each of these components:\n",
    "\n",
    "1. **input_ids**: \n",
    "   - This is a tensor containing the packed token sequences\n",
    "   - Shape: [out_batch_size, out_pseq_len] (in this case [2, 100])\n",
    "   - Contains the actual token IDs, with some tokens masked (replaced with mask_token_id=0)\n",
    "   - Padding tokens are represented by pad_token_id=-1\n",
    "\n",
    "2. **labels**:\n",
    "   - This is a tensor used for the masked language modeling (MLM) training objective\n",
    "   - Shape: Same as input_ids [2, 100]\n",
    "   - Contains:\n",
    "     - The original token IDs at positions that were masked\n",
    "     - ignore_token_id (-100) at positions that weren't masked\n",
    "     - The model is trained to predict only the masked tokens during training\n",
    "\n",
    "3. **cu_seqlens** (cumulative sequence lengths):\n",
    "   - A list of tensors (one per packed sequence) that tells you where each original sequence begins and ends within the packed sequence\n",
    "   - Each tensor contains indices that mark the boundaries between sequences\n",
    "   - For example, [0, 40, 83, 100] means:\n",
    "     - First sequence: positions 0-39 (40 tokens)\n",
    "     - Second sequence: positions 40-82 (43 tokens)\n",
    "     - Third sequence: positions 83-99 (17 tokens)\n",
    "     - The last value (100) is the total length of the packed sequence\n",
    "\n",
    "The packer efficiently combines multiple variable-length sequences into fixed-length packed sequences, keeping track of where each original sequence begins and ends. This improves training efficiency by reducing padding waste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "I want to use sequence packer for autoregressive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "For using the SequencePacker with an autoregressive model, you'll need to modify a few things. The current implementation is designed for masked language modeling (MLM) rather than autoregressive training.\n",
    "\n",
    "Here's how you can adapt it for autoregressive models:\n",
    "\n",
    "```python\n",
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # For autoregressive models:\n",
    "            # - inputs are all tokens except the last one\n",
    "            # - labels are all tokens except the first one\n",
    "            inputs = input_ids.clone()\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (inputs != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, create proper labels\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set labels to ignore_token_id at sequence boundaries\n",
    "                    if j < len(cu_seqlens) - 2:  # Not the last sequence\n",
    "                        labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Create the causal mask for autoregressive attention\n",
    "            # Each token can only attend to itself and previous tokens\n",
    "            seq_len = inputs.size(1)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"causal_mask\": causal_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n",
    "```\n",
    "\n",
    "And here's how you'd use it:\n",
    "\n",
    "```python\n",
    "autoregressive_packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=100,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(autoregressive_packer):\n",
    "    if i >= 1: break\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  Causal mask shape: {batch['causal_mask'].shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in batch['cu_seqlens']]}\")\n",
    "```\n",
    "\n",
    "This implementation:\n",
    "1. Inherits from GreedyBestFitSequencePacker but turns off masking\n",
    "2. Overrides _generate_batches to create proper inputs and labels for autoregressive training\n",
    "3. Adds a causal attention mask to ensure tokens only attend to previous tokens\n",
    "4. Sets labels at sequence boundaries to ignore_token_id to avoid the model learning to predict across sequence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Create the shifted inputs (tokens 0 to n-1) and labels (tokens 1 to n)\n",
    "            inputs = input_ids.clone()\n",
    "            # For labels, we shift by 1 (each position predicts the next token)\n",
    "            labels = torch.roll(input_ids, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (inputs != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set the label at the last position of each sequence to ignore_token_id\n",
    "                    # We don't want to predict across sequence boundaries\n",
    "                    labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Create the causal mask for autoregressive attention\n",
    "            seq_len = inputs.size(1)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"causal_mask\": causal_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregressive_packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=100,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(autoregressive_packer):\n",
    "    if i >= 1: break\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  Causal mask shape: {batch['causal_mask'].shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in batch['cu_seqlens']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['causal_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "Does it look good to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Yes, the implementation looks very good! The AutoregressiveSequencePacker is correctly configured for autoregressive training:\n",
    "\n",
    "1. **Input/Label Shifting**: The labels are properly shifted (using `torch.roll`) so that each position predicts the next token.\n",
    "\n",
    "2. **Sequence Boundary Handling**: The code correctly identifies the end of each sequence (using `cu_seqlens`) and sets the label at those positions to `ignore_token_id` (-100). This prevents the model from learning to predict across sequence boundaries.\n",
    "\n",
    "3. **Attention Masking**: Two important masks are created:\n",
    "   - `attention_mask`: Distinguishes real tokens (1) from padding tokens (0)\n",
    "   - `causal_mask`: A lower triangular matrix ensuring each token can only attend to itself and previous tokens\n",
    "\n",
    "4. **Packed Sequences**: The original packing functionality is preserved, efficiently combining multiple sequences into fixed-length batches.\n",
    "\n",
    "The output shows everything is working correctly:\n",
    "- The input and label tensors have the expected shapes\n",
    "- The cumulative sequence lengths show we've packed multiple sequences (4 in the first packed sequence, 4 in the second)\n",
    "- The labels are properly shifted, with `-100` at sequence boundaries\n",
    "- The attention mask correctly identifies non-padding tokens\n",
    "- The causal mask has the expected lower triangular structure\n",
    "\n",
    "This implementation will work well for training autoregressive models like GPT with packed sequences, which should improve training efficiency by reducing padding waste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "How do I use this now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Here's how to use the AutoregressiveSequencePacker with a language model like GPT:\n",
    "\n",
    "```python\n",
    "# 1. Create your dataset and dataloader\n",
    "train_dataset = SimpleDataset(num_samples=10000, min_len=20, max_len=200)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=simple_collate\n",
    ")\n",
    "\n",
    "# 2. Create the sequence packer\n",
    "packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=train_dataloader,\n",
    "    src_batch_size=16,\n",
    "    src_max_seq_len=200,\n",
    "    out_batch_size=4,\n",
    "    out_pseq_len=512,\n",
    "    buffer_size=64,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "# 3. Create your model (example with a simple GPT-style model)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=1000,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=6,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "# 4. Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "```\n",
    "\n",
    "For inference with the trained model:\n",
    "\n",
    "```python\n",
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Convert prompt to tensor\n",
    "    input_ids = torch.tensor([prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "# Example prompt (list of token IDs)\n",
    "prompt = [10, 25, 36, 42]\n",
    "generated = generate_text(model, prompt)\n",
    "print(generated)\n",
    "```\n",
    "\n",
    "Key points:\n",
    "1. The packer efficiently combines sequences for better GPU utilization\n",
    "2. The model receives properly formatted inputs with causal masking\n",
    "3. The labels are shifted and have sequence boundaries masked\n",
    "4. The loss is automatically computed only on valid positions (ignoring padding and sequence boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create your dataset and dataloader\n",
    "train_dataset = SimpleDataset(num_samples=10000, min_len=20, max_len=200)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=simple_collate\n",
    ")\n",
    "\n",
    "# 2. Create the sequence packer\n",
    "packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=train_dataloader,\n",
    "    src_batch_size=16,\n",
    "    src_max_seq_len=200,\n",
    "    out_batch_size=4,\n",
    "    out_pseq_len=512,\n",
    "    buffer_size=64,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "# 3. Create your model (example with a simple GPT-style model)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=1000,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=6,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "# 4. Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/home/kappa/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/functional.py\u001b[39m(\u001b[92m2551\u001b[39m)\u001b[36membedding\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m   2549\u001b[39m         \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n",
      "\u001b[32m   2550\u001b[39m         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)\n",
      "\u001b[32m-> 2551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[32m   2552\u001b[39m \n",
      "\u001b[32m   2553\u001b[39m \n",
      "\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Convert prompt to tensor\n",
    "    input_ids = torch.tensor([prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "# Example prompt (list of token IDs)\n",
    "prompt = [10, 25, 36, 42]\n",
    "generated = generate_text(model, prompt)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum token ID in data: 999\n"
     ]
    }
   ],
   "source": [
    "# Print the maximum token ID in your data\n",
    "max_token = 0\n",
    "for batch in packer:\n",
    "    curr_max = batch[\"input_ids\"].max().item()\n",
    "    if curr_max > max_token:\n",
    "        max_token = curr_max\n",
    "    break\n",
    "print(f\"Maximum token ID in data: {max_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in input_ids: tensor([ -1,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         43,  45,  46,  47,  49,  50,  52,  53,  55,  56,  57,  58,  59,  60,\n",
      "         61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  74,  75,  77,\n",
      "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "         92,  93,  94,  95,  96,  98,  99, 100, 101, 102, 104, 105, 106, 107,\n",
      "        108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
      "        123, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
      "        139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 152, 153,\n",
      "        155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169,\n",
      "        170, 171, 172, 173, 174, 176, 179, 180, 181, 182, 183, 184, 186, 187,\n",
      "        188, 189, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204,\n",
      "        205, 206, 207, 208, 209, 210, 211, 212, 214, 215, 216, 217, 218, 221,\n",
      "        223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238,\n",
      "        239, 240, 241, 242, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253,\n",
      "        254, 255, 256, 257, 258, 259, 260, 262, 263, 264, 265, 266, 267, 268,\n",
      "        269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
      "        284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 297, 298, 299, 300,\n",
      "        301, 302, 303, 305, 306, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      "        317, 318, 319, 320, 321, 322, 323, 324, 325, 327, 328, 329, 330, 331,\n",
      "        332, 334, 335, 336, 338, 339, 340, 341, 343, 344, 345, 346, 347, 349,\n",
      "        351, 352, 353, 355, 356, 358, 359, 360, 361, 362, 364, 365, 366, 367,\n",
      "        368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 380, 381, 382,\n",
      "        383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 397,\n",
      "        398, 399, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 412, 413,\n",
      "        414, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
      "        429, 430, 431, 432, 433, 434, 437, 438, 439, 442, 443, 444, 445, 446,\n",
      "        447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 469, 471, 473, 474, 475, 476, 477, 478,\n",
      "        479, 480, 482, 483, 484, 485, 486, 487, 488, 489, 490, 492, 493, 494,\n",
      "        496, 497, 498, 499, 501, 502, 503, 504, 506, 507, 509, 510, 511, 512,\n",
      "        513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 528,\n",
      "        529, 531, 532, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544,\n",
      "        545, 546, 547, 548, 549, 550, 552, 554, 555, 556, 557, 558, 559, 560,\n",
      "        561, 562, 563, 564, 565, 566, 567, 568, 571, 572, 573, 574, 575, 576,\n",
      "        577, 578, 581, 582, 584, 585, 586, 588, 590, 591, 592, 593, 595, 596,\n",
      "        597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 608, 609, 610, 611,\n",
      "        612, 613, 614, 615, 616, 618, 619, 620, 621, 622, 623, 624, 625, 626,\n",
      "        627, 629, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
      "        646, 647, 648, 650, 652, 653, 654, 655, 656, 657, 658, 659, 660, 662,\n",
      "        663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676,\n",
      "        677, 678, 679, 680, 681, 682, 685, 686, 687, 688, 689, 690, 691, 692,\n",
      "        693, 694, 695, 696, 698, 699, 700, 701, 702, 703, 705, 706, 707, 708,\n",
      "        709, 710, 711, 713, 715, 716, 717, 719, 721, 722, 723, 724, 725, 727,\n",
      "        728, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742,\n",
      "        743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,\n",
      "        757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770,\n",
      "        771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784,\n",
      "        785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 797, 798, 799,\n",
      "        800, 804, 805, 806, 807, 808, 809, 810, 812, 813, 814, 816, 817, 818,\n",
      "        819, 820, 821, 822, 823, 825, 826, 829, 830, 831, 832, 833, 834, 835,\n",
      "        836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849,\n",
      "        850, 851, 852, 853, 854, 856, 857, 858, 860, 861, 862, 864, 865, 866,\n",
      "        867, 868, 869, 870, 871, 872, 873, 874, 875, 877, 878, 879, 880, 881,\n",
      "        882, 883, 884, 885, 886, 888, 889, 890, 891, 892, 893, 894, 895, 896,\n",
      "        897, 898, 900, 901, 904, 905, 906, 907, 908, 909, 912, 913, 914, 915,\n",
      "        916, 917, 918, 920, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931,\n",
      "        932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 943, 944, 945, 946,\n",
      "        947, 948, 949, 950, 951, 952, 953, 955, 956, 957, 958, 959, 960, 961,\n",
      "        962, 963, 964, 965, 966, 967, 968, 969, 971, 972, 973, 975, 976, 978,\n",
      "        979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992,\n",
      "        993, 994, 995, 996, 997, 998, 999])\n",
      "Negative values: tensor([-1])\n",
      "Value -1 occurs 25 times\n"
     ]
    }
   ],
   "source": [
    "# Check for negative values besides the padding token\n",
    "for batch in packer:\n",
    "    # Find all unique values\n",
    "    unique_values = torch.unique(batch[\"input_ids\"])\n",
    "    print(f\"Unique values in input_ids: {unique_values}\")\n",
    "    \n",
    "    # Check if there are negative values other than pad_token_id\n",
    "    neg_values = unique_values[unique_values < 0]\n",
    "    print(f\"Negative values: {neg_values}\")\n",
    "    \n",
    "    # Count occurrences of each value\n",
    "    for val in neg_values:\n",
    "        count = (batch[\"input_ids\"] == val).sum().item()\n",
    "        print(f\"Value {val} occurs {count} times\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "# Try explicitly telling the model to ignore padding in attention mask\n",
    "for batch in packer:\n",
    "    # Make sure padding tokens have 0 in attention mask\n",
    "    attention_mask = (batch[\"input_ids\"] != -1).long()\n",
    "    \n",
    "    # Convert -1 padding to 0 (or any valid token ID that's safe to use)\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    inputs[inputs == -1] = 0\n",
    "    \n",
    "    # Fix labels too\n",
    "    labels = batch[\"labels\"].clone()\n",
    "    labels[labels == -1] = -100  # -100 is the standard ignore index for CrossEntropyLoss\n",
    "    \n",
    "    # Try with the fixed inputs\n",
    "    outputs = model(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try without using the custom causal mask\n",
    "for batch in packer:\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    inputs[inputs == -1] = 0\n",
    "    \n",
    "    # GPT models create their own causal mask internally\n",
    "    outputs = model(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        labels=batch[\"labels\"].clone().masked_fill_(batch[\"labels\"] == -1, -100)\n",
    "    )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding weight shape: torch.Size([1000, 768])\n",
      "Model embedding weight indices: min=999, max=0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the model's embedding parameters\n",
    "print(f\"Model embedding weight shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"Model embedding weight indices: min={model.transformer.wte.weight.shape[0]-1}, max={0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Create the shifted inputs (tokens 0 to n-1) and labels (tokens 1 to n)\n",
    "            inputs = input_ids.clone()\n",
    "            # For labels, we shift by 1 (each position predicts the next token)\n",
    "            labels = torch.roll(input_ids, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Handle padding: replace -1 with 0 in inputs\n",
    "            inputs[inputs == self.pad_token_id] = 0\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set the label at the last position of each sequence to ignore_token_id\n",
    "                    # We don't want to predict across sequence boundaries\n",
    "                    labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Handle padding in labels: replace -1 with ignore_token_id\n",
    "            labels[labels == self.pad_token_id] = self.ignore_token_id\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Forward pass - no need for additional conversions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m loss = outputs.loss\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:819\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    816\u001b[39m     position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     inputs_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwte\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    820\u001b[39m position_embeds = \u001b[38;5;28mself\u001b[39m.wpe(position_ids)\n\u001b[32m    821\u001b[39m hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass - no need for additional conversions\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in inputs after replacing padding: 999\n",
      "Still encountering error: Target -1 is out of bounds.\n"
     ]
    }
   ],
   "source": [
    "# Debug the exact values that might be causing issues\n",
    "for batch in packer:\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Replace pad tokens with 0\n",
    "    inputs[inputs == packer.pad_token_id] = 0\n",
    "    \n",
    "    # Check for any values >= vocab_size\n",
    "    max_val = inputs.max().item()\n",
    "    print(f\"Maximum value in inputs after replacing padding: {max_val}\")\n",
    "    \n",
    "    # Check if there are any values outside the vocabulary range\n",
    "    invalid_indices = (inputs >= 1000).nonzero()\n",
    "    if len(invalid_indices) > 0:\n",
    "        print(f\"Found {len(invalid_indices)} invalid indices\")\n",
    "        print(f\"Invalid values: {inputs[invalid_indices[:, 0], invalid_indices[:, 1]]}\")\n",
    "    \n",
    "    # Try explicitly setting the range of values\n",
    "    valid_inputs = inputs.clamp(0, 999)\n",
    "    \n",
    "    # Check if there are still any issues\n",
    "    try:\n",
    "        outputs = model(\n",
    "            input_ids=valid_inputs.to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            labels=batch[\"labels\"].clamp(-100, 999).to(device)\n",
    "        )\n",
    "        print(\"Success with clamped values!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Still encountering error: {e}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test successful! Loss: 7.109584331512451\n"
     ]
    }
   ],
   "source": [
    "# Create a very simple input to test the model directly\n",
    "test_input = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "test_labels = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "test_labels[:, -1] = -100  # Ignore last token\n",
    "test_mask = torch.ones_like(test_input).to(device)\n",
    "\n",
    "try:\n",
    "    test_output = model(\n",
    "        input_ids=test_input,\n",
    "        attention_mask=test_mask,\n",
    "        labels=test_labels\n",
    "    )\n",
    "    print(f\"Test successful! Loss: {test_output.loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Test failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedAutoregressive(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, vocab_size=1000, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            # Get the input_ids and create a valid version\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            valid_inputs = input_ids.clone()\n",
    "            valid_inputs[valid_inputs == self.pad_token_id] = 0\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # Create shifted labels for autoregressive prediction\n",
    "            labels = torch.roll(valid_inputs, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Set padding positions in labels to -100 (not -1)\n",
    "            padding_mask = (input_ids == self.pad_token_id)\n",
    "            labels[padding_mask] = -100\n",
    "            \n",
    "            # Handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    end = cu_seqlens[j+1].item()\n",
    "                    if end > 0 and end < labels.size(1):\n",
    "                        labels[i, end-1] = -100  # Don't predict across sequence boundaries\n",
    "            \n",
    "            # Double-check no -1 values remain in labels\n",
    "            labels[labels == -1] = -100\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": valid_inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_batch(batch, vocab_size=1000):\n",
    "    \"\"\"Check for any values that might cause issues with the model\"\"\"\n",
    "    inputs = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    # Check input values\n",
    "    if inputs.min().item() < 0:\n",
    "        print(f\"Warning: inputs contain values < 0: {inputs.min().item()}\")\n",
    "    if inputs.max().item() >= vocab_size:\n",
    "        print(f\"Warning: inputs contain values >= vocab_size: {inputs.max().item()}\")\n",
    "    \n",
    "    # Check label values\n",
    "    valid_label_mask = (labels != -100)\n",
    "    if valid_label_mask.sum() > 0:\n",
    "        valid_labels = labels[valid_label_mask]\n",
    "        if valid_labels.min().item() < 0:\n",
    "            print(f\"Warning: labels contain values < 0 (excluding -100): {valid_labels.min().item()}\")\n",
    "        if valid_labels.max().item() >= vocab_size:\n",
    "            print(f\"Warning: labels contain values >= vocab_size: {valid_labels.max().item()}\")\n",
    "    \n",
    "    # Check for -1 values\n",
    "    if (inputs == -1).any():\n",
    "        print(f\"Warning: inputs contain -1 values: {(inputs == -1).sum().item()}\")\n",
    "    if (labels == -1).any():\n",
    "        print(f\"Warning: labels contain -1 values: {(labels == -1).sum().item()}\")\n",
    "    \n",
    "    return \"All checks passed!\" if not (\n",
    "        inputs.min().item() < 0 or \n",
    "        inputs.max().item() >= vocab_size or \n",
    "        (valid_label_mask.sum() > 0 and (valid_labels.min().item() < 0 or valid_labels.max().item() >= vocab_size)) or\n",
    "        (inputs == -1).any() or \n",
    "        (labels == -1).any()\n",
    "    ) else \"Issues found!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: inputs contain values < 0: -1\n",
      "Warning: labels contain values < 0 (excluding -100): -1\n",
      "Warning: inputs contain -1 values: 30\n",
      "Warning: labels contain -1 values: 27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Issues found!'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Loss: 7.064399242401123\n"
     ]
    }
   ],
   "source": [
    "# Fix the current batch\n",
    "def fix_batch(batch, vocab_size=1000):\n",
    "    \"\"\"Fix any problematic values in the batch\"\"\"\n",
    "    # Create fixed copies\n",
    "    fixed_inputs = batch[\"input_ids\"].clone()\n",
    "    fixed_labels = batch[\"labels\"].clone()\n",
    "    \n",
    "    # Replace -1 with 0 in inputs\n",
    "    fixed_inputs[fixed_inputs == -1] = 0\n",
    "    \n",
    "    # Replace -1 with -100 in labels\n",
    "    fixed_labels[fixed_labels == -1] = -100\n",
    "    \n",
    "    # Ensure all values are in range\n",
    "    fixed_inputs = fixed_inputs.clamp(0, vocab_size-1)\n",
    "    \n",
    "    # For labels, we only clamp the non-ignore values\n",
    "    valid_mask = (fixed_labels != -100)\n",
    "    if valid_mask.any():\n",
    "        fixed_labels[valid_mask] = fixed_labels[valid_mask].clamp(0, vocab_size-1)\n",
    "    \n",
    "    # Return the fixed batch\n",
    "    return {\n",
    "        \"input_ids\": fixed_inputs,\n",
    "        \"labels\": fixed_labels,\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"cu_seqlens\": batch[\"cu_seqlens\"],\n",
    "        \"max_seqlen\": batch[\"max_seqlen\"]\n",
    "    }\n",
    "\n",
    "# Try with the fixed batch\n",
    "fixed = fix_batch(batch)\n",
    "verify_batch(fixed)\n",
    "\n",
    "# Try running the model with the fixed batch\n",
    "try:\n",
    "    outputs = model(\n",
    "        input_ids=fixed[\"input_ids\"].to(device),\n",
    "        attention_mask=fixed[\"attention_mask\"].to(device),\n",
    "        labels=fixed[\"labels\"].to(device)\n",
    "    )\n",
    "    print(f\"Success! Loss: {outputs.loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, vocab_size=1000, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            # Get the input_ids and create a valid version\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Replace padding with 0 and ensure all values are in range\n",
    "            valid_inputs = input_ids.clone()\n",
    "            valid_inputs[valid_inputs == self.pad_token_id] = 0\n",
    "            valid_inputs = valid_inputs.clamp(0, self.vocab_size-1)\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # Create shifted labels for autoregressive prediction\n",
    "            labels = torch.roll(valid_inputs, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Mark padding positions with ignore_token_id\n",
    "            labels[input_ids == self.pad_token_id] = self.ignore_token_id\n",
    "            \n",
    "            # Handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    end = cu_seqlens[j+1].item()\n",
    "                    if end > 0 and end < labels.size(1):\n",
    "                        labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Final safety check - ensure no -1 values remain\n",
    "            labels[labels == -1] = self.ignore_token_id\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": valid_inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.074440002441406\n",
      "Loss: 7.057838439941406\n",
      "Loss: 7.064158916473389\n",
      "Loss: 7.048040390014648\n",
      "Loss: 7.010573863983154\n",
      "Loss: 7.013488292694092\n",
      "Loss: 7.014577388763428\n",
      "Loss: 6.999983787536621\n",
      "Loss: 6.993265151977539\n",
      "Loss: 6.98762321472168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     21\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        batch = fix_batch(batch)\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass - no need for additional conversions\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
