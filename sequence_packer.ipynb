{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence packer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seuqnce packing for more efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modernbert Source code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://github.dev/AnswerDotAI/ModernBERT/blob/main/src/sequence_packer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "import math, random\n",
    "\n",
    "# from composer.core.types import Batch\n",
    "# from composer.core import Time\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens,\n",
    "        # warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            # self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "            pass\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens = None,\n",
    "        # batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens = None,\n",
    "        # batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()\n",
    "\n",
    "\n",
    "def split_packed_batch(batch: Any, microbatch_size: Union[int, float], padding_tolerance=1.0) -> Sequence:\n",
    "    # NOTE: Packed sequences are already packed into a microbatch size worth of tokens.\n",
    "    # So to correctly return a microbatch worth of data, we will simply return each item (i.e. microbatch_size 1)\n",
    "\n",
    "    num_items = batch[\"input_ids\"].shape[0]\n",
    "    split_inputs = [x.squeeze() for x in batch[\"input_ids\"].split(1)]\n",
    "    split_labels = [x.squeeze() for x in batch[\"labels\"].split(1)]\n",
    "    split_attention_masks = [x.squeeze() for x in batch[\"attention_mask\"].split(1)]\n",
    "    split_cu_seqlens = batch[\"cu_seqlens\"]\n",
    "\n",
    "    result = []\n",
    "    for i in range(num_items):\n",
    "        attention_mask = split_attention_masks[i]\n",
    "        padding_amount = 1 - (attention_mask.sum() / len(attention_mask))\n",
    "\n",
    "        if padding_amount > padding_tolerance:\n",
    "            last_non_pad = attention_mask.nonzero().max()\n",
    "            input_ids = split_inputs[i][: last_non_pad + 1]\n",
    "            labels = split_labels[i][: last_non_pad + 1]\n",
    "            cu_seqlens = split_cu_seqlens[i][:-1]\n",
    "            attention_mask = attention_mask[: last_non_pad + 1]\n",
    "        else:\n",
    "            input_ids = split_inputs[i]\n",
    "            labels = split_labels[i]\n",
    "            cu_seqlens = split_cu_seqlens[i]\n",
    "\n",
    "        result.append(\n",
    "            {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"labels\": labels,\n",
    "                \"cu_seqlens\": cu_seqlens,\n",
    "                \"max_seqlen\": batch[\"max_seqlen\"][i],\n",
    "                \"attention_mask\": attention_mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    assert all([x[\"input_ids\"].shape[-1] == y[\"cu_seqlens\"][-1] for x, y in zip(result, result)])\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_num_samples_in_packed_batch(batch) -> int:\n",
    "    # Number of sequences can be inferred from cu_seqlens arrays\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    if isinstance(cu_seqlens, torch.Tensor):\n",
    "        return cu_seqlens.size()[0] - 1\n",
    "    elif isinstance(cu_seqlens, list):\n",
    "        return sum([x.size()[0] - 1 for x in batch[\"cu_seqlens\"]])\n",
    "    else:\n",
    "        raise TypeError('Expected a batch with a \"cu_seqlens\" key of type list or Tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math, random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, BoolTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLASH_ATTN_AVAILABLE = False\n",
    "try:\n",
    "    from flash_attn import flash_attn_varlen_func\n",
    "\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Flash Attention is not available\n",
    "    pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_collate(batch): return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "I'll show you a simple example of how to use the SequencePacker, specifically the GreedyBestFitSequencePacker implementation. Let's create a minimal working example:\n",
    "\n",
    "```python\n",
    "# First, let's create some sample data\n",
    "import random\n",
    "\n",
    "# Create a simple dataset of variable length sequences\n",
    "class SimpleDataset:\n",
    "    def __init__(self, num_samples=100, min_len=10, max_len=100, vocab_size=1000):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq_len = random.randint(min_len, max_len)\n",
    "            self.samples.append({\"input_ids\": [random.randint(1, vocab_size-1) for _ in range(seq_len)]})\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "# Create a batch sampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = SimpleDataset(num_samples=100)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Create the sequence packer\n",
    "packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=dataloader,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=100,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=200,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.15,\n",
    "    suppress_masking=False\n",
    ")\n",
    "\n",
    "# Use the packer\n",
    "for i, batch in enumerate(packer):\n",
    "    if i >= 3: break  # Just show first few batches\n",
    "    \n",
    "    # Unpack the batch\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {input_ids.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in cu_seqlens]}\")\n",
    "    print(f\"  Sequences per packed sequence: {[len(s)-1 for s in cu_seqlens]}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "This example:\n",
    "\n",
    "1. Creates a simple dataset with variable length sequences\n",
    "2. Sets up a DataLoader to batch the sequences\n",
    "3. Creates a GreedyBestFitSequencePacker that:\n",
    "   - Takes batches of 8 sequences from the DataLoader\n",
    "   - Produces output batches with 2 packed sequences, each with length 200\n",
    "   - Uses a buffer size of 32 sequences\n",
    "   - Applies 15% masking for MLM training\n",
    "4. Iterates through the packed batches and prints information about each one\n",
    "\n",
    "The key benefit of the sequence packer is efficiently packing variable-length sequences into fixed-length batches, which maximizes GPU utilization for training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset:\n",
    "    def __init__(self, num_samples=100, min_len=10, max_len=100, vocab_size=1000):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq_len = random.randint(min_len, max_len)\n",
    "            self.samples.append({\"input_ids\": [random.randint(1, vocab_size-1) for _ in range(seq_len)]})\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [35,\n",
       "  28,\n",
       "  22,\n",
       "  27,\n",
       "  44,\n",
       "  40,\n",
       "  28,\n",
       "  41,\n",
       "  14,\n",
       "  10,\n",
       "  15,\n",
       "  26,\n",
       "  10,\n",
       "  20,\n",
       "  32,\n",
       "  41,\n",
       "  33,\n",
       "  33,\n",
       "  1,\n",
       "  13,\n",
       "  14,\n",
       "  24,\n",
       "  18]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = SimpleDataset(num_samples=5, min_len=10, max_len=50, vocab_size=50)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23, 14, 49, 32, 45]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(ds[i]['input_ids']) for i in range(len(ds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[23, 49, 32, 14, 45]\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "batch = next(iter(dl))\n",
    "print(len(batch))\n",
    "print([len(item['input_ids']) for item in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 100])\n",
      "  Labels shape: torch.Size([2, 100])\n",
      "  Cumulative sequence lengths: [[0, 45, 93, 100], [0, 24, 68, 91, 100]]\n",
      "  Sequences per packed sequence: [3, 4]\n",
      "\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([2, 100])\n",
      "  Labels shape: torch.Size([2, 100])\n",
      "  Cumulative sequence lengths: [[0, 25, 63, 93, 100], [0, 47, 94, 100]]\n",
      "  Sequences per packed sequence: [4, 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "larger_ds = SimpleDataset(num_samples=50, min_len=20, max_len=50, vocab_size=1000)\n",
    "larger_dl = DataLoader(larger_ds, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "smaller_packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,  # Match our max sequence length\n",
    "    out_batch_size=2,    # Create 2 packed sequences per batch\n",
    "    out_pseq_len=100,    # Each packed sequence can hold ~2-5 of our sequences\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.15,\n",
    "    suppress_masking=False\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(smaller_packer):\n",
    "    if i >= 2: break\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {input_ids.shape}\")\n",
    "    print(f\"  Labels shape: {labels.shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in cu_seqlens]}\")\n",
    "    print(f\"  Sequences per packed sequence: {[len(s)-1 for s in cu_seqlens]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[251, 958, 979, 679, 380, 418, 804,   0, 425, 512, 598, 856, 256, 137,\n",
       "         898, 366, 848, 409, 120,   0, 929, 768, 712, 943, 832, 800,  40,   0,\n",
       "         790, 548, 943, 763,   0, 602,   0,   0, 727, 317, 234, 134, 898, 152,\n",
       "           0, 569, 519, 371,   0, 283, 125,   0,  17, 854, 305, 371, 794, 508,\n",
       "         468, 789, 369,  25, 622, 751, 789,   0, 920, 801, 563, 822, 474, 801,\n",
       "         353, 951,  85, 960, 138, 303, 990, 723, 917, 992,   0, 330,   0, 428,\n",
       "         884, 145, 278, 498, 445, 965, 261,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1],\n",
       "        [401, 127, 558, 191, 158, 893, 490,  91, 397, 629, 262, 323,  47, 762,\n",
       "         714, 132,  93,   0, 499, 491, 656, 306,   0,  33,   0,   0, 301,   0,\n",
       "         238, 723, 850,   0,   0, 248,   0, 426,  50, 923, 884, 311,   0,   0,\n",
       "         448,   0, 361, 694, 744, 382, 209, 244, 529,  51, 908, 555, 981, 445,\n",
       "         830, 484,   0, 207, 132, 509,   0, 760, 306,   0, 158,   0,   0,  55,\n",
       "         549, 290, 565, 407,   0, 473, 433, 643, 632, 137,   4, 309, 901, 950,\n",
       "         970,  58, 948,  -1,  -1,  -1,  -1,  -1,  -1,   0,   0,  -1,   0,  -1,\n",
       "          -1,  -1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, -100,  435, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100,  279,   89, -100, -100, -100, -100,\n",
       "         -100, -100, -100,  947, -100, -100, -100, -100,  409, -100,  903,  747,\n",
       "         -100, -100, -100, -100, -100, -100,  219, -100, -100, -100,  428, -100,\n",
       "         -100,  596, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100,   58, -100, -100, -100, -100, -100, -100,  353, -100,\n",
       "         -100, -100, -100, -100, -100, -100,  793, -100,  733, -100,  680, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100,  824, -100, -100, -100, -100,  252, -100,\n",
       "          327,  830, -100,  277, -100, -100, -100,  848,   32, -100,  542, -100,\n",
       "         -100, -100, -100, -100,  418,  685, -100,  681, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,  213, -100,\n",
       "         -100, -100,  474,  760, -100,  148, -100,  542,  487, -100, -100, -100,\n",
       "         -100, -100,  538, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  0,  44,  91, 100], dtype=torch.int32),\n",
       " tensor([  0,  45,  87, 100], dtype=torch.int32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"cu_seqlens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "What are input_ids, labels, and cu_seqlens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Looking at the output from the SequencePacker, let me explain each of these components:\n",
    "\n",
    "1. **input_ids**: \n",
    "   - This is a tensor containing the packed token sequences\n",
    "   - Shape: [out_batch_size, out_pseq_len] (in this case [2, 100])\n",
    "   - Contains the actual token IDs, with some tokens masked (replaced with mask_token_id=0)\n",
    "   - Padding tokens are represented by pad_token_id=-1\n",
    "\n",
    "2. **labels**:\n",
    "   - This is a tensor used for the masked language modeling (MLM) training objective\n",
    "   - Shape: Same as input_ids [2, 100]\n",
    "   - Contains:\n",
    "     - The original token IDs at positions that were masked\n",
    "     - ignore_token_id (-100) at positions that weren't masked\n",
    "     - The model is trained to predict only the masked tokens during training\n",
    "\n",
    "3. **cu_seqlens** (cumulative sequence lengths):\n",
    "   - A list of tensors (one per packed sequence) that tells you where each original sequence begins and ends within the packed sequence\n",
    "   - Each tensor contains indices that mark the boundaries between sequences\n",
    "   - For example, [0, 40, 83, 100] means:\n",
    "     - First sequence: positions 0-39 (40 tokens)\n",
    "     - Second sequence: positions 40-82 (43 tokens)\n",
    "     - Third sequence: positions 83-99 (17 tokens)\n",
    "     - The last value (100) is the total length of the packed sequence\n",
    "\n",
    "The packer efficiently combines multiple variable-length sequences into fixed-length packed sequences, keeping track of where each original sequence begins and ends. This improves training efficiency by reducing padding waste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "I want to use sequence packer for autoregressive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "For using the SequencePacker with an autoregressive model, you'll need to modify a few things. The current implementation is designed for masked language modeling (MLM) rather than autoregressive training.\n",
    "\n",
    "Here's how you can adapt it for autoregressive models:\n",
    "\n",
    "```python\n",
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # For autoregressive models:\n",
    "            # - inputs are all tokens except the last one\n",
    "            # - labels are all tokens except the first one\n",
    "            inputs = input_ids.clone()\n",
    "            labels = input_ids.clone()\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (inputs != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, create proper labels\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set labels to ignore_token_id at sequence boundaries\n",
    "                    if j < len(cu_seqlens) - 2:  # Not the last sequence\n",
    "                        labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Create the causal mask for autoregressive attention\n",
    "            # Each token can only attend to itself and previous tokens\n",
    "            seq_len = inputs.size(1)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"causal_mask\": causal_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n",
    "```\n",
    "\n",
    "And here's how you'd use it:\n",
    "\n",
    "```python\n",
    "autoregressive_packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=100,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(autoregressive_packer):\n",
    "    if i >= 1: break\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  Causal mask shape: {batch['causal_mask'].shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in batch['cu_seqlens']]}\")\n",
    "```\n",
    "\n",
    "This implementation:\n",
    "1. Inherits from GreedyBestFitSequencePacker but turns off masking\n",
    "2. Overrides _generate_batches to create proper inputs and labels for autoregressive training\n",
    "3. Adds a causal attention mask to ensure tokens only attend to previous tokens\n",
    "4. Sets labels at sequence boundaries to ignore_token_id to avoid the model learning to predict across sequence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Create the shifted inputs (tokens 0 to n-1) and labels (tokens 1 to n)\n",
    "            inputs = input_ids.clone()\n",
    "            # For labels, we shift by 1 (each position predicts the next token)\n",
    "            labels = torch.roll(input_ids, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (inputs != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set the label at the last position of each sequence to ignore_token_id\n",
    "                    # We don't want to predict across sequence boundaries\n",
    "                    labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Create the causal mask for autoregressive attention\n",
    "            seq_len = inputs.size(1)\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"causal_mask\": causal_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Input shape: torch.Size([2, 100])\n",
      "  Labels shape: torch.Size([2, 100])\n",
      "  Attention mask shape: torch.Size([2, 100])\n",
      "  Causal mask shape: torch.Size([1, 1, 100, 100])\n",
      "  Cumulative sequence lengths: [[0, 48, 77, 98, 100], [0, 43, 85, 100]]\n"
     ]
    }
   ],
   "source": [
    "autoregressive_packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=larger_dl,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=50,\n",
    "    out_batch_size=2,\n",
    "    out_pseq_len=100,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(autoregressive_packer):\n",
    "    if i >= 1: break\n",
    "    \n",
    "    print(f\"Batch {i}:\")\n",
    "    print(f\"  Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  Causal mask shape: {batch['causal_mask'].shape}\")\n",
    "    print(f\"  Cumulative sequence lengths: {[s.tolist() for s in batch['cu_seqlens']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[186, 503, 198, 795, 831, 221, 575, 313, 627, 718, 721, 971, 570,  35,\n",
       "         314, 600, 650, 394,  95, 132, 274,  72, 201, 493, 319, 141, 540, 761,\n",
       "          28, 608, 131, 764, 830, 617, 272, 329, 827, 950, 731,  44, 626, 761,\n",
       "         404,  16, 241, 535, 577, 360, 323, 507, 461, 183, 812, 960, 907, 982,\n",
       "         314, 513, 151, 724, 578, 711, 111,  18, 701, 116, 132,  70, 447, 712,\n",
       "         457, 127, 324, 932, 834, 441, 795, 831, 223, 178,  69, 256, 551, 144,\n",
       "         311, 908, 634, 496, 848, 903, 210, 721, 576, 408, 764, 810, 825, 138,\n",
       "         858, 959],\n",
       "        [493, 216, 382, 583, 984, 896, 546, 248,  62, 904, 400, 712, 572, 850,\n",
       "         358, 106, 997,  90, 482,  62, 100, 786, 367, 610, 301, 544,  51, 611,\n",
       "          22,  20, 700, 785, 899, 839, 927,  43, 568, 274, 121, 532,  52, 104,\n",
       "         128, 653, 149,  56, 145, 506, 776, 701, 831, 347, 158, 270, 460, 327,\n",
       "         705, 790, 274, 265, 993, 107, 657, 945, 681, 255, 336, 343, 194, 654,\n",
       "          72, 658, 761, 913, 908, 781, 217, 228, 181, 381, 663, 841, 911,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 503,  198,  795,  831,  221,  575,  313,  627,  718,  721,  971,  570,\n",
       "           35,  314,  600,  650,  394,   95,  132,  274,   72,  201,  493,  319,\n",
       "          141,  540,  761,   28,  608,  131,  764,  830,  617, -100,  329,  827,\n",
       "          950,  731,   44,  626,  761,  404,   16,  241,  535,  577,  360,  323,\n",
       "          507,  461,  183,  812,  960,  907,  982,  314,  513,  151,  724,  578,\n",
       "          711,  111,   18,  701,  116,  132,   70,  447,  712,  457,  127,  324,\n",
       "          932,  834,  441,  795,  831, -100,  178,   69,  256,  551,  144,  311,\n",
       "          908,  634,  496,  848,  903,  210,  721,  576,  408,  764,  810,  825,\n",
       "          138,  858,  959, -100],\n",
       "        [ 216,  382,  583,  984,  896,  546,  248,   62,  904,  400,  712,  572,\n",
       "          850,  358,  106,  997,   90,  482,   62,  100,  786,  367,  610,  301,\n",
       "          544,   51,  611,   22,   20,  700,  785,  899,  839,  927,   43, -100,\n",
       "          274,  121,  532,   52,  104,  128,  653,  149,   56,  145,  506,  776,\n",
       "          701,  831,  347,  158,  270,  460,  327,  705,  790,  274,  265,  993,\n",
       "          107,  657,  945,  681,  255,  336,  343,  194,  654,   72,  658,  761,\n",
       "          913,  908,  781,  217,  228,  181,  381,  663,  841,  911, -100,   -1,\n",
       "           -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
       "           -1,   -1,   -1, -100]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['causal_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "Does it look good to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Yes, the implementation looks very good! The AutoregressiveSequencePacker is correctly configured for autoregressive training:\n",
    "\n",
    "1. **Input/Label Shifting**: The labels are properly shifted (using `torch.roll`) so that each position predicts the next token.\n",
    "\n",
    "2. **Sequence Boundary Handling**: The code correctly identifies the end of each sequence (using `cu_seqlens`) and sets the label at those positions to `ignore_token_id` (-100). This prevents the model from learning to predict across sequence boundaries.\n",
    "\n",
    "3. **Attention Masking**: Two important masks are created:\n",
    "   - `attention_mask`: Distinguishes real tokens (1) from padding tokens (0)\n",
    "   - `causal_mask`: A lower triangular matrix ensuring each token can only attend to itself and previous tokens\n",
    "\n",
    "4. **Packed Sequences**: The original packing functionality is preserved, efficiently combining multiple sequences into fixed-length batches.\n",
    "\n",
    "The output shows everything is working correctly:\n",
    "- The input and label tensors have the expected shapes\n",
    "- The cumulative sequence lengths show we've packed multiple sequences (4 in the first packed sequence, 4 in the second)\n",
    "- The labels are properly shifted, with `-100` at sequence boundaries\n",
    "- The attention mask correctly identifies non-padding tokens\n",
    "- The causal mask has the expected lower triangular structure\n",
    "\n",
    "This implementation will work well for training autoregressive models like GPT with packed sequences, which should improve training efficiency by reducing padding waste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Prompt**\n",
    "\n",
    "How do I use this now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI Response**\n",
    "\n",
    "Here's how to use the AutoregressiveSequencePacker with a language model like GPT:\n",
    "\n",
    "```python\n",
    "# 1. Create your dataset and dataloader\n",
    "train_dataset = SimpleDataset(num_samples=10000, min_len=20, max_len=200)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=simple_collate\n",
    ")\n",
    "\n",
    "# 2. Create the sequence packer\n",
    "packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=train_dataloader,\n",
    "    src_batch_size=16,\n",
    "    src_max_seq_len=200,\n",
    "    out_batch_size=4,\n",
    "    out_pseq_len=512,\n",
    "    buffer_size=64,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "# 3. Create your model (example with a simple GPT-style model)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=1000,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=6,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "# 4. Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "```\n",
    "\n",
    "For inference with the trained model:\n",
    "\n",
    "```python\n",
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Convert prompt to tensor\n",
    "    input_ids = torch.tensor([prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "# Example prompt (list of token IDs)\n",
    "prompt = [10, 25, 36, 42]\n",
    "generated = generate_text(model, prompt)\n",
    "print(generated)\n",
    "```\n",
    "\n",
    "Key points:\n",
    "1. The packer efficiently combines sequences for better GPU utilization\n",
    "2. The model receives properly formatted inputs with causal masking\n",
    "3. The labels are shifted and have sequence boundaries masked\n",
    "4. The loss is automatically computed only on valid positions (ignoring padding and sequence boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1422: indexSelectLargeIndex: block: [439,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m loss = outputs.loss\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m   1054\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1055\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py:821\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    819\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.wte(input_ids)\n\u001b[32m    820\u001b[39m position_embeds = \u001b[38;5;28mself\u001b[39m.wpe(position_ids)\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m hidden_states = \u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    823\u001b[39m \u001b[38;5;66;03m# Attention mask.\u001b[39;00m\n\u001b[32m    824\u001b[39m _use_sdpa = \u001b[38;5;28mself\u001b[39m._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m output_attentions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 1. Create your dataset and dataloader\n",
    "train_dataset = SimpleDataset(num_samples=10000, min_len=20, max_len=200)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=simple_collate\n",
    ")\n",
    "\n",
    "# 2. Create the sequence packer\n",
    "packer = AutoregressiveSequencePacker(\n",
    "    src_iterable=train_dataloader,\n",
    "    src_batch_size=16,\n",
    "    src_max_seq_len=200,\n",
    "    out_batch_size=4,\n",
    "    out_pseq_len=512,\n",
    "    buffer_size=64,\n",
    "    pad_token_id=-1,\n",
    "    ignore_token_id=-100\n",
    ")\n",
    "\n",
    "# 3. Create your model (example with a simple GPT-style model)\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=1000,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=6,\n",
    "    n_head=12\n",
    ")\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "# 4. Training loop\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=50):\n",
    "    # Convert prompt to tensor\n",
    "    input_ids = torch.tensor([prompt], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "    \n",
    "    return output[0]\n",
    "\n",
    "# Example prompt (list of token IDs)\n",
    "prompt = [10, 25, 36, 42]\n",
    "generated = generate_text(model, prompt)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the maximum token ID in your data\n",
    "max_token = 0\n",
    "for batch in packer:\n",
    "    curr_max = batch[\"input_ids\"].max().item()\n",
    "    if curr_max > max_token:\n",
    "        max_token = curr_max\n",
    "    break\n",
    "print(f\"Maximum token ID in data: {max_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative values besides the padding token\n",
    "for batch in packer:\n",
    "    # Find all unique values\n",
    "    unique_values = torch.unique(batch[\"input_ids\"])\n",
    "    print(f\"Unique values in input_ids: {unique_values}\")\n",
    "    \n",
    "    # Check if there are negative values other than pad_token_id\n",
    "    neg_values = unique_values[unique_values < 0]\n",
    "    print(f\"Negative values: {neg_values}\")\n",
    "    \n",
    "    # Count occurrences of each value\n",
    "    for val in neg_values:\n",
    "        count = (batch[\"input_ids\"] == val).sum().item()\n",
    "        print(f\"Value {val} occurs {count} times\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try explicitly telling the model to ignore padding in attention mask\n",
    "for batch in packer:\n",
    "    # Make sure padding tokens have 0 in attention mask\n",
    "    attention_mask = (batch[\"input_ids\"] != -1).long()\n",
    "    \n",
    "    # Convert -1 padding to 0 (or any valid token ID that's safe to use)\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    inputs[inputs == -1] = 0\n",
    "    \n",
    "    # Fix labels too\n",
    "    labels = batch[\"labels\"].clone()\n",
    "    labels[labels == -1] = -100  # -100 is the standard ignore index for CrossEntropyLoss\n",
    "    \n",
    "    # Try with the fixed inputs\n",
    "    outputs = model(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try without using the custom causal mask\n",
    "for batch in packer:\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    inputs[inputs == -1] = 0\n",
    "    \n",
    "    # GPT models create their own causal mask internally\n",
    "    outputs = model(\n",
    "        input_ids=inputs,\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        labels=batch[\"labels\"].clone().masked_fill_(batch[\"labels\"] == -1, -100)\n",
    "    )\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the model's embedding parameters\n",
    "print(f\"Model embedding weight shape: {model.transformer.wte.weight.shape}\")\n",
    "print(f\"Model embedding weight indices: min={model.transformer.wte.weight.shape[0]-1}, max={0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"Override to create shifted inputs and labels for autoregressive training\"\"\"\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Create the shifted inputs (tokens 0 to n-1) and labels (tokens 1 to n)\n",
    "            inputs = input_ids.clone()\n",
    "            # For labels, we shift by 1 (each position predicts the next token)\n",
    "            labels = torch.roll(input_ids, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Handle padding: replace -1 with 0 in inputs\n",
    "            inputs[inputs == self.pad_token_id] = 0\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # For each packed sequence, handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    start, end = cu_seqlens[j].item(), cu_seqlens[j+1].item()\n",
    "                    \n",
    "                    # Set the label at the last position of each sequence to ignore_token_id\n",
    "                    # We don't want to predict across sequence boundaries\n",
    "                    labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Handle padding in labels: replace -1 with ignore_token_id\n",
    "            labels[labels == self.pad_token_id] = self.ignore_token_id\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass - no need for additional conversions\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the exact values that might be causing issues\n",
    "for batch in packer:\n",
    "    inputs = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Replace pad tokens with 0\n",
    "    inputs[inputs == packer.pad_token_id] = 0\n",
    "    \n",
    "    # Check for any values >= vocab_size\n",
    "    max_val = inputs.max().item()\n",
    "    print(f\"Maximum value in inputs after replacing padding: {max_val}\")\n",
    "    \n",
    "    # Check if there are any values outside the vocabulary range\n",
    "    invalid_indices = (inputs >= 1000).nonzero()\n",
    "    if len(invalid_indices) > 0:\n",
    "        print(f\"Found {len(invalid_indices)} invalid indices\")\n",
    "        print(f\"Invalid values: {inputs[invalid_indices[:, 0], invalid_indices[:, 1]]}\")\n",
    "    \n",
    "    # Try explicitly setting the range of values\n",
    "    valid_inputs = inputs.clamp(0, 999)\n",
    "    \n",
    "    # Check if there are still any issues\n",
    "    try:\n",
    "        outputs = model(\n",
    "            input_ids=valid_inputs.to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            labels=batch[\"labels\"].clamp(-100, 999).to(device)\n",
    "        )\n",
    "        print(\"Success with clamped values!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Still encountering error: {e}\")\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a very simple input to test the model directly\n",
    "test_input = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "test_labels = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "test_labels[:, -1] = -100  # Ignore last token\n",
    "test_mask = torch.ones_like(test_input).to(device)\n",
    "\n",
    "try:\n",
    "    test_output = model(\n",
    "        input_ids=test_input,\n",
    "        attention_mask=test_mask,\n",
    "        labels=test_labels\n",
    "    )\n",
    "    print(f\"Test successful! Loss: {test_output.loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Test failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedAutoregressive(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, vocab_size=1000, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            # Get the input_ids and create a valid version\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            valid_inputs = input_ids.clone()\n",
    "            valid_inputs[valid_inputs == self.pad_token_id] = 0\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # Create shifted labels for autoregressive prediction\n",
    "            labels = torch.roll(valid_inputs, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Set padding positions in labels to -100 (not -1)\n",
    "            padding_mask = (input_ids == self.pad_token_id)\n",
    "            labels[padding_mask] = -100\n",
    "            \n",
    "            # Handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    end = cu_seqlens[j+1].item()\n",
    "                    if end > 0 and end < labels.size(1):\n",
    "                        labels[i, end-1] = -100  # Don't predict across sequence boundaries\n",
    "            \n",
    "            # Double-check no -1 values remain in labels\n",
    "            labels[labels == -1] = -100\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": valid_inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_batch(batch, vocab_size=1000):\n",
    "    \"\"\"Check for any values that might cause issues with the model\"\"\"\n",
    "    inputs = batch[\"input_ids\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    # Check input values\n",
    "    if inputs.min().item() < 0:\n",
    "        print(f\"Warning: inputs contain values < 0: {inputs.min().item()}\")\n",
    "    if inputs.max().item() >= vocab_size:\n",
    "        print(f\"Warning: inputs contain values >= vocab_size: {inputs.max().item()}\")\n",
    "    \n",
    "    # Check label values\n",
    "    valid_label_mask = (labels != -100)\n",
    "    if valid_label_mask.sum() > 0:\n",
    "        valid_labels = labels[valid_label_mask]\n",
    "        if valid_labels.min().item() < 0:\n",
    "            print(f\"Warning: labels contain values < 0 (excluding -100): {valid_labels.min().item()}\")\n",
    "        if valid_labels.max().item() >= vocab_size:\n",
    "            print(f\"Warning: labels contain values >= vocab_size: {valid_labels.max().item()}\")\n",
    "    \n",
    "    # Check for -1 values\n",
    "    if (inputs == -1).any():\n",
    "        print(f\"Warning: inputs contain -1 values: {(inputs == -1).sum().item()}\")\n",
    "    if (labels == -1).any():\n",
    "        print(f\"Warning: labels contain -1 values: {(labels == -1).sum().item()}\")\n",
    "    \n",
    "    return \"All checks passed!\" if not (\n",
    "        inputs.min().item() < 0 or \n",
    "        inputs.max().item() >= vocab_size or \n",
    "        (valid_label_mask.sum() > 0 and (valid_labels.min().item() < 0 or valid_labels.max().item() >= vocab_size)) or\n",
    "        (inputs == -1).any() or \n",
    "        (labels == -1).any()\n",
    "    ) else \"Issues found!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the current batch\n",
    "def fix_batch(batch, vocab_size=1000):\n",
    "    \"\"\"Fix any problematic values in the batch\"\"\"\n",
    "    # Create fixed copies\n",
    "    fixed_inputs = batch[\"input_ids\"].clone()\n",
    "    fixed_labels = batch[\"labels\"].clone()\n",
    "    \n",
    "    # Replace -1 with 0 in inputs\n",
    "    fixed_inputs[fixed_inputs == -1] = 0\n",
    "    \n",
    "    # Replace -1 with -100 in labels\n",
    "    fixed_labels[fixed_labels == -1] = -100\n",
    "    \n",
    "    # Ensure all values are in range\n",
    "    fixed_inputs = fixed_inputs.clamp(0, vocab_size-1)\n",
    "    \n",
    "    # For labels, we only clamp the non-ignore values\n",
    "    valid_mask = (fixed_labels != -100)\n",
    "    if valid_mask.any():\n",
    "        fixed_labels[valid_mask] = fixed_labels[valid_mask].clamp(0, vocab_size-1)\n",
    "    \n",
    "    # Return the fixed batch\n",
    "    return {\n",
    "        \"input_ids\": fixed_inputs,\n",
    "        \"labels\": fixed_labels,\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"cu_seqlens\": batch[\"cu_seqlens\"],\n",
    "        \"max_seqlen\": batch[\"max_seqlen\"]\n",
    "    }\n",
    "\n",
    "# Try with the fixed batch\n",
    "fixed = fix_batch(batch)\n",
    "verify_batch(fixed)\n",
    "\n",
    "# Try running the model with the fixed batch\n",
    "try:\n",
    "    outputs = model(\n",
    "        input_ids=fixed[\"input_ids\"].to(device),\n",
    "        attention_mask=fixed[\"attention_mask\"].to(device),\n",
    "        labels=fixed[\"labels\"].to(device)\n",
    "    )\n",
    "    print(f\"Success! Loss: {outputs.loss.item()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __init__(self, *args, vocab_size=1000, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        kwargs['suppress_masking'] = True\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        for batch_dict in super()._generate_batches():\n",
    "            # Get the input_ids and create a valid version\n",
    "            input_ids = batch_dict[\"input_ids\"]\n",
    "            \n",
    "            # Replace padding with 0 and ensure all values are in range\n",
    "            valid_inputs = input_ids.clone()\n",
    "            valid_inputs[valid_inputs == self.pad_token_id] = 0\n",
    "            valid_inputs = valid_inputs.clamp(0, self.vocab_size-1)\n",
    "            \n",
    "            # Create attention mask (1 for tokens, 0 for padding)\n",
    "            attention_mask = (input_ids != self.pad_token_id).long()\n",
    "            \n",
    "            # Create shifted labels for autoregressive prediction\n",
    "            labels = torch.roll(valid_inputs, shifts=-1, dims=1).clone()\n",
    "            \n",
    "            # Mark padding positions with ignore_token_id\n",
    "            labels[input_ids == self.pad_token_id] = self.ignore_token_id\n",
    "            \n",
    "            # Handle sequence boundaries\n",
    "            for i, cu_seqlens in enumerate(batch_dict[\"cu_seqlens\"]):\n",
    "                for j in range(len(cu_seqlens) - 1):\n",
    "                    end = cu_seqlens[j+1].item()\n",
    "                    if end > 0 and end < labels.size(1):\n",
    "                        labels[i, end-1] = self.ignore_token_id\n",
    "            \n",
    "            # Final safety check - ensure no -1 values remain\n",
    "            labels[labels == -1] = self.ignore_token_id\n",
    "            \n",
    "            yield {\n",
    "                \"input_ids\": valid_inputs,\n",
    "                \"labels\": labels,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cu_seqlens\": batch_dict[\"cu_seqlens\"],\n",
    "                \"max_seqlen\": batch_dict[\"max_seqlen\"]\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in packer:\n",
    "        batch = fix_batch(batch)\n",
    "        # Move batch to device\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Forward pass - no need for additional conversions\n",
    "        outputs = model(\n",
    "            input_ids=inputs,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sqeunce packer + FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    Flash Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with Flash Attention implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - Check if Flash Attention is available (FLASH_ATTN_AVAILABLE)\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [total_seq_len, hidden_dim].\n",
    "            cu_seqlens: Cumulative sequence lengths tensor of shape [batch_size + 1]\n",
    "                    Used instead of an attention mask for both masking and\n",
    "                    variable-length sequences. Example:\n",
    "                        cu_seqlens = torch.tensor([0, 10, 30, 60])\n",
    "                    This means there are three sequences in the batch:\n",
    "                        - First sequence has 10 tokens\n",
    "                        - Second sequence has 20 tokens\n",
    "                        - Third sequence has 30 tokens\n",
    "            max_seqlen: Maximum sequence length in the batch. In the example above,\n",
    "                        the maximum sequence length is 30.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [total_seq_len, hidden_dim] after attention.\n",
    "        \"\"\"\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            raise ImportError(\"Flash Attention is not available. Please install it with `pip install flash-attn`\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "\n",
    "        k_reshaped = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        q_reshaped = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v_reshaped = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        output = flash_attn_varlen_func(\n",
    "            q_reshaped,\n",
    "            k_reshaped,\n",
    "            v_reshaped,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_heads, num_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            FlashCausalAttentionBlock(hidden_dim, num_heads) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, cu_seqlens, max_seqlen):\n",
    "        total_seq_len = input_ids.size(0)\n",
    "        \n",
    "        x = self.token_embedding(input_ids)\n",
    "        \n",
    "        positions = torch.arange(total_seq_len, device=input_ids.device)\n",
    "        offsets = torch.zeros_like(positions)\n",
    "        for i in range(len(cu_seqlens) - 1):\n",
    "            start_idx, end_idx = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            offsets[start_idx:end_idx] = cu_seqlens[i]\n",
    "        \n",
    "        seq_positions = positions - offsets\n",
    "        x = x + self.position_embedding(seq_positions)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cu_seqlens, max_seqlen)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoregressiveDataset:\n",
    "    def __init__(self, num_samples=100, min_len=10, max_len=100, vocab_size=1000):\n",
    "        self.samples = []\n",
    "        for _ in range(num_samples):\n",
    "            seq_len = random.randint(min_len, max_len)\n",
    "            tokens = [random.randint(1, vocab_size-1) for _ in range(seq_len)]\n",
    "            # For autoregressive training, input is tokens[:-1], target is tokens[1:]\n",
    "            self.samples.append({\"input_ids\": tokens})\n",
    "    \n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dataset = AutoregressiveDataset(num_samples=100, vocab_size=1000)\n",
    "gpt_dataloader = DataLoader(gpt_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "\n",
    "gpt_packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=gpt_dataloader,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=100,\n",
    "    out_batch_size=1,\n",
    "    out_pseq_len=200,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=0,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.0,  # Not used with suppress_masking=True\n",
    "    suppress_masking=True  # Important for autoregressive models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.0602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.1148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(7.0717, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = SimpleGPTModel(vocab_size=1000, hidden_dim=256, num_heads=8, num_layers=4, max_seq_len=100).to(device)\n",
    "\n",
    "# Training loop\n",
    "with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
    "    for batch in gpt_packer:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        cu_seqlens = batch[\"cu_seqlens\"]\n",
    "        max_seqlen = batch[\"max_seqlen\"]\n",
    "\n",
    "        # For autoregressive training:\n",
    "        # - Inputs are all tokens except the last one in each sequence\n",
    "        # - Targets are all tokens except the first one in each sequence\n",
    "\n",
    "        # This requires careful handling with packed sequences\n",
    "        # We can use cu_seqlens to identify sequence boundaries\n",
    "\n",
    "        # Get logits from model\n",
    "        logits = model(input_ids.squeeze().to(device), cu_seqlens[0].to(device), max_seqlen[0])\n",
    "\n",
    "        # Compute loss (autoregressive next-token prediction)\n",
    "        # This is complex with packed sequences - you need to shift targets\n",
    "        # and handle sequence boundaries carefully\n",
    "\n",
    "        targets = torch.roll(input_ids, shifts=-1)\n",
    "        loss = F.cross_entropy(logits, targets.squeeze().to(device))\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/home/galopy/miniforge3/envs/fromscratch/lib/python3.12/site-packages/flash_attn/flash_attn_interface.py\u001b[39m(\u001b[92m170\u001b[39m)\u001b[36m_flash_attn_varlen_forward\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    168\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
      "\u001b[32m    169\u001b[39m     q, k, v = [maybe_contiguous(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;28;01min\u001b[39;00m (q, k, v)]\n",
      "\u001b[32m--> 170\u001b[39m     out, softmax_lse, S_dmask, rng_state = flash_attn_gpu.varlen_fwd(\n",
      "\u001b[32m    171\u001b[39m         q,\n",
      "\u001b[32m    172\u001b[39m         k,\n",
      "\n",
      "ipdb> p q\n",
      "tensor([[[ 1.0527, -0.1959, -0.8595,  ..., -0.8361,  0.3578,  1.2478],\n",
      "         [ 0.5617,  0.1539, -0.3003,  ..., -2.3932, -0.9189, -0.6507],\n",
      "         [-1.4578, -0.8788, -0.8945,  ...,  0.5563, -0.5626,  1.1726],\n",
      "         ...,\n",
      "         [ 0.2234, -1.4744, -0.7962,  ..., -1.7971, -0.6704,  0.3328],\n",
      "         [-1.0220, -1.0418, -1.0343,  ..., -0.7659,  0.2398,  1.7561],\n",
      "         [ 0.1045,  0.7630,  0.2812,  ..., -0.3854, -0.1902, -0.4510]],\n",
      "\n",
      "        [[-0.2990,  0.1779,  1.0363,  ...,  0.0101, -0.5598, -0.1354],\n",
      "         [ 0.8516, -0.4582, -1.0212,  ..., -0.4882,  0.2797,  0.7525],\n",
      "         [ 0.2919, -0.7758, -0.3448,  ...,  0.1806, -1.8000, -1.2779],\n",
      "         ...,\n",
      "         [-0.7113, -0.6095, -1.1379,  ..., -1.1082,  0.7887, -0.4479],\n",
      "         [-0.7095,  0.4652,  0.2826,  ..., -0.5564, -0.4372, -0.7410],\n",
      "         [ 0.1065,  0.4300, -0.0483,  ...,  0.3825,  0.1215, -0.1012]],\n",
      "\n",
      "        [[-1.0166, -1.6522,  0.6715,  ..., -0.1236,  0.7001,  0.4468],\n",
      "         [-0.6575,  0.8342, -0.5860,  ..., -0.3991, -0.6526,  0.6052],\n",
      "         [ 0.7983,  2.2726, -0.6829,  ..., -0.1004,  0.0868,  1.6235],\n",
      "         ...,\n",
      "         [ 0.7156,  0.1150, -0.7856,  ..., -0.6290,  1.0327, -0.6848],\n",
      "         [-0.1152, -0.9948, -0.1319,  ..., -0.8435, -1.0135,  1.5700],\n",
      "         [-0.5139, -0.0620,  0.6615,  ...,  0.8749,  0.0246, -1.4302]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0731, -0.8446, -0.4834,  ..., -0.0797, -0.7329,  0.6418],\n",
      "         [-0.9940,  0.5313, -1.4234,  ..., -0.6377,  0.0485, -0.3838],\n",
      "         [ 1.2624,  1.2599, -0.4564,  ..., -0.4921,  0.4266,  0.1690],\n",
      "         ...,\n",
      "         [-0.8497,  1.4136,  0.0258,  ...,  0.4417,  0.1214,  0.8474],\n",
      "         [-0.6810,  0.9810, -0.2926,  ..., -0.2296,  0.3284,  0.4961],\n",
      "         [ 0.6790,  0.7998, -0.7891,  ...,  0.4443,  0.2496, -0.0501]],\n",
      "\n",
      "        [[ 0.5247, -0.8932,  0.4495,  ..., -0.7233,  0.8628,  0.5571],\n",
      "         [-0.4411, -0.0484, -0.7113,  ..., -0.3609, -0.5994,  0.7374],\n",
      "         [ 0.9996,  0.3428,  0.0309,  ..., -0.4451, -0.6712,  1.1334],\n",
      "         ...,\n",
      "         [-0.6977, -0.1422, -0.2153,  ..., -0.1014, -0.2859, -0.5131],\n",
      "         [-1.2570, -0.5608,  0.6715,  ..., -0.2958, -1.4891,  0.9168],\n",
      "         [ 0.0375,  0.1996,  0.0685,  ..., -0.0130, -0.3571,  0.1713]],\n",
      "\n",
      "        [[ 0.9792, -0.1704, -1.0174,  ..., -0.6447, -0.7916, -0.7225],\n",
      "         [-3.0397, -0.0827,  0.2195,  ..., -0.7257,  0.6671,  0.6949],\n",
      "         [ 0.3369,  0.4766,  0.9395,  ..., -1.0663,  0.8611, -1.1158],\n",
      "         ...,\n",
      "         [-1.5008, -0.2621, -1.2755,  ..., -0.7678,  0.1500, -0.4591],\n",
      "         [-0.3567, -0.9439,  0.2060,  ..., -0.4398, -0.9028, -1.1078],\n",
      "         [ 1.1787,  0.6191, -1.0696,  ..., -0.9153, -0.0227, -0.4263]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim, end, theta=10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(0)\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(*x.shape)\n",
    "    return x_out.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_freqs_cis(self.head_dim, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "gpt_dataset = AutoregressiveDataset(num_samples=100, vocab_size=1000)\n",
    "gpt_dataloader = DataLoader(gpt_dataset, batch_size=8, shuffle=True, collate_fn=simple_collate)\n",
    "\n",
    "# Create sequence packer\n",
    "gpt_packer = GreedyBestFitSequencePacker(\n",
    "    src_iterable=gpt_dataloader,\n",
    "    src_batch_size=8,\n",
    "    src_max_seq_len=100,\n",
    "    out_batch_size=1,\n",
    "    out_pseq_len=200,\n",
    "    buffer_size=32,\n",
    "    pad_token_id=0,\n",
    "    mask_token_id=0,\n",
    "    ignore_token_id=-100,\n",
    "    mask_prob=0.0,  # Not used with suppress_masking=True\n",
    "    suppress_masking=True  # Important for autoregressive models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Precompute RoPE frequencies\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(self.head_dim, max_seq_len))\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Create position indices that reset at sequence boundaries\n",
    "        positions = torch.arange(total_seq_len, device=x.device)\n",
    "        seq_positions = positions.clone()\n",
    "        for i in range(len(cu_seqlens)-1):\n",
    "            start, end = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            seq_positions[start:end] = torch.arange(end-start, device=x.device)\n",
    "            \n",
    "        # Apply RoPE to q and k\n",
    "        freqs_cis = self.freqs_cis[seq_positions]\n",
    "        q = apply_rotary_emb(q, freqs_cis)\n",
    "        k = apply_rotary_emb(k, freqs_cis)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # Precompute RoPE frequencies\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(self.head_dim, max_seq_len))\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Create position indices that reset at sequence boundaries\n",
    "        positions = torch.arange(total_seq_len, device=x.device)\n",
    "        seq_positions = torch.zeros_like(positions)\n",
    "        \n",
    "        # Compute positions within each sequence, ensuring they don't exceed max_seq_len\n",
    "        for i in range(len(cu_seqlens)-1):\n",
    "            start, end = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            seq_len = end - start\n",
    "            seq_positions[start:end] = torch.arange(seq_len, device=x.device)\n",
    "        \n",
    "        # Ensure positions don't exceed our precomputed table\n",
    "        seq_positions = seq_positions.clamp(0, self.max_seq_len - 1)\n",
    "        \n",
    "        # Apply RoPE to q and k\n",
    "        freqs_cis = self.freqs_cis[seq_positions]\n",
    "        q = apply_rotary_emb(q, freqs_cis)\n",
    "        k = apply_rotary_emb(k, freqs_cis)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim, end, theta=10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "def apply_rotary_emb(x, freqs_cis):\n",
    "    # x shape: [total_seq_len, num_heads, head_dim]\n",
    "    # freqs_cis shape: [total_seq_len, head_dim//2]\n",
    "    \n",
    "    # Make sure head_dim is even\n",
    "    head_dim = x.shape[-1]\n",
    "    assert head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "    \n",
    "    # Reshape x for complex view\n",
    "    x_reshaped = x.float().reshape(x.shape[0], x.shape[1], head_dim//2, 2)\n",
    "    x_complex = torch.view_as_complex(x_reshaped)\n",
    "    \n",
    "    # Reshape freqs_cis to match x_complex for broadcasting\n",
    "    # We need to add dimensions for the head axis\n",
    "    freqs_cis = freqs_cis.unsqueeze(1)  # [total_seq_len, 1, head_dim//2]\n",
    "    \n",
    "    # Apply the rotation\n",
    "    x_rotated = x_complex * freqs_cis\n",
    "    \n",
    "    # Convert back to real and reshape to original dimensions\n",
    "    x_out = torch.view_as_real(x_rotated).reshape(x.shape)\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        assert self.head_dim % 2 == 0, \"head_dim must be even for RoPE\"\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # Precompute RoPE frequencies - note the head_dim//2 for complex numbers\n",
    "        self.register_buffer(\"freqs_cis\", precompute_freqs_cis(self.head_dim//2, max_seq_len))\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Create position indices that reset at sequence boundaries\n",
    "        positions = torch.arange(total_seq_len, device=x.device)\n",
    "        seq_positions = torch.zeros_like(positions)\n",
    "        \n",
    "        # Compute positions within each sequence\n",
    "        for i in range(len(cu_seqlens)-1):\n",
    "            start, end = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            seq_len = end - start\n",
    "            seq_positions[start:end] = torch.arange(seq_len, device=x.device)\n",
    "        \n",
    "        # Ensure positions don't exceed our precomputed table\n",
    "        seq_positions = seq_positions.clamp(0, self.max_seq_len - 1)\n",
    "        \n",
    "        # Apply RoPE to q and k\n",
    "        freqs_cis = self.freqs_cis[seq_positions]  # [total_seq_len, head_dim//2]\n",
    "        q = apply_rotary_emb(q, freqs_cis)\n",
    "        k = apply_rotary_emb(k, freqs_cis)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn.layers.rotary import RotaryEmbedding\n",
    "\n",
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize RotaryEmbedding\n",
    "        self.rotary = RotaryEmbedding(dim=self.head_dim, interleaved=False)\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Create position indices that reset at sequence boundaries\n",
    "        positions = torch.arange(total_seq_len, device=x.device)\n",
    "        seq_positions = torch.zeros_like(positions)\n",
    "        \n",
    "        # Compute positions within each sequence\n",
    "        for i in range(len(cu_seqlens)-1):\n",
    "            start, end = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            seq_positions[start:end] = torch.arange(end-start, device=x.device)\n",
    "        \n",
    "        # Update the rotary embedding cache if needed\n",
    "        self.rotary._update_cos_sin_cache(max_seqlen, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        # For packed sequences with variable lengths, we use seqlen_offset to handle position reset\n",
    "        # and cu_seqlens to properly apply rotary embeddings per sequence\n",
    "        q = apply_rotary_emb_func(\n",
    "            q, \n",
    "            self.rotary._cos_cached, \n",
    "            self.rotary._sin_cached,\n",
    "            interleaved=self.rotary.interleaved,\n",
    "            inplace=True,\n",
    "            seqlen_offsets=0,  # We're handling offsets with seq_positions\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seqlen\n",
    "        )\n",
    "        \n",
    "        k = apply_rotary_emb_func(\n",
    "            k, \n",
    "            self.rotary._cos_cached, \n",
    "            self.rotary._sin_cached,\n",
    "            interleaved=self.rotary.interleaved,\n",
    "            inplace=True,\n",
    "            seqlen_offsets=0,  # We're handling offsets with seq_positions\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seqlen\n",
    "        )\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_heads, num_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            FlashCausalAttentionBlock(hidden_dim, num_heads, max_seq_len=max_seq_len) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, cu_seqlens, max_seqlen):\n",
    "        # Get embeddings\n",
    "        x = self.token_embedding(input_ids)\n",
    "        \n",
    "        # Apply attention layers with RoPE\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cu_seqlens, max_seqlen)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apply_rotary_emb_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      8\u001b[39m max_seqlen = batch[\u001b[33m\"\u001b[39m\u001b[33mmax_seqlen\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# For autoregressive training:\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# - Inputs are all tokens except the last one in each sequence\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# - Targets are all tokens except the first one in each sequence\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get logits from model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Compute loss (autoregressive next-token prediction)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# This is complex with packed sequences - you need to shift targets\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# and handle sequence boundaries carefully\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Simplified loss calculation (would need refinement)\u001b[39;00m\n\u001b[32m     25\u001b[39m targets = torch.roll(input_ids, shifts=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mSimpleGPTModel.forward\u001b[39m\u001b[34m(self, input_ids, cu_seqlens, max_seqlen)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Apply attention layers with RoPE\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x)\n\u001b[32m     23\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mFlashCausalAttentionBlock.forward\u001b[39m\u001b[34m(self, x, cu_seqlens, max_seqlen)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mself\u001b[39m.rotary._update_cos_sin_cache(max_seq_len_value, device=x.device, dtype=x.dtype)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Apply rotary embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m q = \u001b[43mapply_rotary_emb_func\u001b[49m(\n\u001b[32m     44\u001b[39m     q, \n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.rotary._cos_cached, \n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m.rotary._sin_cached,\n\u001b[32m     47\u001b[39m     interleaved=\u001b[38;5;28mself\u001b[39m.rotary.interleaved,\n\u001b[32m     48\u001b[39m     inplace=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     49\u001b[39m     seqlen_offsets=\u001b[32m0\u001b[39m,\n\u001b[32m     50\u001b[39m     cu_seqlens=cu_seqlens,\n\u001b[32m     51\u001b[39m     max_seqlen=max_seq_len_value\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m k = apply_rotary_emb_func(\n\u001b[32m     55\u001b[39m     k, \n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m.rotary._cos_cached, \n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     max_seqlen=max_seq_len_value\n\u001b[32m     63\u001b[39m )\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Call Flash Attention\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'apply_rotary_emb_func' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = SimpleGPTModel(vocab_size=1000, hidden_dim=256, num_heads=8, num_layers=4, max_seq_len=100)\n",
    "\n",
    "# Training loop\n",
    "for batch in gpt_packer:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    max_seqlen = batch[\"max_seqlen\"]\n",
    "    \n",
    "    # For autoregressive training:\n",
    "    # - Inputs are all tokens except the last one in each sequence\n",
    "    # - Targets are all tokens except the first one in each sequence\n",
    "    \n",
    "    # This requires careful handling with packed sequences\n",
    "    # We can use cu_seqlens to identify sequence boundaries\n",
    "    \n",
    "    # Get logits from model\n",
    "    logits = model(input_ids.squeeze(0), cu_seqlens, max_seqlen[0])\n",
    "    \n",
    "    # Compute loss (autoregressive next-token prediction)\n",
    "    # This is complex with packed sequences - you need to shift targets\n",
    "    # and handle sequence boundaries carefully\n",
    "    \n",
    "    # Simplified loss calculation (would need refinement)\n",
    "    targets = torch.roll(input_ids, shifts=-1)\n",
    "    # Mask out padding and sequence boundaries\n",
    "    mask = (input_ids != -1) & (torch.arange(input_ids.size(0)) + 1 != cu_seqlens[1:].unsqueeze(1))\n",
    "    loss = F.cross_entropy(logits[mask], targets[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/home/galopy/miniforge3/envs/fromscratch/lib/python3.12/site-packages/flash_attn/layers/rotary.py\u001b[39m(\u001b[92m415\u001b[39m)\u001b[36m_update_cos_sin_cache\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m    413\u001b[39m         \u001b[38;5;66;03m# or if we're switching from inference mode to training\u001b[39;00m\n",
      "\u001b[32m    414\u001b[39m         if (\n",
      "\u001b[32m--> 415\u001b[39m             seqlen > self._seq_len_cached\n",
      "\u001b[32m    416\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m self._cos_cached \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    417\u001b[39m             \u001b[38;5;28;01mor\u001b[39;00m self._cos_cached.device != device\n",
      "\n",
      "ipdb> seqlen\n",
      "[80]\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "import pdb; pdb.pm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    113\u001b[39m     max_seqlen_values = max_seqlen\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Get logits from model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Rest of your training code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mSimpleGPTModel.forward\u001b[39m\u001b[34m(self, input_ids, cu_seqlens, max_seqlen)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Apply attention layers with RoPE\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x)\n\u001b[32m     23\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mFlashCausalAttentionBlock.forward\u001b[39m\u001b[34m(self, x, cu_seqlens, max_seqlen)\u001b[39m\n\u001b[32m     33\u001b[39m     seq_positions[start:end] = torch.arange(end-start, device=x.device)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Update the rotary embedding cache if needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_update_cos_sin_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Apply rotary embeddings\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# For packed sequences with variable lengths, we use seqlen_offset to handle position reset\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# and cu_seqlens to properly apply rotary embeddings per sequence\u001b[39;00m\n\u001b[32m     41\u001b[39m q = apply_rotary_emb_func(\n\u001b[32m     42\u001b[39m     q, \n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m.rotary._cos_cached, \n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     max_seqlen=max_seqlen\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/flash_attn/layers/rotary.py:415\u001b[39m, in \u001b[36mRotaryEmbedding._update_cos_sin_cache\u001b[39m\u001b[34m(self, seqlen, device, dtype)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_cos_sin_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, seqlen, device=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    411\u001b[39m     \u001b[38;5;66;03m# Reset the tables if the sequence length has changed,\u001b[39;00m\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# if we're on a new device (possibly due to tracing for instance),\u001b[39;00m\n\u001b[32m    413\u001b[39m     \u001b[38;5;66;03m# or if we're switching from inference mode to training\u001b[39;00m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[43mseqlen\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_seq_len_cached\u001b[49m\n\u001b[32m    416\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cos_cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    417\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cos_cached.device != device\n\u001b[32m    418\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cos_cached.dtype != dtype\n\u001b[32m    419\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.training \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cos_cached.is_inference())\n\u001b[32m    420\u001b[39m     ):\n\u001b[32m    421\u001b[39m         \u001b[38;5;28mself\u001b[39m._seq_len_cached = seqlen\n\u001b[32m    422\u001b[39m         \u001b[38;5;66;03m# We want fp32 here, not self.inv_freq.dtype, since the model could be loaded in bf16\u001b[39;00m\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# And the output of arange can be quite large, so bf16 would lose a lot of precision.\u001b[39;00m\n\u001b[32m    424\u001b[39m         \u001b[38;5;66;03m# However, for compatibility reason, we add an option to use the dtype of self.inv_freq.\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize RotaryEmbedding\n",
    "        self.rotary = RotaryEmbedding(dim=self.head_dim, interleaved=False)\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Create position indices that reset at sequence boundaries\n",
    "        positions = torch.arange(total_seq_len, device=x.device)\n",
    "        seq_positions = torch.zeros_like(positions)\n",
    "        \n",
    "        # Compute positions within each sequence\n",
    "        for i in range(len(cu_seqlens)-1):\n",
    "            start, end = cu_seqlens[i], cu_seqlens[i+1]\n",
    "            seq_positions[start:end] = torch.arange(end-start, device=x.device)\n",
    "        \n",
    "        # Handle max_seqlen if it's a list - use the maximum value\n",
    "        if isinstance(max_seqlen, list):\n",
    "            max_seq_len_value = max(max_seqlen)\n",
    "        else:\n",
    "            max_seq_len_value = max_seqlen\n",
    "            \n",
    "        # Update the rotary embedding cache if needed\n",
    "        self.rotary._update_cos_sin_cache(max_seq_len_value, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q = apply_rotary_emb_func(\n",
    "            q, \n",
    "            self.rotary._cos_cached, \n",
    "            self.rotary._sin_cached,\n",
    "            interleaved=self.rotary.interleaved,\n",
    "            inplace=True,\n",
    "            seqlen_offsets=0,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seq_len_value\n",
    "        )\n",
    "        \n",
    "        k = apply_rotary_emb_func(\n",
    "            k, \n",
    "            self.rotary._cos_cached, \n",
    "            self.rotary._sin_cached,\n",
    "            interleaved=self.rotary.interleaved,\n",
    "            inplace=True,\n",
    "            seqlen_offsets=0,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seq_len_value\n",
    "        )\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seq_len_value,\n",
    "            max_seqlen_k=max_seq_len_value,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))\n",
    "\n",
    "class SimpleGPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_heads, num_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            FlashCausalAttentionBlock(hidden_dim, num_heads, max_seq_len=max_seq_len) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(hidden_dim)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, cu_seqlens, max_seqlen):\n",
    "        # Get embeddings\n",
    "        x = self.token_embedding(input_ids)\n",
    "        \n",
    "        # Apply attention layers with RoPE\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cu_seqlens, max_seqlen)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# In your training loop\n",
    "for batch in gpt_packer:\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    cu_seqlens = batch[\"cu_seqlens\"]\n",
    "    max_seqlen = batch[\"max_seqlen\"]\n",
    "    \n",
    "    # If max_seqlen is a list, extract the integer values\n",
    "    if isinstance(max_seqlen, list):\n",
    "        max_seqlen_values = [x.item() if isinstance(x, torch.Tensor) else x for x in max_seqlen]\n",
    "    else:\n",
    "        max_seqlen_values = max_seqlen\n",
    "    \n",
    "    # Get logits from model\n",
    "    logits = model(input_ids.squeeze(0), cu_seqlens, max_seqlen_values)\n",
    "    \n",
    "    # Rest of your training code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rotary_embedding_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflash_attn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrotary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_rotary\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# or from the context\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrotary_embedding_source\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RotaryEmbedding, apply_rotary_emb\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFlashCausalAttentionBlock\u001b[39;00m(nn.Module):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_dim, num_heads, dropout=\u001b[32m0.0\u001b[39m, max_seq_len=\u001b[32m2048\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rotary_embedding_source'"
     ]
    }
   ],
   "source": [
    "from flash_attn.ops.triton.rotary import apply_rotary\n",
    "# or from the context\n",
    "from rotary_embedding_source import RotaryEmbedding, apply_rotary_emb\n",
    "\n",
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Initialize rotary embeddings\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=self.head_dim,\n",
    "            base=10000.0,\n",
    "            interleaved=False,\n",
    "            device=next(self.parameters()).device\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        if not FLASH_ATTN_AVAILABLE: raise ImportError(\"Flash Attention is not available\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        k = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Update rotary embedding cache if needed\n",
    "        self.rotary_emb._update_cos_sin_cache(max_seqlen, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Apply rotary embeddings\n",
    "        q = apply_rotary_emb(\n",
    "            q, \n",
    "            self.rotary_emb._cos_cached, \n",
    "            self.rotary_emb._sin_cached,\n",
    "            interleaved=False,\n",
    "            inplace=True,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seqlen\n",
    "        )\n",
    "        \n",
    "        k = apply_rotary_emb(\n",
    "            k, \n",
    "            self.rotary_emb._cos_cached, \n",
    "            self.rotary_emb._sin_cached,\n",
    "            interleaved=False,\n",
    "            inplace=True,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seqlen\n",
    "        )\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q, k, v,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "        \n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
