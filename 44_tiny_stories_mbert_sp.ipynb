{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbf607",
   "metadata": {},
   "source": [
    "Using \n",
    "- sdpa \n",
    "- optimizer\n",
    "- compile \n",
    "- more data\n",
    "- MixedPrecision()\n",
    "- lr_sched\n",
    "- Double layer norm\n",
    "- Using a custom tokenizer.\n",
    "- GLU\n",
    "- sdpa linear\n",
    "- modern bert sequence packing + FA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c75570",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- Gradient accumulation or microbatch.\n",
    "- W&B logging.\n",
    "- Train a tokenizer using huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967927c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6938ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbed3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home()/'git/minai/TinyStories_All_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a959b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257, 2365, 1597]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "# tokenizer.train(txt_raw, vocab_size=3000)\n",
    "\n",
    "tokenizer.load((path/\"tok3k_regex.model\").name) # loads the model back from disk\n",
    "tokenizer.encode(\"hello world\") # string -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_toks(txt, toker): return toker.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a08231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks_to_txt(toks, toker): return toker.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_txts = 10\n",
    "num_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d73f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=\"\\n\\n\\n\"\n",
    "ctx_len = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d68efb",
   "metadata": {},
   "source": [
    "We create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    d = {}\n",
    "    d['input_ids'] = [tokenizer.encode(t, allowed_special={\"<|endoftext|>\"}) for t in b['text']]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(itertools.islice(iterator, n)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5014bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[516, 327, 44, 258, 390, 479, 402, 406, 507, 258]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = ds.with_transform(transforms)\n",
    "tds['train'][0]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9317e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   479,\n",
       "   402,\n",
       "   406,\n",
       "   507,\n",
       "   258,\n",
       "   775,\n",
       "   302,\n",
       "   313,\n",
       "   338,\n",
       "   720,\n",
       "   46,\n",
       "   342,\n",
       "   677,\n",
       "   309,\n",
       "   282,\n",
       "   2876,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   309,\n",
       "   708,\n",
       "   309,\n",
       "   282,\n",
       "   2073,\n",
       "   46,\n",
       "   406,\n",
       "   407,\n",
       "   265,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   328,\n",
       "   338,\n",
       "   386,\n",
       "   44,\n",
       "   391,\n",
       "   392,\n",
       "   468,\n",
       "   459,\n",
       "   119,\n",
       "   258,\n",
       "   1674,\n",
       "   354,\n",
       "   338,\n",
       "   2377,\n",
       "   304,\n",
       "   10,\n",
       "   670,\n",
       "   426,\n",
       "   265,\n",
       "   338,\n",
       "   386,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   844,\n",
       "   44,\n",
       "   337,\n",
       "   507,\n",
       "   733,\n",
       "   775,\n",
       "   302,\n",
       "   46,\n",
       "   1127,\n",
       "   349,\n",
       "   850,\n",
       "   309,\n",
       "   328,\n",
       "   524,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   627,\n",
       "   2377,\n",
       "   476,\n",
       "   937,\n",
       "   386,\n",
       "   565,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   732,\n",
       "   44,\n",
       "   406,\n",
       "   44,\n",
       "   363,\n",
       "   469,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   629,\n",
       "   2377,\n",
       "   505,\n",
       "   10,\n",
       "   2826,\n",
       "   44,\n",
       "   360,\n",
       "   1208,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   263,\n",
       "   261,\n",
       "   1674,\n",
       "   354,\n",
       "   406,\n",
       "   384,\n",
       "   2377,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   364,\n",
       "   2876,\n",
       "   387,\n",
       "   493,\n",
       "   708,\n",
       "   360,\n",
       "   405,\n",
       "   1714,\n",
       "   266,\n",
       "   1398,\n",
       "   766,\n",
       "   558,\n",
       "   46,\n",
       "   1559,\n",
       "   360,\n",
       "   1699,\n",
       "   44,\n",
       "   406,\n",
       "   943,\n",
       "   338,\n",
       "   386,\n",
       "   387,\n",
       "   1714,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   297,\n",
       "   338,\n",
       "   2377,\n",
       "   46,\n",
       "   312,\n",
       "   722,\n",
       "   536,\n",
       "   377,\n",
       "   708,\n",
       "   360,\n",
       "   365,\n",
       "   1208,\n",
       "   266,\n",
       "   1228,\n",
       "   458,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   528,\n",
       "   402,\n",
       "   2456,\n",
       "   626,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   508,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   313,\n",
       "   261,\n",
       "   631,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   258,\n",
       "   2489,\n",
       "   528,\n",
       "   708,\n",
       "   285,\n",
       "   704,\n",
       "   365,\n",
       "   561,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1213,\n",
       "   462,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   586,\n",
       "   2456,\n",
       "   626,\n",
       "   377,\n",
       "   266,\n",
       "   973,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1402,\n",
       "   817,\n",
       "   313,\n",
       "   261,\n",
       "   527,\n",
       "   634,\n",
       "   285,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   501,\n",
       "   365,\n",
       "   664,\n",
       "   1333,\n",
       "   383,\n",
       "   405,\n",
       "   1455,\n",
       "   297,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   532,\n",
       "   756,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   493,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   2241,\n",
       "   776,\n",
       "   261,\n",
       "   501,\n",
       "   266,\n",
       "   1233,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   354,\n",
       "   475,\n",
       "   46,\n",
       "   316,\n",
       "   703,\n",
       "   266,\n",
       "   322,\n",
       "   626,\n",
       "   263,\n",
       "   340,\n",
       "   2771,\n",
       "   304,\n",
       "   10,\n",
       "   2120,\n",
       "   626,\n",
       "   477,\n",
       "   328,\n",
       "   261,\n",
       "   1455,\n",
       "   297,\n",
       "   1333,\n",
       "   431,\n",
       "   327,\n",
       "   46,\n",
       "   931,\n",
       "   309,\n",
       "   282,\n",
       "   397,\n",
       "   265,\n",
       "   483,\n",
       "   584,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   677,\n",
       "   285,\n",
       "   1341,\n",
       "   673,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   316,\n",
       "   426,\n",
       "   265,\n",
       "   261,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   962,\n",
       "   266,\n",
       "   660,\n",
       "   673,\n",
       "   2489,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1139,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1367,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   601,\n",
       "   261,\n",
       "   988,\n",
       "   327,\n",
       "   46,\n",
       "   710,\n",
       "   2456,\n",
       "   626,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]},\n",
       " {'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   779,\n",
       "   402,\n",
       "   1221,\n",
       "   282,\n",
       "   2439,\n",
       "   810,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   1798,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   322,\n",
       "   413,\n",
       "   46,\n",
       "   317,\n",
       "   1090,\n",
       "   44,\n",
       "   337,\n",
       "   743,\n",
       "   1221,\n",
       "   46,\n",
       "   1174,\n",
       "   349,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   476,\n",
       "   543,\n",
       "   261,\n",
       "   390,\n",
       "   779,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   506,\n",
       "   450,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   947,\n",
       "   44,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   46,\n",
       "   337,\n",
       "   743,\n",
       "   1192,\n",
       "   266,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   2916,\n",
       "   505,\n",
       "   10,\n",
       "   1804,\n",
       "   536,\n",
       "   496,\n",
       "   409,\n",
       "   407,\n",
       "   265,\n",
       "   432,\n",
       "   261,\n",
       "   1798,\n",
       "   735,\n",
       "   833,\n",
       "   46,\n",
       "   316,\n",
       "   1588,\n",
       "   583,\n",
       "   266,\n",
       "   578,\n",
       "   371,\n",
       "   258,\n",
       "   1350,\n",
       "   46,\n",
       "   316,\n",
       "   1533,\n",
       "   383,\n",
       "   261,\n",
       "   631,\n",
       "   468,\n",
       "   533,\n",
       "   630,\n",
       "   1061,\n",
       "   46,\n",
       "   707,\n",
       "   44,\n",
       "   1221,\n",
       "   1588,\n",
       "   265,\n",
       "   261,\n",
       "   1306,\n",
       "   371,\n",
       "   261,\n",
       "   686,\n",
       "   266,\n",
       "   1041,\n",
       "   265,\n",
       "   261,\n",
       "   631,\n",
       "   44,\n",
       "   317,\n",
       "   1936,\n",
       "   44,\n",
       "   631,\n",
       "   44,\n",
       "   432,\n",
       "   627,\n",
       "   545,\n",
       "   358,\n",
       "   735,\n",
       "   2916,\n",
       "   266,\n",
       "   364,\n",
       "   1764,\n",
       "   1782,\n",
       "   687,\n",
       "   10,\n",
       "   412,\n",
       "   631,\n",
       "   837,\n",
       "   1221,\n",
       "   384,\n",
       "   946,\n",
       "   266,\n",
       "   389,\n",
       "   512,\n",
       "   832,\n",
       "   1061,\n",
       "   1366,\n",
       "   354,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   833,\n",
       "   266,\n",
       "   364,\n",
       "   391,\n",
       "   1192,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   848,\n",
       "   349,\n",
       "   44,\n",
       "   390,\n",
       "   779,\n",
       "   44,\n",
       "   387,\n",
       "   1232,\n",
       "   524,\n",
       "   735,\n",
       "   2916,\n",
       "   46,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   519,\n",
       "   337,\n",
       "   621,\n",
       "   1764,\n",
       "   1782,\n",
       "   972,\n",
       "   46,\n",
       "   1046,\n",
       "   384,\n",
       "   325,\n",
       "   458,\n",
       "   414,\n",
       "   710,\n",
       "   391,\n",
       "   44,\n",
       "   1221,\n",
       "   266,\n",
       "   261,\n",
       "   1798,\n",
       "   477,\n",
       "   266,\n",
       "   692,\n",
       "   561,\n",
       "   413,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   313,\n",
       "   258,\n",
       "   1492,\n",
       "   1328,\n",
       "   371,\n",
       "   1286,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   380,\n",
       "   496,\n",
       "   708,\n",
       "   309,\n",
       "   449,\n",
       "   364,\n",
       "   500,\n",
       "   671,\n",
       "   413,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   558,\n",
       "   1286,\n",
       "   405,\n",
       "   346,\n",
       "   266,\n",
       "   973,\n",
       "   44,\n",
       "   409,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   563,\n",
       "   266,\n",
       "   2404,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   842,\n",
       "   118,\n",
       "   1245,\n",
       "   371,\n",
       "   261,\n",
       "   346,\n",
       "   1286,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   536,\n",
       "   258,\n",
       "   2578,\n",
       "   302,\n",
       "   313,\n",
       "   832,\n",
       "   2885,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   2922,\n",
       "   861,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   741,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   364,\n",
       "   265,\n",
       "   322,\n",
       "   496,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   540,\n",
       "   486,\n",
       "   868,\n",
       "   708,\n",
       "   349,\n",
       "   500,\n",
       "   1304,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   383,\n",
       "   1027,\n",
       "   2526,\n",
       "   398,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   258,\n",
       "   390,\n",
       "   833,\n",
       "   304,\n",
       "   10,\n",
       "   934,\n",
       "   397,\n",
       "   426,\n",
       "   354,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   2025,\n",
       "   673,\n",
       "   266,\n",
       "   673,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   823,\n",
       "   313,\n",
       "   261,\n",
       "   1492,\n",
       "   552,\n",
       "   265,\n",
       "   711,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   266,\n",
       "   325,\n",
       "   776,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   377,\n",
       "   708,\n",
       "   309,\n",
       "   365,\n",
       "   664,\n",
       "   413,\n",
       "   972,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   691,\n",
       "   383,\n",
       "   1011,\n",
       "   1314,\n",
       "   469,\n",
       "   322,\n",
       "   258,\n",
       "   561,\n",
       "   1253,\n",
       "   46,\n",
       "   710,\n",
       "   360,\n",
       "   431,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "next(batched(tds['train'].select(range(50)), bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ead127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches = list(batched(tds['train'].select(range(100)), bs))\n",
    "len(input_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8732d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'input_ids': [516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    507,\n",
       "    258,\n",
       "    775,\n",
       "    302,\n",
       "    313,\n",
       "    338,\n",
       "    720,\n",
       "    46,\n",
       "    342,\n",
       "    677,\n",
       "    309,\n",
       "    282,\n",
       "    2876,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    309,\n",
       "    708,\n",
       "    309,\n",
       "    282,\n",
       "    2073,\n",
       "    46,\n",
       "    406,\n",
       "    407,\n",
       "    265,\n",
       "    850,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    328,\n",
       "    338,\n",
       "    386,\n",
       "    44,\n",
       "    391,\n",
       "    392,\n",
       "    468,\n",
       "    459,\n",
       "    119,\n",
       "    258,\n",
       "    1674,\n",
       "    354,\n",
       "    338,\n",
       "    2377,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    426,\n",
       "    265,\n",
       "    338,\n",
       "    386,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    844,\n",
       "    44,\n",
       "    337,\n",
       "    507,\n",
       "    733,\n",
       "    775,\n",
       "    302,\n",
       "    46,\n",
       "    1127,\n",
       "    349,\n",
       "    850,\n",
       "    309,\n",
       "    328,\n",
       "    524,\n",
       "    266,\n",
       "    459,\n",
       "    119,\n",
       "    627,\n",
       "    2377,\n",
       "    476,\n",
       "    937,\n",
       "    386,\n",
       "    565,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    732,\n",
       "    44,\n",
       "    406,\n",
       "    44,\n",
       "    363,\n",
       "    469,\n",
       "    850,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    1125,\n",
       "    629,\n",
       "    2377,\n",
       "    505,\n",
       "    10,\n",
       "    2826,\n",
       "    44,\n",
       "    360,\n",
       "    1208,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    459,\n",
       "    119,\n",
       "    263,\n",
       "    261,\n",
       "    1674,\n",
       "    354,\n",
       "    406,\n",
       "    384,\n",
       "    2377,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    364,\n",
       "    2876,\n",
       "    387,\n",
       "    493,\n",
       "    708,\n",
       "    360,\n",
       "    405,\n",
       "    1714,\n",
       "    266,\n",
       "    1398,\n",
       "    766,\n",
       "    558,\n",
       "    46,\n",
       "    1559,\n",
       "    360,\n",
       "    1699,\n",
       "    44,\n",
       "    406,\n",
       "    943,\n",
       "    338,\n",
       "    386,\n",
       "    387,\n",
       "    1714,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    1125,\n",
       "    297,\n",
       "    338,\n",
       "    2377,\n",
       "    46,\n",
       "    312,\n",
       "    722,\n",
       "    536,\n",
       "    377,\n",
       "    708,\n",
       "    360,\n",
       "    365,\n",
       "    1208,\n",
       "    266,\n",
       "    1228,\n",
       "    458,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    528,\n",
       "    402,\n",
       "    2456,\n",
       "    626,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    508,\n",
       "    265,\n",
       "    483,\n",
       "    737,\n",
       "    266,\n",
       "    325,\n",
       "    313,\n",
       "    261,\n",
       "    631,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    258,\n",
       "    2489,\n",
       "    528,\n",
       "    708,\n",
       "    285,\n",
       "    704,\n",
       "    365,\n",
       "    561,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    1213,\n",
       "    462,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    586,\n",
       "    2456,\n",
       "    626,\n",
       "    377,\n",
       "    266,\n",
       "    973,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    1402,\n",
       "    817,\n",
       "    313,\n",
       "    261,\n",
       "    527,\n",
       "    634,\n",
       "    285,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    501,\n",
       "    365,\n",
       "    664,\n",
       "    1333,\n",
       "    383,\n",
       "    405,\n",
       "    1455,\n",
       "    297,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    532,\n",
       "    756,\n",
       "    261,\n",
       "    1333,\n",
       "    1455,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    493,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    2241,\n",
       "    776,\n",
       "    261,\n",
       "    501,\n",
       "    266,\n",
       "    1233,\n",
       "    261,\n",
       "    1333,\n",
       "    1455,\n",
       "    354,\n",
       "    475,\n",
       "    46,\n",
       "    316,\n",
       "    703,\n",
       "    266,\n",
       "    322,\n",
       "    626,\n",
       "    263,\n",
       "    340,\n",
       "    2771,\n",
       "    304,\n",
       "    10,\n",
       "    2120,\n",
       "    626,\n",
       "    477,\n",
       "    328,\n",
       "    261,\n",
       "    1455,\n",
       "    297,\n",
       "    1333,\n",
       "    431,\n",
       "    327,\n",
       "    46,\n",
       "    931,\n",
       "    309,\n",
       "    282,\n",
       "    397,\n",
       "    265,\n",
       "    483,\n",
       "    584,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    677,\n",
       "    285,\n",
       "    1341,\n",
       "    673,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    316,\n",
       "    426,\n",
       "    265,\n",
       "    261,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    962,\n",
       "    266,\n",
       "    660,\n",
       "    673,\n",
       "    2489,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    1139,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    1367,\n",
       "    265,\n",
       "    483,\n",
       "    737,\n",
       "    266,\n",
       "    325,\n",
       "    601,\n",
       "    261,\n",
       "    988,\n",
       "    327,\n",
       "    46,\n",
       "    710,\n",
       "    2456,\n",
       "    626,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]},\n",
       "  {'input_ids': [516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    779,\n",
       "    402,\n",
       "    1221,\n",
       "    282,\n",
       "    2439,\n",
       "    810,\n",
       "    261,\n",
       "    389,\n",
       "    440,\n",
       "    46,\n",
       "    316,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    1798,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    322,\n",
       "    413,\n",
       "    46,\n",
       "    317,\n",
       "    1090,\n",
       "    44,\n",
       "    337,\n",
       "    743,\n",
       "    1221,\n",
       "    46,\n",
       "    1174,\n",
       "    349,\n",
       "    367,\n",
       "    265,\n",
       "    325,\n",
       "    476,\n",
       "    543,\n",
       "    261,\n",
       "    390,\n",
       "    779,\n",
       "    46,\n",
       "    284,\n",
       "    1798,\n",
       "    506,\n",
       "    450,\n",
       "    1221,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    947,\n",
       "    44,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    367,\n",
       "    265,\n",
       "    325,\n",
       "    46,\n",
       "    337,\n",
       "    743,\n",
       "    1192,\n",
       "    266,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    735,\n",
       "    2916,\n",
       "    505,\n",
       "    10,\n",
       "    1804,\n",
       "    536,\n",
       "    496,\n",
       "    409,\n",
       "    407,\n",
       "    265,\n",
       "    432,\n",
       "    261,\n",
       "    1798,\n",
       "    735,\n",
       "    833,\n",
       "    46,\n",
       "    316,\n",
       "    1588,\n",
       "    583,\n",
       "    266,\n",
       "    578,\n",
       "    371,\n",
       "    258,\n",
       "    1350,\n",
       "    46,\n",
       "    316,\n",
       "    1533,\n",
       "    383,\n",
       "    261,\n",
       "    631,\n",
       "    468,\n",
       "    533,\n",
       "    630,\n",
       "    1061,\n",
       "    46,\n",
       "    707,\n",
       "    44,\n",
       "    1221,\n",
       "    1588,\n",
       "    265,\n",
       "    261,\n",
       "    1306,\n",
       "    371,\n",
       "    261,\n",
       "    686,\n",
       "    266,\n",
       "    1041,\n",
       "    265,\n",
       "    261,\n",
       "    631,\n",
       "    44,\n",
       "    317,\n",
       "    1936,\n",
       "    44,\n",
       "    631,\n",
       "    44,\n",
       "    432,\n",
       "    627,\n",
       "    545,\n",
       "    358,\n",
       "    735,\n",
       "    2916,\n",
       "    266,\n",
       "    364,\n",
       "    1764,\n",
       "    1782,\n",
       "    687,\n",
       "    10,\n",
       "    412,\n",
       "    631,\n",
       "    837,\n",
       "    1221,\n",
       "    384,\n",
       "    946,\n",
       "    266,\n",
       "    389,\n",
       "    512,\n",
       "    832,\n",
       "    1061,\n",
       "    1366,\n",
       "    354,\n",
       "    261,\n",
       "    389,\n",
       "    440,\n",
       "    46,\n",
       "    284,\n",
       "    1798,\n",
       "    548,\n",
       "    265,\n",
       "    735,\n",
       "    833,\n",
       "    266,\n",
       "    364,\n",
       "    391,\n",
       "    1192,\n",
       "    46,\n",
       "    316,\n",
       "    382,\n",
       "    1221,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    848,\n",
       "    349,\n",
       "    44,\n",
       "    390,\n",
       "    779,\n",
       "    44,\n",
       "    387,\n",
       "    1232,\n",
       "    524,\n",
       "    735,\n",
       "    2916,\n",
       "    46,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    735,\n",
       "    519,\n",
       "    337,\n",
       "    621,\n",
       "    1764,\n",
       "    1782,\n",
       "    972,\n",
       "    46,\n",
       "    1046,\n",
       "    384,\n",
       "    325,\n",
       "    458,\n",
       "    414,\n",
       "    710,\n",
       "    391,\n",
       "    44,\n",
       "    1221,\n",
       "    266,\n",
       "    261,\n",
       "    1798,\n",
       "    477,\n",
       "    266,\n",
       "    692,\n",
       "    561,\n",
       "    413,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    1492,\n",
       "    1328,\n",
       "    371,\n",
       "    1286,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    380,\n",
       "    496,\n",
       "    708,\n",
       "    309,\n",
       "    449,\n",
       "    364,\n",
       "    500,\n",
       "    671,\n",
       "    413,\n",
       "    46,\n",
       "    1407,\n",
       "    261,\n",
       "    558,\n",
       "    1286,\n",
       "    405,\n",
       "    346,\n",
       "    266,\n",
       "    973,\n",
       "    44,\n",
       "    409,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    563,\n",
       "    266,\n",
       "    2404,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    842,\n",
       "    118,\n",
       "    1245,\n",
       "    371,\n",
       "    261,\n",
       "    346,\n",
       "    1286,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    536,\n",
       "    258,\n",
       "    2578,\n",
       "    302,\n",
       "    313,\n",
       "    832,\n",
       "    2885,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    2922,\n",
       "    861,\n",
       "    46,\n",
       "    284,\n",
       "    861,\n",
       "    741,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    364,\n",
       "    265,\n",
       "    322,\n",
       "    496,\n",
       "    46,\n",
       "    284,\n",
       "    861,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    540,\n",
       "    486,\n",
       "    868,\n",
       "    708,\n",
       "    349,\n",
       "    500,\n",
       "    1304,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    383,\n",
       "    1027,\n",
       "    2526,\n",
       "    398,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    548,\n",
       "    265,\n",
       "    735,\n",
       "    258,\n",
       "    390,\n",
       "    833,\n",
       "    304,\n",
       "    10,\n",
       "    934,\n",
       "    397,\n",
       "    426,\n",
       "    354,\n",
       "    44,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    2025,\n",
       "    673,\n",
       "    266,\n",
       "    673,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    46,\n",
       "    1407,\n",
       "    261,\n",
       "    823,\n",
       "    313,\n",
       "    261,\n",
       "    1492,\n",
       "    552,\n",
       "    265,\n",
       "    711,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    266,\n",
       "    325,\n",
       "    776,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    377,\n",
       "    708,\n",
       "    309,\n",
       "    365,\n",
       "    664,\n",
       "    413,\n",
       "    972,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    691,\n",
       "    383,\n",
       "    1011,\n",
       "    1314,\n",
       "    469,\n",
       "    322,\n",
       "    258,\n",
       "    561,\n",
       "    1253,\n",
       "    46,\n",
       "    710,\n",
       "    360,\n",
       "    431,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]}],\n",
       " [{'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    46,\n",
       "    406,\n",
       "    532,\n",
       "    265,\n",
       "    1371,\n",
       "    392,\n",
       "    282,\n",
       "    258,\n",
       "    2053,\n",
       "    2184,\n",
       "    2108,\n",
       "    46,\n",
       "    342,\n",
       "    636,\n",
       "    313,\n",
       "    258,\n",
       "    346,\n",
       "    1576,\n",
       "    328,\n",
       "    338,\n",
       "    696,\n",
       "    413,\n",
       "    44,\n",
       "    258,\n",
       "    456,\n",
       "    266,\n",
       "    258,\n",
       "    465,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    1000,\n",
       "    585,\n",
       "    313,\n",
       "    261,\n",
       "    1576,\n",
       "    44,\n",
       "    406,\n",
       "    507,\n",
       "    258,\n",
       "    346,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    282,\n",
       "    313,\n",
       "    261,\n",
       "    936,\n",
       "    371,\n",
       "    338,\n",
       "    442,\n",
       "    1043,\n",
       "    46,\n",
       "    342,\n",
       "    407,\n",
       "    265,\n",
       "    650,\n",
       "    334,\n",
       "    292,\n",
       "    371,\n",
       "    309,\n",
       "    44,\n",
       "    409,\n",
       "    392,\n",
       "    282,\n",
       "    607,\n",
       "    371,\n",
       "    261,\n",
       "    2212,\n",
       "    383,\n",
       "    636,\n",
       "    401,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    543,\n",
       "    338,\n",
       "    413,\n",
       "    44,\n",
       "    261,\n",
       "    456,\n",
       "    266,\n",
       "    261,\n",
       "    465,\n",
       "    44,\n",
       "    265,\n",
       "    432,\n",
       "    338,\n",
       "    46,\n",
       "    312,\n",
       "    431,\n",
       "    1228,\n",
       "    458,\n",
       "    265,\n",
       "    873,\n",
       "    261,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    46,\n",
       "    284,\n",
       "    2212,\n",
       "    282,\n",
       "    496,\n",
       "    44,\n",
       "    409,\n",
       "    309,\n",
       "    507,\n",
       "    258,\n",
       "    545,\n",
       "    584,\n",
       "    697,\n",
       "    46,\n",
       "    406,\n",
       "    44,\n",
       "    261,\n",
       "    456,\n",
       "    44,\n",
       "    266,\n",
       "    261,\n",
       "    465,\n",
       "    405,\n",
       "    377,\n",
       "    360,\n",
       "    468,\n",
       "    325,\n",
       "    1807,\n",
       "    261,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    313,\n",
       "    261,\n",
       "    936,\n",
       "    46,\n",
       "    710,\n",
       "    360,\n",
       "    431,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    346,\n",
       "    1903,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    46,\n",
       "    284,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    532,\n",
       "    265,\n",
       "    1349,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    431,\n",
       "    327,\n",
       "    734,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    380,\n",
       "    377,\n",
       "    634,\n",
       "    309,\n",
       "    468,\n",
       "    1349,\n",
       "    266,\n",
       "    2290,\n",
       "    313,\n",
       "    261,\n",
       "    1903,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    497,\n",
       "    402,\n",
       "    341,\n",
       "    552,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    46,\n",
       "    341,\n",
       "    266,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    1814,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    458,\n",
       "    46,\n",
       "    312,\n",
       "    703,\n",
       "    266,\n",
       "    365,\n",
       "    258,\n",
       "    653,\n",
       "    371,\n",
       "    442,\n",
       "    46,\n",
       "    284,\n",
       "    631,\n",
       "    282,\n",
       "    1875,\n",
       "    44,\n",
       "    266,\n",
       "    261,\n",
       "    686,\n",
       "    282,\n",
       "    1061,\n",
       "    304,\n",
       "    10,\n",
       "    1260,\n",
       "    258,\n",
       "    1000,\n",
       "    44,\n",
       "    309,\n",
       "    282,\n",
       "    397,\n",
       "    387,\n",
       "    341,\n",
       "    265,\n",
       "    483,\n",
       "    584,\n",
       "    46,\n",
       "    316,\n",
       "    323,\n",
       "    1418,\n",
       "    265,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    266,\n",
       "    760,\n",
       "    309,\n",
       "    258,\n",
       "    346,\n",
       "    695,\n",
       "    46,\n",
       "    284,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    282,\n",
       "    496,\n",
       "    265,\n",
       "    547,\n",
       "    341,\n",
       "    483,\n",
       "    44,\n",
       "    409,\n",
       "    309,\n",
       "    677,\n",
       "    360,\n",
       "    529,\n",
       "    325,\n",
       "    458,\n",
       "    601,\n",
       "    1105,\n",
       "    46,\n",
       "    707,\n",
       "    44,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    1102,\n",
       "    1349,\n",
       "    297,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    44,\n",
       "    2425,\n",
       "    387,\n",
       "    261,\n",
       "    988,\n",
       "    442,\n",
       "    327,\n",
       "    328,\n",
       "    341,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    563,\n",
       "    1020,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    1974,\n",
       "    2887,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    46,\n",
       "    342,\n",
       "    282,\n",
       "    704,\n",
       "    496,\n",
       "    708,\n",
       "    392,\n",
       "    1036,\n",
       "    338,\n",
       "    1226,\n",
       "    446,\n",
       "    44,\n",
       "    258,\n",
       "    2963,\n",
       "    302,\n",
       "    46,\n",
       "    342,\n",
       "    506,\n",
       "    1577,\n",
       "    313,\n",
       "    338,\n",
       "    605,\n",
       "    409,\n",
       "    468,\n",
       "    364,\n",
       "    572,\n",
       "    309,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    1163,\n",
       "    327,\n",
       "    44,\n",
       "    406,\n",
       "    426,\n",
       "    265,\n",
       "    261,\n",
       "    527,\n",
       "    265,\n",
       "    325,\n",
       "    46,\n",
       "    342,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    2010,\n",
       "    371,\n",
       "    686,\n",
       "    266,\n",
       "    578,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    1446,\n",
       "    322,\n",
       "    401,\n",
       "    46,\n",
       "    342,\n",
       "    587,\n",
       "    338,\n",
       "    789,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    265,\n",
       "    391,\n",
       "    881,\n",
       "    309,\n",
       "    266,\n",
       "    506,\n",
       "    387,\n",
       "    338,\n",
       "    446,\n",
       "    46,\n",
       "    342,\n",
       "    536,\n",
       "    575,\n",
       "    450,\n",
       "    261,\n",
       "    1554,\n",
       "    283,\n",
       "    371,\n",
       "    261,\n",
       "    2010,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    1487,\n",
       "    309,\n",
       "    485,\n",
       "    266,\n",
       "    382,\n",
       "    383,\n",
       "    309,\n",
       "    282,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    33,\n",
       "    342,\n",
       "    282,\n",
       "    391,\n",
       "    377,\n",
       "    383,\n",
       "    392,\n",
       "    507,\n",
       "    309,\n",
       "    46,\n",
       "    939,\n",
       "    383,\n",
       "    327,\n",
       "    354,\n",
       "    44,\n",
       "    406,\n",
       "    282,\n",
       "    858,\n",
       "    1974,\n",
       "    2887,\n",
       "    601,\n",
       "    46,\n",
       "    342,\n",
       "    477,\n",
       "    328,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    589,\n",
       "    327,\n",
       "    266,\n",
       "    704,\n",
       "    1102,\n",
       "    309,\n",
       "    1372,\n",
       "    265,\n",
       "    338,\n",
       "    46,\n",
       "    710,\n",
       "    634,\n",
       "    392,\n",
       "    382,\n",
       "    1851,\n",
       "    1120,\n",
       "    44,\n",
       "    392,\n",
       "    529,\n",
       "    963,\n",
       "    266,\n",
       "    1195,\n",
       "    756,\n",
       "    392,\n",
       "    507,\n",
       "    338,\n",
       "    446,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    2770,\n",
       "    1020,\n",
       "    44,\n",
       "    401,\n",
       "    636,\n",
       "    258,\n",
       "    390,\n",
       "    497,\n",
       "    402,\n",
       "    341,\n",
       "    46,\n",
       "    341,\n",
       "    508,\n",
       "    265,\n",
       "    740,\n",
       "    266,\n",
       "    325,\n",
       "    697,\n",
       "    46,\n",
       "    451,\n",
       "    327,\n",
       "    44,\n",
       "    341,\n",
       "    382,\n",
       "    258,\n",
       "    1178,\n",
       "    313,\n",
       "    261,\n",
       "    527,\n",
       "    46,\n",
       "    316,\n",
       "    282,\n",
       "    729,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    1528,\n",
       "    261,\n",
       "    1178,\n",
       "    304,\n",
       "    10,\n",
       "    645,\n",
       "    426,\n",
       "    265,\n",
       "    340,\n",
       "    358,\n",
       "    44,\n",
       "    1669,\n",
       "    44,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    914,\n",
       "    384,\n",
       "    1922,\n",
       "    261,\n",
       "    1178,\n",
       "    414,\n",
       "    1669,\n",
       "    565,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    732,\n",
       "    44,\n",
       "    781,\n",
       "    384,\n",
       "    483,\n",
       "    414,\n",
       "    312,\n",
       "    275,\n",
       "    1879,\n",
       "    368,\n",
       "    328,\n",
       "    261,\n",
       "    558,\n",
       "    1006,\n",
       "    266,\n",
       "    1564,\n",
       "    387,\n",
       "    261,\n",
       "    1178,\n",
       "    265,\n",
       "    322,\n",
       "    103,\n",
       "    270,\n",
       "    46,\n",
       "    931,\n",
       "    360,\n",
       "    837,\n",
       "    261,\n",
       "    1504,\n",
       "    317,\n",
       "    71,\n",
       "    111,\n",
       "    414,\n",
       "    44,\n",
       "    360,\n",
       "    548,\n",
       "    1374,\n",
       "    445,\n",
       "    737,\n",
       "    445,\n",
       "    360,\n",
       "    468,\n",
       "    304,\n",
       "    10,\n",
       "    645,\n",
       "    266,\n",
       "    1669,\n",
       "    598,\n",
       "    328,\n",
       "    431,\n",
       "    470,\n",
       "    2310,\n",
       "    263,\n",
       "    44,\n",
       "    1847,\n",
       "    266,\n",
       "    1111,\n",
       "    442,\n",
       "    46,\n",
       "    312,\n",
       "    468,\n",
       "    735,\n",
       "    261,\n",
       "    861,\n",
       "    313,\n",
       "    470,\n",
       "    1309,\n",
       "    445,\n",
       "    360,\n",
       "    334,\n",
       "    2490,\n",
       "    265,\n",
       "    261,\n",
       "    2317,\n",
       "    2315,\n",
       "    46,\n",
       "    964,\n",
       "    261,\n",
       "    882,\n",
       "    44,\n",
       "    341,\n",
       "    1583,\n",
       "    261,\n",
       "    1178,\n",
       "    266,\n",
       "    1669,\n",
       "    552,\n",
       "    313,\n",
       "    459,\n",
       "    1324,\n",
       "    262,\n",
       "    46,\n",
       "    312,\n",
       "    405,\n",
       "    722,\n",
       "    391,\n",
       "    377,\n",
       "    266,\n",
       "    900,\n",
       "    371,\n",
       "    493,\n",
       "    2933,\n",
       "    46,\n",
       "    312,\n",
       "    2798,\n",
       "    1771,\n",
       "    328,\n",
       "    470,\n",
       "    413,\n",
       "    266,\n",
       "    365,\n",
       "    258,\n",
       "    968,\n",
       "    327,\n",
       "    450,\n",
       "    261,\n",
       "    527,\n",
       "    46]}]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59606a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_subset = tds['train'].select(range(100))\n",
    "trn_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c607d",
   "metadata": {},
   "source": [
    "### Sequence packer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "from composer.core.types import Batch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "import math\n",
    "from composer.core import Time\n",
    "\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def __len__(self): return self.src_dataloader_len\n",
    "    \n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences for causal attention.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                labels = np.full_like(batch, self.pad_token_id)\n",
    "                labels[:, :-1] = batch[:, 1:]\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            yield yieldval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95469fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_batches = list(batched(xds, bs))\n",
    "out_pseq_len= 250\n",
    "d = {\n",
    "    \"src_iterable\": input_batches,\n",
    "    \"src_batch_size\": bs,\n",
    "    \"src_max_seq_len\": 1,\n",
    "    \"out_batch_size\": bs,\n",
    "    \"out_pseq_len\": out_pseq_len,\n",
    "    \"buffer_size\": 5,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 250]), torch.Size([4, 250]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches = next(iter(CausalSequencePacker(**d)))\n",
    "out_batches['input_ids'].shape, out_batches['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0dc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 516,  327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,\n",
       "           313,  338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,\n",
       "           309,  708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,\n",
       "           302,  328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,\n",
       "           354,  338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,\n",
       "            44,  317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,\n",
       "           850,  309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,\n",
       "           565,  266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,\n",
       "           261,  775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360,\n",
       "          1208,  261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,\n",
       "           384, 2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405,\n",
       "          1714,  266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,\n",
       "           338,  386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,\n",
       "            46,  312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  401,  282,  258,  390,  528,  402, 2456,\n",
       "           626,   46, 2456,  626,  508,  265,  483,  737,  266,  325,  313,  261,\n",
       "           631,   46, 2456,  626,  282,  258, 2489,  528,  708,  285,  704,  365,\n",
       "           561,  268,  117,  417,   46, 1213,  462,  268,  117,  417,  586, 2456,\n",
       "           626,  377,  266,  973,  304,   10,  516,  327,   44, 2456,  626,  282,\n",
       "          1402,  817,  313,  261,  527,  634,  285,  382,  258,  346,  501,   46,\n",
       "           284,  501,  365,  664, 1333,  383,  405, 1455,  297,   46, 2456,  626,\n",
       "           532,  756,  261, 1333, 1455,  266,  407,  265,  325,  328,  493,   46,\n",
       "          2456,  626, 2241,  776,  261,  501,  266, 1233,  261, 1333, 1455,  354,\n",
       "           475,   46,  316,  703,  266,  322,  626,  263,  340, 2771,  304,   10,\n",
       "          2120,  626,  477,  328,  261, 1455,  297, 1333,  431,  327,   46,  931,\n",
       "           309,  282,  397,  265,  483,  584,   44, 2456,  626,  677,  285, 1341,\n",
       "           673,  268,  117,  417,   46,  316,  426,  265,  261,  268,  117,  417,\n",
       "           962,  266,  660,  673, 2489,  268,  117,  417,   46, 1139,   44, 2456,\n",
       "           626,  282, 1367,  265,  483,  737,  266,  325,  601,  261,  988,  327,\n",
       "            46,  710, 2456,  626,  636,  992,  933,  886,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 516,  327,   44,  258,  390,  779,  402, 1221,  282, 2439,  810,  261,\n",
       "           389,  440,   46,  316,  382,  258,  346, 1798,  266,  407,  265,  322,\n",
       "           413,   46,  317, 1090,   44,  337,  743, 1221,   46, 1174,  349,  367,\n",
       "           265,  325,  476,  543,  261,  390,  779,   46,  284, 1798,  506,  450,\n",
       "          1221,  266,  323,   44,  317,  947,   44,  337,  862,  492,  367,  265,\n",
       "           325,   46,  337,  743, 1192,  266,  337,  862,  492,  735, 2916,  505,\n",
       "            10, 1804,  536,  496,  409,  407,  265,  432,  261, 1798,  735,  833,\n",
       "            46,  316, 1588,  583,  266,  578,  371,  258, 1350,   46,  316, 1533,\n",
       "           383,  261,  631,  468,  533,  630, 1061,   46,  707,   44, 1221, 1588,\n",
       "           265,  261, 1306,  371,  261,  686,  266, 1041,  265,  261,  631,   44,\n",
       "           317, 1936,   44,  631,   44,  432,  627,  545,  358,  735, 2916,  266,\n",
       "           364, 1764, 1782,  687,   10,  412,  631,  837, 1221,  384,  946,  266,\n",
       "           389,  512,  832, 1061, 1366,  354,  261,  389,  440,   46,  284, 1798,\n",
       "           548,  265,  735,  833,  266,  364,  391, 1192,   46,  316,  382, 1221,\n",
       "           266,  323,   44,  317,  848,  349,   44,  390,  779,   44,  387, 1232,\n",
       "           524,  735, 2916,   46,  337,  862,  492,  735,  519,  337,  621, 1764,\n",
       "          1782,  972,   46, 1046,  384,  325,  458,  414,  710,  391,   44, 1221,\n",
       "           266,  261, 1798,  477,  266,  692,  561,  413,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  313,  258, 1492, 1328,  371, 1286,   44,\n",
       "           401,  282,  258,  390,  277,  370,  439,  501,   46,  284,  277,  370,\n",
       "           439,  501,  282,  380,  496,  708,  309,  449,  364,  500,  671,  413,\n",
       "            46, 1407,  261,  558, 1286,  405,  346,  266,  973,   44,  409,  261,\n",
       "           277,  370,  439,  501,  282,  563,  266, 2404,   46,  284,  277,  370,\n",
       "           439,  501,  282,  842,  118, 1245,  371,  261,  346, 1286,  304,   10,\n",
       "           516,  327,   44,  261,  277,  370,  439,  501,  536,  258, 2578,  302,\n",
       "           313,  832, 2885,   46,  421,  282,  258,  390, 2922,  861,   46,  284,\n",
       "           861,  741,  261,  277,  370,  439,  501,  364,  265,  322,  496,   46,\n",
       "           284,  861,  323,   44,  317,  540,  486,  868,  708,  349,  500, 1304,\n",
       "           277,  370, 1582,  383, 1027, 2526,  398,  284,  277,  370,  439,  501,\n",
       "           548,  265,  735,  258,  390,  833,  304,   10,  934,  397,  426,  354,\n",
       "            44,  261,  277,  370,  439,  501, 2025,  673,  266,  673,  277,  370,\n",
       "          1582,   46, 1407,  261,  823,  313,  261, 1492,  552,  265,  711,  261,\n",
       "           277,  370, 1582,  266,  325,  776,  261,  277,  370,  439,  501,   46,\n",
       "           284,  277,  370,  439,  501,  282,  377,  708,  309,  365,  664,  413,\n",
       "           972,   46,  284,  277,  370,  439,  501,  691,  383, 1011, 1314,  469,\n",
       "           322,  258,  561, 1253,   46,  710,  360,  431,  636,  992,  933,  886,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'labels': tensor([[ 327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,  313,\n",
       "           338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,  309,\n",
       "           708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,  302,\n",
       "           328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,  354,\n",
       "           338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,   44,\n",
       "           317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,  850,\n",
       "           309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,  565,\n",
       "           266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,  261,\n",
       "           775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360, 1208,\n",
       "           261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,  384,\n",
       "          2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405, 1714,\n",
       "           266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,  338,\n",
       "           386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,   46,\n",
       "           312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,   46,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 438,  258,  397,   44,  401,  282,  258,  390,  528,  402, 2456,  626,\n",
       "            46, 2456,  626,  508,  265,  483,  737,  266,  325,  313,  261,  631,\n",
       "            46, 2456,  626,  282,  258, 2489,  528,  708,  285,  704,  365,  561,\n",
       "           268,  117,  417,   46, 1213,  462,  268,  117,  417,  586, 2456,  626,\n",
       "           377,  266,  973,  304,   10,  516,  327,   44, 2456,  626,  282, 1402,\n",
       "           817,  313,  261,  527,  634,  285,  382,  258,  346,  501,   46,  284,\n",
       "           501,  365,  664, 1333,  383,  405, 1455,  297,   46, 2456,  626,  532,\n",
       "           756,  261, 1333, 1455,  266,  407,  265,  325,  328,  493,   46, 2456,\n",
       "           626, 2241,  776,  261,  501,  266, 1233,  261, 1333, 1455,  354,  475,\n",
       "            46,  316,  703,  266,  322,  626,  263,  340, 2771,  304,   10, 2120,\n",
       "           626,  477,  328,  261, 1455,  297, 1333,  431,  327,   46,  931,  309,\n",
       "           282,  397,  265,  483,  584,   44, 2456,  626,  677,  285, 1341,  673,\n",
       "           268,  117,  417,   46,  316,  426,  265,  261,  268,  117,  417,  962,\n",
       "           266,  660,  673, 2489,  268,  117,  417,   46, 1139,   44, 2456,  626,\n",
       "           282, 1367,  265,  483,  737,  266,  325,  601,  261,  988,  327,   46,\n",
       "           710, 2456,  626,  636,  992,  933,  886,   46,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 327,   44,  258,  390,  779,  402, 1221,  282, 2439,  810,  261,  389,\n",
       "           440,   46,  316,  382,  258,  346, 1798,  266,  407,  265,  322,  413,\n",
       "            46,  317, 1090,   44,  337,  743, 1221,   46, 1174,  349,  367,  265,\n",
       "           325,  476,  543,  261,  390,  779,   46,  284, 1798,  506,  450, 1221,\n",
       "           266,  323,   44,  317,  947,   44,  337,  862,  492,  367,  265,  325,\n",
       "            46,  337,  743, 1192,  266,  337,  862,  492,  735, 2916,  505,   10,\n",
       "          1804,  536,  496,  409,  407,  265,  432,  261, 1798,  735,  833,   46,\n",
       "           316, 1588,  583,  266,  578,  371,  258, 1350,   46,  316, 1533,  383,\n",
       "           261,  631,  468,  533,  630, 1061,   46,  707,   44, 1221, 1588,  265,\n",
       "           261, 1306,  371,  261,  686,  266, 1041,  265,  261,  631,   44,  317,\n",
       "          1936,   44,  631,   44,  432,  627,  545,  358,  735, 2916,  266,  364,\n",
       "          1764, 1782,  687,   10,  412,  631,  837, 1221,  384,  946,  266,  389,\n",
       "           512,  832, 1061, 1366,  354,  261,  389,  440,   46,  284, 1798,  548,\n",
       "           265,  735,  833,  266,  364,  391, 1192,   46,  316,  382, 1221,  266,\n",
       "           323,   44,  317,  848,  349,   44,  390,  779,   44,  387, 1232,  524,\n",
       "           735, 2916,   46,  337,  862,  492,  735,  519,  337,  621, 1764, 1782,\n",
       "           972,   46, 1046,  384,  325,  458,  414,  710,  391,   44, 1221,  266,\n",
       "           261, 1798,  477,  266,  692,  561,  413,   46,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 438,  258,  397,   44,  313,  258, 1492, 1328,  371, 1286,   44,  401,\n",
       "           282,  258,  390,  277,  370,  439,  501,   46,  284,  277,  370,  439,\n",
       "           501,  282,  380,  496,  708,  309,  449,  364,  500,  671,  413,   46,\n",
       "          1407,  261,  558, 1286,  405,  346,  266,  973,   44,  409,  261,  277,\n",
       "           370,  439,  501,  282,  563,  266, 2404,   46,  284,  277,  370,  439,\n",
       "           501,  282,  842,  118, 1245,  371,  261,  346, 1286,  304,   10,  516,\n",
       "           327,   44,  261,  277,  370,  439,  501,  536,  258, 2578,  302,  313,\n",
       "           832, 2885,   46,  421,  282,  258,  390, 2922,  861,   46,  284,  861,\n",
       "           741,  261,  277,  370,  439,  501,  364,  265,  322,  496,   46,  284,\n",
       "           861,  323,   44,  317,  540,  486,  868,  708,  349,  500, 1304,  277,\n",
       "           370, 1582,  383, 1027, 2526,  398,  284,  277,  370,  439,  501,  548,\n",
       "           265,  735,  258,  390,  833,  304,   10,  934,  397,  426,  354,   44,\n",
       "           261,  277,  370,  439,  501, 2025,  673,  266,  673,  277,  370, 1582,\n",
       "            46, 1407,  261,  823,  313,  261, 1492,  552,  265,  711,  261,  277,\n",
       "           370, 1582,  266,  325,  776,  261,  277,  370,  439,  501,   46,  284,\n",
       "           277,  370,  439,  501,  282,  377,  708,  309,  365,  664,  413,  972,\n",
       "            46,  284,  277,  370,  439,  501,  691,  383, 1011, 1314,  469,  322,\n",
       "           258,  561, 1253,   46,  710,  360,  431,  636,  992,  933,  886,   46,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'cu_seqlens': [tensor([  0, 169, 250], dtype=torch.int32),\n",
       "  tensor([  0, 189, 250], dtype=torch.int32),\n",
       "  tensor([  0, 213, 250], dtype=torch.int32),\n",
       "  tensor([  0, 217, 250], dtype=torch.int32)],\n",
       " 'max_seqlen': [169, 189, 213, 217]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529c59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(trn_subset, batch_size=bs, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89e925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb = next(iter(dl))\n",
    "len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb5b910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   479,\n",
       "   402,\n",
       "   406,\n",
       "   507,\n",
       "   258,\n",
       "   775,\n",
       "   302,\n",
       "   313,\n",
       "   338,\n",
       "   720,\n",
       "   46,\n",
       "   342,\n",
       "   677,\n",
       "   309,\n",
       "   282,\n",
       "   2876,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   309,\n",
       "   708,\n",
       "   309,\n",
       "   282,\n",
       "   2073,\n",
       "   46,\n",
       "   406,\n",
       "   407,\n",
       "   265,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   328,\n",
       "   338,\n",
       "   386,\n",
       "   44,\n",
       "   391,\n",
       "   392,\n",
       "   468,\n",
       "   459,\n",
       "   119,\n",
       "   258,\n",
       "   1674,\n",
       "   354,\n",
       "   338,\n",
       "   2377,\n",
       "   304,\n",
       "   10,\n",
       "   670,\n",
       "   426,\n",
       "   265,\n",
       "   338,\n",
       "   386,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   844,\n",
       "   44,\n",
       "   337,\n",
       "   507,\n",
       "   733,\n",
       "   775,\n",
       "   302,\n",
       "   46,\n",
       "   1127,\n",
       "   349,\n",
       "   850,\n",
       "   309,\n",
       "   328,\n",
       "   524,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   627,\n",
       "   2377,\n",
       "   476,\n",
       "   937,\n",
       "   386,\n",
       "   565,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   732,\n",
       "   44,\n",
       "   406,\n",
       "   44,\n",
       "   363,\n",
       "   469,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   629,\n",
       "   2377,\n",
       "   505,\n",
       "   10,\n",
       "   2826,\n",
       "   44,\n",
       "   360,\n",
       "   1208,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   263,\n",
       "   261,\n",
       "   1674,\n",
       "   354,\n",
       "   406,\n",
       "   384,\n",
       "   2377,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   364,\n",
       "   2876,\n",
       "   387,\n",
       "   493,\n",
       "   708,\n",
       "   360,\n",
       "   405,\n",
       "   1714,\n",
       "   266,\n",
       "   1398,\n",
       "   766,\n",
       "   558,\n",
       "   46,\n",
       "   1559,\n",
       "   360,\n",
       "   1699,\n",
       "   44,\n",
       "   406,\n",
       "   943,\n",
       "   338,\n",
       "   386,\n",
       "   387,\n",
       "   1714,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   297,\n",
       "   338,\n",
       "   2377,\n",
       "   46,\n",
       "   312,\n",
       "   722,\n",
       "   536,\n",
       "   377,\n",
       "   708,\n",
       "   360,\n",
       "   365,\n",
       "   1208,\n",
       "   266,\n",
       "   1228,\n",
       "   458,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   528,\n",
       "   402,\n",
       "   2456,\n",
       "   626,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   508,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   313,\n",
       "   261,\n",
       "   631,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   258,\n",
       "   2489,\n",
       "   528,\n",
       "   708,\n",
       "   285,\n",
       "   704,\n",
       "   365,\n",
       "   561,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1213,\n",
       "   462,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   586,\n",
       "   2456,\n",
       "   626,\n",
       "   377,\n",
       "   266,\n",
       "   973,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1402,\n",
       "   817,\n",
       "   313,\n",
       "   261,\n",
       "   527,\n",
       "   634,\n",
       "   285,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   501,\n",
       "   365,\n",
       "   664,\n",
       "   1333,\n",
       "   383,\n",
       "   405,\n",
       "   1455,\n",
       "   297,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   532,\n",
       "   756,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   493,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   2241,\n",
       "   776,\n",
       "   261,\n",
       "   501,\n",
       "   266,\n",
       "   1233,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   354,\n",
       "   475,\n",
       "   46,\n",
       "   316,\n",
       "   703,\n",
       "   266,\n",
       "   322,\n",
       "   626,\n",
       "   263,\n",
       "   340,\n",
       "   2771,\n",
       "   304,\n",
       "   10,\n",
       "   2120,\n",
       "   626,\n",
       "   477,\n",
       "   328,\n",
       "   261,\n",
       "   1455,\n",
       "   297,\n",
       "   1333,\n",
       "   431,\n",
       "   327,\n",
       "   46,\n",
       "   931,\n",
       "   309,\n",
       "   282,\n",
       "   397,\n",
       "   265,\n",
       "   483,\n",
       "   584,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   677,\n",
       "   285,\n",
       "   1341,\n",
       "   673,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   316,\n",
       "   426,\n",
       "   265,\n",
       "   261,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   962,\n",
       "   266,\n",
       "   660,\n",
       "   673,\n",
       "   2489,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1139,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1367,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   601,\n",
       "   261,\n",
       "   988,\n",
       "   327,\n",
       "   46,\n",
       "   710,\n",
       "   2456,\n",
       "   626,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]},\n",
       " {'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   779,\n",
       "   402,\n",
       "   1221,\n",
       "   282,\n",
       "   2439,\n",
       "   810,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   1798,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   322,\n",
       "   413,\n",
       "   46,\n",
       "   317,\n",
       "   1090,\n",
       "   44,\n",
       "   337,\n",
       "   743,\n",
       "   1221,\n",
       "   46,\n",
       "   1174,\n",
       "   349,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   476,\n",
       "   543,\n",
       "   261,\n",
       "   390,\n",
       "   779,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   506,\n",
       "   450,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   947,\n",
       "   44,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   46,\n",
       "   337,\n",
       "   743,\n",
       "   1192,\n",
       "   266,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   2916,\n",
       "   505,\n",
       "   10,\n",
       "   1804,\n",
       "   536,\n",
       "   496,\n",
       "   409,\n",
       "   407,\n",
       "   265,\n",
       "   432,\n",
       "   261,\n",
       "   1798,\n",
       "   735,\n",
       "   833,\n",
       "   46,\n",
       "   316,\n",
       "   1588,\n",
       "   583,\n",
       "   266,\n",
       "   578,\n",
       "   371,\n",
       "   258,\n",
       "   1350,\n",
       "   46,\n",
       "   316,\n",
       "   1533,\n",
       "   383,\n",
       "   261,\n",
       "   631,\n",
       "   468,\n",
       "   533,\n",
       "   630,\n",
       "   1061,\n",
       "   46,\n",
       "   707,\n",
       "   44,\n",
       "   1221,\n",
       "   1588,\n",
       "   265,\n",
       "   261,\n",
       "   1306,\n",
       "   371,\n",
       "   261,\n",
       "   686,\n",
       "   266,\n",
       "   1041,\n",
       "   265,\n",
       "   261,\n",
       "   631,\n",
       "   44,\n",
       "   317,\n",
       "   1936,\n",
       "   44,\n",
       "   631,\n",
       "   44,\n",
       "   432,\n",
       "   627,\n",
       "   545,\n",
       "   358,\n",
       "   735,\n",
       "   2916,\n",
       "   266,\n",
       "   364,\n",
       "   1764,\n",
       "   1782,\n",
       "   687,\n",
       "   10,\n",
       "   412,\n",
       "   631,\n",
       "   837,\n",
       "   1221,\n",
       "   384,\n",
       "   946,\n",
       "   266,\n",
       "   389,\n",
       "   512,\n",
       "   832,\n",
       "   1061,\n",
       "   1366,\n",
       "   354,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   833,\n",
       "   266,\n",
       "   364,\n",
       "   391,\n",
       "   1192,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   848,\n",
       "   349,\n",
       "   44,\n",
       "   390,\n",
       "   779,\n",
       "   44,\n",
       "   387,\n",
       "   1232,\n",
       "   524,\n",
       "   735,\n",
       "   2916,\n",
       "   46,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   519,\n",
       "   337,\n",
       "   621,\n",
       "   1764,\n",
       "   1782,\n",
       "   972,\n",
       "   46,\n",
       "   1046,\n",
       "   384,\n",
       "   325,\n",
       "   458,\n",
       "   414,\n",
       "   710,\n",
       "   391,\n",
       "   44,\n",
       "   1221,\n",
       "   266,\n",
       "   261,\n",
       "   1798,\n",
       "   477,\n",
       "   266,\n",
       "   692,\n",
       "   561,\n",
       "   413,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   313,\n",
       "   258,\n",
       "   1492,\n",
       "   1328,\n",
       "   371,\n",
       "   1286,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   380,\n",
       "   496,\n",
       "   708,\n",
       "   309,\n",
       "   449,\n",
       "   364,\n",
       "   500,\n",
       "   671,\n",
       "   413,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   558,\n",
       "   1286,\n",
       "   405,\n",
       "   346,\n",
       "   266,\n",
       "   973,\n",
       "   44,\n",
       "   409,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   563,\n",
       "   266,\n",
       "   2404,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   842,\n",
       "   118,\n",
       "   1245,\n",
       "   371,\n",
       "   261,\n",
       "   346,\n",
       "   1286,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   536,\n",
       "   258,\n",
       "   2578,\n",
       "   302,\n",
       "   313,\n",
       "   832,\n",
       "   2885,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   2922,\n",
       "   861,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   741,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   364,\n",
       "   265,\n",
       "   322,\n",
       "   496,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   540,\n",
       "   486,\n",
       "   868,\n",
       "   708,\n",
       "   349,\n",
       "   500,\n",
       "   1304,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   383,\n",
       "   1027,\n",
       "   2526,\n",
       "   398,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   258,\n",
       "   390,\n",
       "   833,\n",
       "   304,\n",
       "   10,\n",
       "   934,\n",
       "   397,\n",
       "   426,\n",
       "   354,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   2025,\n",
       "   673,\n",
       "   266,\n",
       "   673,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   823,\n",
       "   313,\n",
       "   261,\n",
       "   1492,\n",
       "   552,\n",
       "   265,\n",
       "   711,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   266,\n",
       "   325,\n",
       "   776,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   377,\n",
       "   708,\n",
       "   309,\n",
       "   365,\n",
       "   664,\n",
       "   413,\n",
       "   972,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   691,\n",
       "   383,\n",
       "   1011,\n",
       "   1314,\n",
       "   469,\n",
       "   322,\n",
       "   258,\n",
       "   561,\n",
       "   1253,\n",
       "   46,\n",
       "   710,\n",
       "   360,\n",
       "   431,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67208598",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"src_iterable\": dl,\n",
    "    \"src_batch_size\": bs,\n",
    "    \"src_max_seq_len\": 1,\n",
    "    \"out_batch_size\": bs,\n",
    "    \"out_pseq_len\": out_pseq_len,\n",
    "    \"buffer_size\": 5,\n",
    "    \"pad_token_id\": 0,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_dl = CausalSequencePacker(**d)\n",
    "# sq_batch = next(iter(sq_dl))\n",
    "# sq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fddca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CausalSequencePacker at 0x7ff787ac73d0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaders(sq_dl, sq_dl)\n",
    "dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3e632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1bf0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 516,  327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,\n",
       "           313,  338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,\n",
       "           309,  708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,\n",
       "           302,  328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,\n",
       "           354,  338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,\n",
       "            44,  317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,\n",
       "           850,  309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,\n",
       "           565,  266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,\n",
       "           261,  775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360,\n",
       "          1208,  261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,\n",
       "           384, 2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405,\n",
       "          1714,  266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,\n",
       "           338,  386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,\n",
       "            46,  312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  401,  282,  258,  390,  528,  402, 2456,\n",
       "           626,   46, 2456,  626,  508,  265,  483,  737,  266,  325,  313,  261,\n",
       "           631,   46, 2456,  626,  282,  258, 2489,  528,  708,  285,  704,  365,\n",
       "           561,  268,  117,  417,   46, 1213,  462,  268,  117,  417,  586, 2456,\n",
       "           626,  377,  266,  973,  304,   10,  516,  327,   44, 2456,  626,  282,\n",
       "          1402,  817,  313,  261,  527,  634,  285,  382,  258,  346,  501,   46,\n",
       "           284,  501,  365,  664, 1333,  383,  405, 1455,  297,   46, 2456,  626,\n",
       "           532,  756,  261, 1333, 1455,  266,  407,  265,  325,  328,  493,   46,\n",
       "          2456,  626, 2241,  776,  261,  501,  266, 1233,  261, 1333, 1455,  354,\n",
       "           475,   46,  316,  703,  266,  322,  626,  263,  340, 2771,  304,   10,\n",
       "          2120,  626,  477,  328,  261, 1455,  297, 1333,  431,  327,   46,  931,\n",
       "           309,  282,  397,  265,  483,  584,   44, 2456,  626,  677,  285, 1341,\n",
       "           673,  268,  117,  417,   46,  316,  426,  265,  261,  268,  117,  417,\n",
       "           962,  266,  660,  673, 2489,  268,  117,  417,   46, 1139,   44, 2456,\n",
       "           626,  282, 1367,  265,  483,  737,  266,  325,  601,  261,  988,  327,\n",
       "            46,  710, 2456,  626,  636,  992,  933,  886,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 516,  327,   44,  258,  390,  779,  402, 1221,  282, 2439,  810,  261,\n",
       "           389,  440,   46,  316,  382,  258,  346, 1798,  266,  407,  265,  322,\n",
       "           413,   46,  317, 1090,   44,  337,  743, 1221,   46, 1174,  349,  367,\n",
       "           265,  325,  476,  543,  261,  390,  779,   46,  284, 1798,  506,  450,\n",
       "          1221,  266,  323,   44,  317,  947,   44,  337,  862,  492,  367,  265,\n",
       "           325,   46,  337,  743, 1192,  266,  337,  862,  492,  735, 2916,  505,\n",
       "            10, 1804,  536,  496,  409,  407,  265,  432,  261, 1798,  735,  833,\n",
       "            46,  316, 1588,  583,  266,  578,  371,  258, 1350,   46,  316, 1533,\n",
       "           383,  261,  631,  468,  533,  630, 1061,   46,  707,   44, 1221, 1588,\n",
       "           265,  261, 1306,  371,  261,  686,  266, 1041,  265,  261,  631,   44,\n",
       "           317, 1936,   44,  631,   44,  432,  627,  545,  358,  735, 2916,  266,\n",
       "           364, 1764, 1782,  687,   10,  412,  631,  837, 1221,  384,  946,  266,\n",
       "           389,  512,  832, 1061, 1366,  354,  261,  389,  440,   46,  284, 1798,\n",
       "           548,  265,  735,  833,  266,  364,  391, 1192,   46,  316,  382, 1221,\n",
       "           266,  323,   44,  317,  848,  349,   44,  390,  779,   44,  387, 1232,\n",
       "           524,  735, 2916,   46,  337,  862,  492,  735,  519,  337,  621, 1764,\n",
       "          1782,  972,   46, 1046,  384,  325,  458,  414,  710,  391,   44, 1221,\n",
       "           266,  261, 1798,  477,  266,  692,  561,  413,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  313,  258, 1492, 1328,  371, 1286,   44,\n",
       "           401,  282,  258,  390,  277,  370,  439,  501,   46,  284,  277,  370,\n",
       "           439,  501,  282,  380,  496,  708,  309,  449,  364,  500,  671,  413,\n",
       "            46, 1407,  261,  558, 1286,  405,  346,  266,  973,   44,  409,  261,\n",
       "           277,  370,  439,  501,  282,  563,  266, 2404,   46,  284,  277,  370,\n",
       "           439,  501,  282,  842,  118, 1245,  371,  261,  346, 1286,  304,   10,\n",
       "           516,  327,   44,  261,  277,  370,  439,  501,  536,  258, 2578,  302,\n",
       "           313,  832, 2885,   46,  421,  282,  258,  390, 2922,  861,   46,  284,\n",
       "           861,  741,  261,  277,  370,  439,  501,  364,  265,  322,  496,   46,\n",
       "           284,  861,  323,   44,  317,  540,  486,  868,  708,  349,  500, 1304,\n",
       "           277,  370, 1582,  383, 1027, 2526,  398,  284,  277,  370,  439,  501,\n",
       "           548,  265,  735,  258,  390,  833,  304,   10,  934,  397,  426,  354,\n",
       "            44,  261,  277,  370,  439,  501, 2025,  673,  266,  673,  277,  370,\n",
       "          1582,   46, 1407,  261,  823,  313,  261, 1492,  552,  265,  711,  261,\n",
       "           277,  370, 1582,  266,  325,  776,  261,  277,  370,  439,  501,   46,\n",
       "           284,  277,  370,  439,  501,  282,  377,  708,  309,  365,  664,  413,\n",
       "           972,   46,  284,  277,  370,  439,  501,  691,  383, 1011, 1314,  469,\n",
       "           322,  258,  561, 1253,   46,  710,  360,  431,  636,  992,  933,  886,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'labels': tensor([[ 327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,  313,\n",
       "           338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,  309,\n",
       "           708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,  302,\n",
       "           328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,  354,\n",
       "           338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,   44,\n",
       "           317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,  850,\n",
       "           309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,  565,\n",
       "           266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,  261,\n",
       "           775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360, 1208,\n",
       "           261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,  384,\n",
       "          2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405, 1714,\n",
       "           266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,  338,\n",
       "           386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,   46,\n",
       "           312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,   46,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 438,  258,  397,   44,  401,  282,  258,  390,  528,  402, 2456,  626,\n",
       "            46, 2456,  626,  508,  265,  483,  737,  266,  325,  313,  261,  631,\n",
       "            46, 2456,  626,  282,  258, 2489,  528,  708,  285,  704,  365,  561,\n",
       "           268,  117,  417,   46, 1213,  462,  268,  117,  417,  586, 2456,  626,\n",
       "           377,  266,  973,  304,   10,  516,  327,   44, 2456,  626,  282, 1402,\n",
       "           817,  313,  261,  527,  634,  285,  382,  258,  346,  501,   46,  284,\n",
       "           501,  365,  664, 1333,  383,  405, 1455,  297,   46, 2456,  626,  532,\n",
       "           756,  261, 1333, 1455,  266,  407,  265,  325,  328,  493,   46, 2456,\n",
       "           626, 2241,  776,  261,  501,  266, 1233,  261, 1333, 1455,  354,  475,\n",
       "            46,  316,  703,  266,  322,  626,  263,  340, 2771,  304,   10, 2120,\n",
       "           626,  477,  328,  261, 1455,  297, 1333,  431,  327,   46,  931,  309,\n",
       "           282,  397,  265,  483,  584,   44, 2456,  626,  677,  285, 1341,  673,\n",
       "           268,  117,  417,   46,  316,  426,  265,  261,  268,  117,  417,  962,\n",
       "           266,  660,  673, 2489,  268,  117,  417,   46, 1139,   44, 2456,  626,\n",
       "           282, 1367,  265,  483,  737,  266,  325,  601,  261,  988,  327,   46,\n",
       "           710, 2456,  626,  636,  992,  933,  886,   46,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 327,   44,  258,  390,  779,  402, 1221,  282, 2439,  810,  261,  389,\n",
       "           440,   46,  316,  382,  258,  346, 1798,  266,  407,  265,  322,  413,\n",
       "            46,  317, 1090,   44,  337,  743, 1221,   46, 1174,  349,  367,  265,\n",
       "           325,  476,  543,  261,  390,  779,   46,  284, 1798,  506,  450, 1221,\n",
       "           266,  323,   44,  317,  947,   44,  337,  862,  492,  367,  265,  325,\n",
       "            46,  337,  743, 1192,  266,  337,  862,  492,  735, 2916,  505,   10,\n",
       "          1804,  536,  496,  409,  407,  265,  432,  261, 1798,  735,  833,   46,\n",
       "           316, 1588,  583,  266,  578,  371,  258, 1350,   46,  316, 1533,  383,\n",
       "           261,  631,  468,  533,  630, 1061,   46,  707,   44, 1221, 1588,  265,\n",
       "           261, 1306,  371,  261,  686,  266, 1041,  265,  261,  631,   44,  317,\n",
       "          1936,   44,  631,   44,  432,  627,  545,  358,  735, 2916,  266,  364,\n",
       "          1764, 1782,  687,   10,  412,  631,  837, 1221,  384,  946,  266,  389,\n",
       "           512,  832, 1061, 1366,  354,  261,  389,  440,   46,  284, 1798,  548,\n",
       "           265,  735,  833,  266,  364,  391, 1192,   46,  316,  382, 1221,  266,\n",
       "           323,   44,  317,  848,  349,   44,  390,  779,   44,  387, 1232,  524,\n",
       "           735, 2916,   46,  337,  862,  492,  735,  519,  337,  621, 1764, 1782,\n",
       "           972,   46, 1046,  384,  325,  458,  414,  710,  391,   44, 1221,  266,\n",
       "           261, 1798,  477,  266,  692,  561,  413,   46,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 438,  258,  397,   44,  313,  258, 1492, 1328,  371, 1286,   44,  401,\n",
       "           282,  258,  390,  277,  370,  439,  501,   46,  284,  277,  370,  439,\n",
       "           501,  282,  380,  496,  708,  309,  449,  364,  500,  671,  413,   46,\n",
       "          1407,  261,  558, 1286,  405,  346,  266,  973,   44,  409,  261,  277,\n",
       "           370,  439,  501,  282,  563,  266, 2404,   46,  284,  277,  370,  439,\n",
       "           501,  282,  842,  118, 1245,  371,  261,  346, 1286,  304,   10,  516,\n",
       "           327,   44,  261,  277,  370,  439,  501,  536,  258, 2578,  302,  313,\n",
       "           832, 2885,   46,  421,  282,  258,  390, 2922,  861,   46,  284,  861,\n",
       "           741,  261,  277,  370,  439,  501,  364,  265,  322,  496,   46,  284,\n",
       "           861,  323,   44,  317,  540,  486,  868,  708,  349,  500, 1304,  277,\n",
       "           370, 1582,  383, 1027, 2526,  398,  284,  277,  370,  439,  501,  548,\n",
       "           265,  735,  258,  390,  833,  304,   10,  934,  397,  426,  354,   44,\n",
       "           261,  277,  370,  439,  501, 2025,  673,  266,  673,  277,  370, 1582,\n",
       "            46, 1407,  261,  823,  313,  261, 1492,  552,  265,  711,  261,  277,\n",
       "           370, 1582,  266,  325,  776,  261,  277,  370,  439,  501,   46,  284,\n",
       "           277,  370,  439,  501,  282,  377,  708,  309,  365,  664,  413,  972,\n",
       "            46,  284,  277,  370,  439,  501,  691,  383, 1011, 1314,  469,  322,\n",
       "           258,  561, 1253,   46,  710,  360,  431,  636,  992,  933,  886,   46,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " 'cu_seqlens': [tensor([  0, 169, 250], dtype=torch.int32),\n",
       "  tensor([  0, 189, 250], dtype=torch.int32),\n",
       "  tensor([  0, 213, 250], dtype=torch.int32),\n",
       "  tensor([  0, 217, 250], dtype=torch.int32)],\n",
       " 'max_seqlen': [169, 189, 213, 217]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dls.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 250]) torch.Size([4, 250])\n",
      "1 torch.Size([4, 250]) torch.Size([4, 250])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dls.train):\n",
    "    print(i, batch['input_ids'].shape, batch['labels'].shape)\n",
    "    if i == 1: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### FlashCausalAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    Flash Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with Flash Attention implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - Check if Flash Attention is available (FLASH_ATTN_AVAILABLE)\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [total_seq_len, hidden_dim].\n",
    "            cu_seqlens: Cumulative sequence lengths tensor of shape [batch_size + 1]\n",
    "                    Used instead of an attention mask for both masking and\n",
    "                    variable-length sequences. Example:\n",
    "                        cu_seqlens = torch.tensor([0, 10, 30, 60])\n",
    "                    This means there are three sequences in the batch:\n",
    "                        - First sequence has 10 tokens\n",
    "                        - Second sequence has 20 tokens\n",
    "                        - Third sequence has 30 tokens\n",
    "            max_seqlen: Maximum sequence length in the batch. In the example above,\n",
    "                        the maximum sequence length is 30.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [total_seq_len, hidden_dim] after attention.\n",
    "        \"\"\"    \n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            raise ImportError(\"Flash Attention is not available. Please install it with `pip install flash-attn`\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x) # [batch_size, seq_len, d_out]\n",
    "\n",
    "        k_reshaped = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        q_reshaped = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v_reshaped = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q_reshaped,\n",
    "            k_reshaped,\n",
    "            v_reshaped,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c9878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout = 0.0): super().__init__()\n",
    "    \n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLASH_ATTN_AVAILABLE = False\n",
    "# try:\n",
    "#     from flash_attn import flash_attn_varlen_func\n",
    "\n",
    "#     FLASH_ATTN_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     # Flash Attention is not available\n",
    "#     pass\n",
    "\n",
    "# with torch.no_grad(), torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "#     mha = FlashCausalAttentionBlock(hidden_dim=170, num_heads=1).to(def_device)\n",
    "#     output = mha(out_batches['input_ids'].to(torch.float32).to(def_device), out_batches['cu_seqlens'][0].to(def_device), out_batches['max_seqlen'][0])\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ace140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlashCausalAttentionBlock()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = FlashCausalAttentionBlock(hidden_dim=out_pseq_len, num_heads=2).to(def_device)\n",
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04bf39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 516,  327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,\n",
       "           313,  338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,\n",
       "           309,  708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,\n",
       "           302,  328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,\n",
       "           354,  338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,\n",
       "            44,  317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,\n",
       "           850,  309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,\n",
       "           565,  266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,\n",
       "           261,  775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360,\n",
       "          1208,  261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,\n",
       "           384, 2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405,\n",
       "          1714,  266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,\n",
       "           338,  386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,\n",
       "            46,  312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  401,  282,  258,  390,  528,  402, 2456,\n",
       "           626,   46, 2456,  626,  508,  265,  483,  737,  266,  325,  313,  261,\n",
       "           631,   46, 2456,  626,  282,  258, 2489,  528,  708,  285,  704,  365,\n",
       "           561,  268,  117,  417,   46, 1213,  462,  268,  117,  417,  586, 2456,\n",
       "           626,  377,  266,  973,  304,   10,  516,  327,   44, 2456,  626,  282,\n",
       "          1402,  817,  313,  261,  527,  634,  285,  382,  258,  346,  501,   46,\n",
       "           284,  501,  365,  664, 1333,  383,  405, 1455,  297,   46, 2456,  626,\n",
       "           532,  756,  261, 1333, 1455,  266,  407,  265,  325,  328,  493,   46,\n",
       "          2456,  626, 2241,  776,  261,  501,  266, 1233,  261, 1333, 1455,  354,\n",
       "           475,   46,  316,  703,  266,  322,  626,  263,  340, 2771,  304,   10,\n",
       "          2120,  626,  477,  328,  261, 1455,  297, 1333,  431,  327,   46,  931,\n",
       "           309,  282,  397,  265,  483,  584,   44, 2456,  626,  677,  285, 1341,\n",
       "           673,  268,  117,  417,   46,  316,  426,  265,  261,  268,  117,  417,\n",
       "           962,  266,  660,  673, 2489,  268,  117,  417,   46, 1139,   44, 2456,\n",
       "           626,  282, 1367,  265,  483,  737,  266,  325,  601,  261,  988,  327,\n",
       "            46,  710, 2456,  626,  636,  992,  933,  886,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 516,  327,   44,  258,  390,  779,  402, 1221,  282, 2439,  810,  261,\n",
       "           389,  440,   46,  316,  382,  258,  346, 1798,  266,  407,  265,  322,\n",
       "           413,   46,  317, 1090,   44,  337,  743, 1221,   46, 1174,  349,  367,\n",
       "           265,  325,  476,  543,  261,  390,  779,   46,  284, 1798,  506,  450,\n",
       "          1221,  266,  323,   44,  317,  947,   44,  337,  862,  492,  367,  265,\n",
       "           325,   46,  337,  743, 1192,  266,  337,  862,  492,  735, 2916,  505,\n",
       "            10, 1804,  536,  496,  409,  407,  265,  432,  261, 1798,  735,  833,\n",
       "            46,  316, 1588,  583,  266,  578,  371,  258, 1350,   46,  316, 1533,\n",
       "           383,  261,  631,  468,  533,  630, 1061,   46,  707,   44, 1221, 1588,\n",
       "           265,  261, 1306,  371,  261,  686,  266, 1041,  265,  261,  631,   44,\n",
       "           317, 1936,   44,  631,   44,  432,  627,  545,  358,  735, 2916,  266,\n",
       "           364, 1764, 1782,  687,   10,  412,  631,  837, 1221,  384,  946,  266,\n",
       "           389,  512,  832, 1061, 1366,  354,  261,  389,  440,   46,  284, 1798,\n",
       "           548,  265,  735,  833,  266,  364,  391, 1192,   46,  316,  382, 1221,\n",
       "           266,  323,   44,  317,  848,  349,   44,  390,  779,   44,  387, 1232,\n",
       "           524,  735, 2916,   46,  337,  862,  492,  735,  519,  337,  621, 1764,\n",
       "          1782,  972,   46, 1046,  384,  325,  458,  414,  710,  391,   44, 1221,\n",
       "           266,  261, 1798,  477,  266,  692,  561,  413,   46,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
       "         [ 763,  438,  258,  397,   44,  313,  258, 1492, 1328,  371, 1286,   44,\n",
       "           401,  282,  258,  390,  277,  370,  439,  501,   46,  284,  277,  370,\n",
       "           439,  501,  282,  380,  496,  708,  309,  449,  364,  500,  671,  413,\n",
       "            46, 1407,  261,  558, 1286,  405,  346,  266,  973,   44,  409,  261,\n",
       "           277,  370,  439,  501,  282,  563,  266, 2404,   46,  284,  277,  370,\n",
       "           439,  501,  282,  842,  118, 1245,  371,  261,  346, 1286,  304,   10,\n",
       "           516,  327,   44,  261,  277,  370,  439,  501,  536,  258, 2578,  302,\n",
       "           313,  832, 2885,   46,  421,  282,  258,  390, 2922,  861,   46,  284,\n",
       "           861,  741,  261,  277,  370,  439,  501,  364,  265,  322,  496,   46,\n",
       "           284,  861,  323,   44,  317,  540,  486,  868,  708,  349,  500, 1304,\n",
       "           277,  370, 1582,  383, 1027, 2526,  398,  284,  277,  370,  439,  501,\n",
       "           548,  265,  735,  258,  390,  833,  304,   10,  934,  397,  426,  354,\n",
       "            44,  261,  277,  370,  439,  501, 2025,  673,  266,  673,  277,  370,\n",
       "          1582,   46, 1407,  261,  823,  313,  261, 1492,  552,  265,  711,  261,\n",
       "           277,  370, 1582,  266,  325,  776,  261,  277,  370,  439,  501,   46,\n",
       "           284,  277,  370,  439,  501,  282,  377,  708,  309,  365,  664,  413,\n",
       "           972,   46,  284,  277,  370,  439,  501,  691,  383, 1011, 1314,  469,\n",
       "           322,  258,  561, 1253,   46,  710,  360,  431,  636,  992,  933,  886,\n",
       "            46,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0]]),\n",
       " torch.Size([4, 250]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches['input_ids'], out_batches['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2900ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_batch = out_batches['input_ids'].view((out_pseq_len*bs))\n",
    "sp_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21137432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  0, 169, 250], dtype=torch.int32),\n",
       " tensor([  0, 189, 250], dtype=torch.int32),\n",
       " tensor([  0, 213, 250], dtype=torch.int32),\n",
       " tensor([  0, 217, 250], dtype=torch.int32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens = out_batches['cu_seqlens']\n",
    "cu_seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941e2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  0, 169, 250], dtype=torch.int32),\n",
       " tensor([  0, 189, 250], dtype=torch.int32),\n",
       " tensor([  0, 213, 250], dtype=torch.int32)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8fe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([250, 250, 250], dtype=torch.int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = torch.tensor([t[-1] for t in cu_seqlens[:-1]], dtype=cu_seqlens[0].dtype)\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 250, 500, 750])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets = torch.cat([torch.tensor([0], dtype=lengths.dtype), torch.cumsum(lengths, dim=0)])\n",
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8223098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0, 169,   0, 189,   0, 213,   0, 217, 250], dtype=torch.int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([t[:-1] if i < len(cu_seqlens)-1 else t for i, t in enumerate(cu_seqlens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fc44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0, 250, 250, 500, 500, 750, 750, 750])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets.repeat_interleave(torch.tensor([t.numel()-1 if i < len(cu_seqlens)-1 else t.numel() for i, t in enumerate(cu_seqlens)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ed7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  169,  250,  439,  500,  713,  750,  967, 1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([t[:-1] if i < len(cu_seqlens)-1 else t for i, t in enumerate(cu_seqlens)]) + offsets.repeat_interleave(torch.tensor([t.numel()-1 if i < len(cu_seqlens)-1 else t.numel() for i, t in enumerate(cu_seqlens)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c6d722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  169,  250,  439,  500,  713,  750,  967, 1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_cu_seqlens(cu_seqlens):\n",
    "    \"Combine multiple tensors into cumulative sequence\"\n",
    "    lengths = torch.tensor([t[-1] for t in cu_seqlens[:-1]], dtype=cu_seqlens[0].dtype)\n",
    "    offsets = torch.cat([torch.tensor([0], dtype=lengths.dtype), torch.cumsum(lengths, dim=0)])\n",
    "    return torch.cat([t[:-1] if i < len(cu_seqlens)-1 else t for i, t in enumerate(cu_seqlens)]) + offsets.repeat_interleave(torch.tensor([t.numel()-1 if i < len(cu_seqlens)-1 else t.numel() for i, t in enumerate(cu_seqlens)]))\n",
    "combine_cu_seqlens(out_batches['cu_seqlens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5deb812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  169,  250,  439,  500,  713,  750,  967, 1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cu_seqlens = combine_cu_seqlens(out_batches['cu_seqlens'])\n",
    "cat_cu_seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5b6662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mha(sp_batch.to(torch.bfloat16).to(def_device), cat_cu_seqlens.to(def_device), max(out_batches['max_seqlen']))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim, bias=False)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "    The Gated Linear Unit has two parallel linear transforms: one for the gate and one for the value.\n",
    "    Apply the activation only to the gate, then multiply elementwise with the value, followed by a\n",
    "    final linear projection and optional dropout.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        intermediate_dim: int,\n",
    "        act: nn.Module = nn.GELU,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a GLU.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            intermediate_dim: Dimension of each intermediate branch\n",
    "                              Often set to 2/3 * 4 * hidden_dim to maintain similar parameter\n",
    "                              count to a standard MLP with 4x expansion\n",
    "            activation: Activation function to use, defaults to GELU\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.Wg = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.act = act\n",
    "        self.Wo = nn.Linear(intermediate_dim, hidden_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        gate = self.act(self.Wg(x))\n",
    "        val = self.Wv(x)\n",
    "        out = self.Wo(gate * val)\n",
    "        return self.do(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "        self.ln4 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = FlashCausalAttentionBlock(hidden_dim=emb_dim, num_heads=n_head, dropout=drop_out)\n",
    "        self.ff = GLU(emb_dim, int(emb_dim*ff_mult), act=act)\n",
    "        self.emb_dim = emb_dim\n",
    "    \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        x_shape = x.shape\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        # reshape for flash attention\n",
    "        x = x.view(-1, self.emb_dim)\n",
    "        x = self.mha(x, cu_seqlens, max_seqlen)  # Need to pass (x: Tensor, cu_seqlens: Tensor, max_seqlen: int) \n",
    "        x = x.view(*x_shape)  # x.shape:(total_seq_len, hidden_dim)\n",
    "        x = self.ln2(x) \n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln3(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.ln4(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2b6f3",
   "metadata": {},
   "source": [
    "**TODO: create `pos_emb` using `cu_seqlens`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.ModuleList( # Use module.list instead of sequential\n",
    "            [TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias'], cfg['act']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "#         self.emb_dim = cfg['emb_dim']\n",
    "    \n",
    "    def forward(self, x, cu_seqlens, max_seqlen):\n",
    "        cu_seqlens = combine_cu_seqlens(cu_seqlens)\n",
    "        max_seqlen = max(max_seqlen)\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        for tb in self.tb:\n",
    "            x = tb(x, cu_seqlens, max_seqlen) \n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:].to(def_device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ceb49",
   "metadata": {},
   "source": [
    "**TODO: Add logging on weights and bias.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, dtype=torch.bfloat16):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def before_fit(self, learn): self.scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=self.dtype)\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "        \n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6ef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 12,    # num transformer blocks\n",
    "    'vocab_sz': 3008,\n",
    "    'emb_dim': 384,\n",
    "    'ctx_len': ctx_len,\n",
    "    'n_head': 12,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 2/3 * 4,\n",
    "    'qkv_bias': False,\n",
    "    'act': nn.GELU(),   # activation function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 3008,\n",
    "    'emb_dim': 8,\n",
    "    'ctx_len': 1,\n",
    "    'n_head': 2,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 2/3 * 4,\n",
    "    'qkv_bias': False,\n",
    "    'act': nn.GELU(),   # activation function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        super().__init__(*ms, **metrics)\n",
    "    \n",
    "    def before_batch(self, learn):\n",
    "        x, y, *_ = learn.batch\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        for m in self.metrics.values(): m.update(learn.preds.flatten(0, 1), learn.lbl)\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(learn.inps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "\n",
    "def to_device(x, device=def_device):\n",
    "    if isinstance(x, torch.Tensor): return x.to(device)\n",
    "    # v can also be a list isntead of a tensor.\n",
    "    if isinstance(x, Mapping): return {k:to_device(v, device) for k,v in x.items()}\n",
    "    if isinstance(x, (list, tuple)): return type(x)(to_device(o, device) for o in x)\n",
    "    return x\n",
    "\n",
    "class DeviceCB(Callback):\n",
    "    def __init__(self, device=def_device): store_attr()\n",
    "    def before_fit(self, learn):\n",
    "        if hasattr(learn.model, 'to'): learn.model.to(self.device)\n",
    "    def before_batch(self, learn): learn.batch = to_device(learn.batch, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eada510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _get_inp(b, n_inp, inp_nm):\n",
    "    if inp_nm is not None: \n",
    "        if isinstance(inp_nm, (list, tuple)):\n",
    "            return [b[k] for k in inp_nm]\n",
    "        return [b[inp_nm]]\n",
    "    return b[:n_inp]\n",
    "\n",
    "def _get_lbl(b, n_inp, lbl_nm):\n",
    "    if lbl_nm is not None: \n",
    "        if isinstance(lbl_nm, (list, tuple)):\n",
    "            return [b[k] for k in lbl_nm]\n",
    "        return [b[lbl_nm]]\n",
    "    return b[:n_inp]\n",
    "\n",
    "def _get_preds(b, preds_nm):\n",
    "    return b if preds_nm is None else getattr(b, preds_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LLMTrainCB(TrainCB):\n",
    "    def __init__(self, n_inp=1, inp_nm=None, lbl_nm=None, preds_nm=None):\n",
    "        self.n_inp = n_inp\n",
    "        self.n_inp,self.inp_nm,self.lbl_nm,self.preds_nm = n_inp,inp_nm,lbl_nm,preds_nm\n",
    "\n",
    "    def predict(self, learn):\n",
    "        learn.inps = _get_inp(learn.batch, self.n_inp, self.inp_nm)\n",
    "        learn.preds = learn.model(*learn.inps)\n",
    "\n",
    "    def get_loss(self, learn):\n",
    "        learn.lbl = _get_lbl(learn.batch, self.n_inp, self.lbl_nm)[0].flatten() # flattened\n",
    "        preds = _get_preds(learn.preds, self.preds_nm)\n",
    "        learn.loss = learn.loss_func(preds, learn.lbl)\n",
    "\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 75,834\n",
      "Total size: 0.29 MB\n"
     ]
    }
   ],
   "source": [
    "# model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "model = GPTModel(cfg).to(def_device)\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.00% [3/10 00:00&lt;00:01]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.098</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.028</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.184</td>\n",
       "      <td>6.534</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [5/25 00:00&lt;00:00 4.669]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = torch.optim.AdamW\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), \n",
    "       LLMTrainCB(inp_nm=['input_ids', 'cu_seqlens', 'max_seqlen'], lbl_nm='labels'),\n",
    "       ProgressCB(), DeviceCB()]  \n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs, opt_func=opt)\n",
    "learn.lr_find()\n",
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = 1e-4, 2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11ddac",
   "metadata": {},
   "source": [
    "GLU for ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639085e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.202</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.199</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.194</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.000</td>\n",
       "      <td>8.188</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "# model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "model = GPTModel(cfg).to(def_device)\n",
    "# cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(plot=True), DeviceCB(), MixedPrecision()]\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), \n",
    "       LLMTrainCB(inp_nm=['input_ids', 'cu_seqlens', 'max_seqlen'], lbl_nm='labels'),\n",
    "       ProgressCB(plot=True), DeviceCB()] \n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs+xtra, opt_func=opt)\n",
    "learn.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could not run or get enough food, so she couldn't even call for help!\n",
      "\n",
      "Lucy was so sad she didn't feel like when it was getting late. She regretted not getting to see the festival. Eventually, it was time to go to the festival. Lucy was sad that she had to leave.\n",
      "\n",
      "Then, Lucy had an idea. She decided to take a special nap under her favourite dress and put it in her dresser. After that, when she saw the festival, she realized that she had been weak for a moment. But, it made her feel better. She was relieved to see that all the way to the festival had paid off, but she learned that it must be okay to have an even worthier way. \n",
      "\n",
      "Lucy was happy and excited to have re\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "model.eval()\n",
    "token_ids = generate(\n",
    "    model=model.eval(),\n",
    "    idx=text_to_token_ids(\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\", tokenizer).to(def_device),\n",
    "    max_new_tokens=180,\n",
    "    context_size=cfg[\"ctx_len\"],\n",
    "    top_k=25,\n",
    "    temperature=1.1\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f52dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97909f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "1  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "2  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "3  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "4  Once upon a time, there lived a bunny in a fie...         NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('evaluation_prompts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, model, tokenizer, max_tokens=180, context_size=cfg[\"ctx_len\"], \n",
    "                top_k=25, temperature=1.3):\n",
    "    # Tokenize the prompt\n",
    "    toks = text_to_token_ids(row['prompt'], tokenizer)\n",
    "    \n",
    "    # Generate completion\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=toks.to(def_device),\n",
    "        max_new_tokens=max_tokens,\n",
    "        context_size=context_size,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract only the generated part (not the original prompt)\n",
    "    completion = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['completion'] = df.apply(lambda row: process_row(row, model, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b414bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"0401_init.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12510bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"0401_init.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. minbpe\n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019726f",
   "metadata": {},
   "source": [
    "Use triton cross entropy loss or compile nn.crosstropyloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a80a",
   "metadata": {},
   "source": [
    "Add view(-1,...) before flash attention and remove view(-1,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01d2c9f",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- What is a good padding token?\n",
    "- How to use dictionary?\n",
    "- `View` in MHA\n",
    "- What does `src_max_seq_len` do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72152b7",
   "metadata": {},
   "source": [
    "Tokenizer train with number fo merges and optionally add count."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
