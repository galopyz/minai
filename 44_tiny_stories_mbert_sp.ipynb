{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbf607",
   "metadata": {},
   "source": [
    "Using \n",
    "- sdpa \n",
    "- optimizer\n",
    "- compile \n",
    "- more data\n",
    "- MixedPrecision()\n",
    "- lr_sched\n",
    "- Double layer norm\n",
    "- Using a custom tokenizer.\n",
    "- GLU\n",
    "- sdpa linear\n",
    "- modern bert sequence packing + FA2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967927c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6938ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import RegexTokenizer\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbed3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.home()/'git/minai/TinyStories_All_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a959b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[257, 2365, 1597]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "# tokenizer.train(txt_raw, vocab_size=3000)\n",
    "\n",
    "tokenizer.load((path/\"tok3k_regex.model\").name) # loads the model back from disk\n",
    "tokenizer.encode(\"hello world\") # string -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_toks(txt, toker): return toker.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a08231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks_to_txt(toks, toker): return toker.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452255f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_txts = 10\n",
    "num_txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d73f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "separator=\"\\n\\n\\n\"\n",
    "ctx_len = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d68efb",
   "metadata": {},
   "source": [
    "We create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(b):\n",
    "    d = {}\n",
    "    d['input_ids'] = [tokenizer.encode(t, allowed_special={\"<|endoftext|>\"}) for t in b['text']]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ccdf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) â†’ ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(itertools.islice(iterator, n)):\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5014bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[516, 327, 44, 258, 390, 479, 402, 406, 507, 258]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds = ds.with_transform(transforms)\n",
    "tds['train'][0]['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48361f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9317e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   479,\n",
       "   402,\n",
       "   406,\n",
       "   507,\n",
       "   258,\n",
       "   775,\n",
       "   302,\n",
       "   313,\n",
       "   338,\n",
       "   720,\n",
       "   46,\n",
       "   342,\n",
       "   677,\n",
       "   309,\n",
       "   282,\n",
       "   2876,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   309,\n",
       "   708,\n",
       "   309,\n",
       "   282,\n",
       "   2073,\n",
       "   46,\n",
       "   406,\n",
       "   407,\n",
       "   265,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   328,\n",
       "   338,\n",
       "   386,\n",
       "   44,\n",
       "   391,\n",
       "   392,\n",
       "   468,\n",
       "   459,\n",
       "   119,\n",
       "   258,\n",
       "   1674,\n",
       "   354,\n",
       "   338,\n",
       "   2377,\n",
       "   304,\n",
       "   10,\n",
       "   670,\n",
       "   426,\n",
       "   265,\n",
       "   338,\n",
       "   386,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   844,\n",
       "   44,\n",
       "   337,\n",
       "   507,\n",
       "   733,\n",
       "   775,\n",
       "   302,\n",
       "   46,\n",
       "   1127,\n",
       "   349,\n",
       "   850,\n",
       "   309,\n",
       "   328,\n",
       "   524,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   627,\n",
       "   2377,\n",
       "   476,\n",
       "   937,\n",
       "   386,\n",
       "   565,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   732,\n",
       "   44,\n",
       "   406,\n",
       "   44,\n",
       "   363,\n",
       "   469,\n",
       "   850,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   629,\n",
       "   2377,\n",
       "   505,\n",
       "   10,\n",
       "   2826,\n",
       "   44,\n",
       "   360,\n",
       "   1208,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   459,\n",
       "   119,\n",
       "   263,\n",
       "   261,\n",
       "   1674,\n",
       "   354,\n",
       "   406,\n",
       "   384,\n",
       "   2377,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   364,\n",
       "   2876,\n",
       "   387,\n",
       "   493,\n",
       "   708,\n",
       "   360,\n",
       "   405,\n",
       "   1714,\n",
       "   266,\n",
       "   1398,\n",
       "   766,\n",
       "   558,\n",
       "   46,\n",
       "   1559,\n",
       "   360,\n",
       "   1699,\n",
       "   44,\n",
       "   406,\n",
       "   943,\n",
       "   338,\n",
       "   386,\n",
       "   387,\n",
       "   1714,\n",
       "   261,\n",
       "   775,\n",
       "   302,\n",
       "   266,\n",
       "   1125,\n",
       "   297,\n",
       "   338,\n",
       "   2377,\n",
       "   46,\n",
       "   312,\n",
       "   722,\n",
       "   536,\n",
       "   377,\n",
       "   708,\n",
       "   360,\n",
       "   365,\n",
       "   1208,\n",
       "   266,\n",
       "   1228,\n",
       "   458,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   528,\n",
       "   402,\n",
       "   2456,\n",
       "   626,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   508,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   313,\n",
       "   261,\n",
       "   631,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   258,\n",
       "   2489,\n",
       "   528,\n",
       "   708,\n",
       "   285,\n",
       "   704,\n",
       "   365,\n",
       "   561,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1213,\n",
       "   462,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   586,\n",
       "   2456,\n",
       "   626,\n",
       "   377,\n",
       "   266,\n",
       "   973,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1402,\n",
       "   817,\n",
       "   313,\n",
       "   261,\n",
       "   527,\n",
       "   634,\n",
       "   285,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   501,\n",
       "   365,\n",
       "   664,\n",
       "   1333,\n",
       "   383,\n",
       "   405,\n",
       "   1455,\n",
       "   297,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   532,\n",
       "   756,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   325,\n",
       "   328,\n",
       "   493,\n",
       "   46,\n",
       "   2456,\n",
       "   626,\n",
       "   2241,\n",
       "   776,\n",
       "   261,\n",
       "   501,\n",
       "   266,\n",
       "   1233,\n",
       "   261,\n",
       "   1333,\n",
       "   1455,\n",
       "   354,\n",
       "   475,\n",
       "   46,\n",
       "   316,\n",
       "   703,\n",
       "   266,\n",
       "   322,\n",
       "   626,\n",
       "   263,\n",
       "   340,\n",
       "   2771,\n",
       "   304,\n",
       "   10,\n",
       "   2120,\n",
       "   626,\n",
       "   477,\n",
       "   328,\n",
       "   261,\n",
       "   1455,\n",
       "   297,\n",
       "   1333,\n",
       "   431,\n",
       "   327,\n",
       "   46,\n",
       "   931,\n",
       "   309,\n",
       "   282,\n",
       "   397,\n",
       "   265,\n",
       "   483,\n",
       "   584,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   677,\n",
       "   285,\n",
       "   1341,\n",
       "   673,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   316,\n",
       "   426,\n",
       "   265,\n",
       "   261,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   962,\n",
       "   266,\n",
       "   660,\n",
       "   673,\n",
       "   2489,\n",
       "   268,\n",
       "   117,\n",
       "   417,\n",
       "   46,\n",
       "   1139,\n",
       "   44,\n",
       "   2456,\n",
       "   626,\n",
       "   282,\n",
       "   1367,\n",
       "   265,\n",
       "   483,\n",
       "   737,\n",
       "   266,\n",
       "   325,\n",
       "   601,\n",
       "   261,\n",
       "   988,\n",
       "   327,\n",
       "   46,\n",
       "   710,\n",
       "   2456,\n",
       "   626,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]},\n",
       " {'input_ids': [516,\n",
       "   327,\n",
       "   44,\n",
       "   258,\n",
       "   390,\n",
       "   779,\n",
       "   402,\n",
       "   1221,\n",
       "   282,\n",
       "   2439,\n",
       "   810,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   258,\n",
       "   346,\n",
       "   1798,\n",
       "   266,\n",
       "   407,\n",
       "   265,\n",
       "   322,\n",
       "   413,\n",
       "   46,\n",
       "   317,\n",
       "   1090,\n",
       "   44,\n",
       "   337,\n",
       "   743,\n",
       "   1221,\n",
       "   46,\n",
       "   1174,\n",
       "   349,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   476,\n",
       "   543,\n",
       "   261,\n",
       "   390,\n",
       "   779,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   506,\n",
       "   450,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   947,\n",
       "   44,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   367,\n",
       "   265,\n",
       "   325,\n",
       "   46,\n",
       "   337,\n",
       "   743,\n",
       "   1192,\n",
       "   266,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   2916,\n",
       "   505,\n",
       "   10,\n",
       "   1804,\n",
       "   536,\n",
       "   496,\n",
       "   409,\n",
       "   407,\n",
       "   265,\n",
       "   432,\n",
       "   261,\n",
       "   1798,\n",
       "   735,\n",
       "   833,\n",
       "   46,\n",
       "   316,\n",
       "   1588,\n",
       "   583,\n",
       "   266,\n",
       "   578,\n",
       "   371,\n",
       "   258,\n",
       "   1350,\n",
       "   46,\n",
       "   316,\n",
       "   1533,\n",
       "   383,\n",
       "   261,\n",
       "   631,\n",
       "   468,\n",
       "   533,\n",
       "   630,\n",
       "   1061,\n",
       "   46,\n",
       "   707,\n",
       "   44,\n",
       "   1221,\n",
       "   1588,\n",
       "   265,\n",
       "   261,\n",
       "   1306,\n",
       "   371,\n",
       "   261,\n",
       "   686,\n",
       "   266,\n",
       "   1041,\n",
       "   265,\n",
       "   261,\n",
       "   631,\n",
       "   44,\n",
       "   317,\n",
       "   1936,\n",
       "   44,\n",
       "   631,\n",
       "   44,\n",
       "   432,\n",
       "   627,\n",
       "   545,\n",
       "   358,\n",
       "   735,\n",
       "   2916,\n",
       "   266,\n",
       "   364,\n",
       "   1764,\n",
       "   1782,\n",
       "   687,\n",
       "   10,\n",
       "   412,\n",
       "   631,\n",
       "   837,\n",
       "   1221,\n",
       "   384,\n",
       "   946,\n",
       "   266,\n",
       "   389,\n",
       "   512,\n",
       "   832,\n",
       "   1061,\n",
       "   1366,\n",
       "   354,\n",
       "   261,\n",
       "   389,\n",
       "   440,\n",
       "   46,\n",
       "   284,\n",
       "   1798,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   833,\n",
       "   266,\n",
       "   364,\n",
       "   391,\n",
       "   1192,\n",
       "   46,\n",
       "   316,\n",
       "   382,\n",
       "   1221,\n",
       "   266,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   848,\n",
       "   349,\n",
       "   44,\n",
       "   390,\n",
       "   779,\n",
       "   44,\n",
       "   387,\n",
       "   1232,\n",
       "   524,\n",
       "   735,\n",
       "   2916,\n",
       "   46,\n",
       "   337,\n",
       "   862,\n",
       "   492,\n",
       "   735,\n",
       "   519,\n",
       "   337,\n",
       "   621,\n",
       "   1764,\n",
       "   1782,\n",
       "   972,\n",
       "   46,\n",
       "   1046,\n",
       "   384,\n",
       "   325,\n",
       "   458,\n",
       "   414,\n",
       "   710,\n",
       "   391,\n",
       "   44,\n",
       "   1221,\n",
       "   266,\n",
       "   261,\n",
       "   1798,\n",
       "   477,\n",
       "   266,\n",
       "   692,\n",
       "   561,\n",
       "   413,\n",
       "   46]},\n",
       " {'input_ids': [763,\n",
       "   438,\n",
       "   258,\n",
       "   397,\n",
       "   44,\n",
       "   313,\n",
       "   258,\n",
       "   1492,\n",
       "   1328,\n",
       "   371,\n",
       "   1286,\n",
       "   44,\n",
       "   401,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   380,\n",
       "   496,\n",
       "   708,\n",
       "   309,\n",
       "   449,\n",
       "   364,\n",
       "   500,\n",
       "   671,\n",
       "   413,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   558,\n",
       "   1286,\n",
       "   405,\n",
       "   346,\n",
       "   266,\n",
       "   973,\n",
       "   44,\n",
       "   409,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   563,\n",
       "   266,\n",
       "   2404,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   842,\n",
       "   118,\n",
       "   1245,\n",
       "   371,\n",
       "   261,\n",
       "   346,\n",
       "   1286,\n",
       "   304,\n",
       "   10,\n",
       "   516,\n",
       "   327,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   536,\n",
       "   258,\n",
       "   2578,\n",
       "   302,\n",
       "   313,\n",
       "   832,\n",
       "   2885,\n",
       "   46,\n",
       "   421,\n",
       "   282,\n",
       "   258,\n",
       "   390,\n",
       "   2922,\n",
       "   861,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   741,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   364,\n",
       "   265,\n",
       "   322,\n",
       "   496,\n",
       "   46,\n",
       "   284,\n",
       "   861,\n",
       "   323,\n",
       "   44,\n",
       "   317,\n",
       "   540,\n",
       "   486,\n",
       "   868,\n",
       "   708,\n",
       "   349,\n",
       "   500,\n",
       "   1304,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   383,\n",
       "   1027,\n",
       "   2526,\n",
       "   398,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   548,\n",
       "   265,\n",
       "   735,\n",
       "   258,\n",
       "   390,\n",
       "   833,\n",
       "   304,\n",
       "   10,\n",
       "   934,\n",
       "   397,\n",
       "   426,\n",
       "   354,\n",
       "   44,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   2025,\n",
       "   673,\n",
       "   266,\n",
       "   673,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   46,\n",
       "   1407,\n",
       "   261,\n",
       "   823,\n",
       "   313,\n",
       "   261,\n",
       "   1492,\n",
       "   552,\n",
       "   265,\n",
       "   711,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   1582,\n",
       "   266,\n",
       "   325,\n",
       "   776,\n",
       "   261,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   282,\n",
       "   377,\n",
       "   708,\n",
       "   309,\n",
       "   365,\n",
       "   664,\n",
       "   413,\n",
       "   972,\n",
       "   46,\n",
       "   284,\n",
       "   277,\n",
       "   370,\n",
       "   439,\n",
       "   501,\n",
       "   691,\n",
       "   383,\n",
       "   1011,\n",
       "   1314,\n",
       "   469,\n",
       "   322,\n",
       "   258,\n",
       "   561,\n",
       "   1253,\n",
       "   46,\n",
       "   710,\n",
       "   360,\n",
       "   431,\n",
       "   636,\n",
       "   992,\n",
       "   933,\n",
       "   886,\n",
       "   46]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "next(batched(tds['train'].select(range(50)), bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ead127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches = list(batched(tds['train'].select(range(100)), bs))\n",
    "len(input_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8732d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'input_ids': [516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    507,\n",
       "    258,\n",
       "    775,\n",
       "    302,\n",
       "    313,\n",
       "    338,\n",
       "    720,\n",
       "    46,\n",
       "    342,\n",
       "    677,\n",
       "    309,\n",
       "    282,\n",
       "    2876,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    309,\n",
       "    708,\n",
       "    309,\n",
       "    282,\n",
       "    2073,\n",
       "    46,\n",
       "    406,\n",
       "    407,\n",
       "    265,\n",
       "    850,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    328,\n",
       "    338,\n",
       "    386,\n",
       "    44,\n",
       "    391,\n",
       "    392,\n",
       "    468,\n",
       "    459,\n",
       "    119,\n",
       "    258,\n",
       "    1674,\n",
       "    354,\n",
       "    338,\n",
       "    2377,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    426,\n",
       "    265,\n",
       "    338,\n",
       "    386,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    844,\n",
       "    44,\n",
       "    337,\n",
       "    507,\n",
       "    733,\n",
       "    775,\n",
       "    302,\n",
       "    46,\n",
       "    1127,\n",
       "    349,\n",
       "    850,\n",
       "    309,\n",
       "    328,\n",
       "    524,\n",
       "    266,\n",
       "    459,\n",
       "    119,\n",
       "    627,\n",
       "    2377,\n",
       "    476,\n",
       "    937,\n",
       "    386,\n",
       "    565,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    732,\n",
       "    44,\n",
       "    406,\n",
       "    44,\n",
       "    363,\n",
       "    469,\n",
       "    850,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    1125,\n",
       "    629,\n",
       "    2377,\n",
       "    505,\n",
       "    10,\n",
       "    2826,\n",
       "    44,\n",
       "    360,\n",
       "    1208,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    459,\n",
       "    119,\n",
       "    263,\n",
       "    261,\n",
       "    1674,\n",
       "    354,\n",
       "    406,\n",
       "    384,\n",
       "    2377,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    364,\n",
       "    2876,\n",
       "    387,\n",
       "    493,\n",
       "    708,\n",
       "    360,\n",
       "    405,\n",
       "    1714,\n",
       "    266,\n",
       "    1398,\n",
       "    766,\n",
       "    558,\n",
       "    46,\n",
       "    1559,\n",
       "    360,\n",
       "    1699,\n",
       "    44,\n",
       "    406,\n",
       "    943,\n",
       "    338,\n",
       "    386,\n",
       "    387,\n",
       "    1714,\n",
       "    261,\n",
       "    775,\n",
       "    302,\n",
       "    266,\n",
       "    1125,\n",
       "    297,\n",
       "    338,\n",
       "    2377,\n",
       "    46,\n",
       "    312,\n",
       "    722,\n",
       "    536,\n",
       "    377,\n",
       "    708,\n",
       "    360,\n",
       "    365,\n",
       "    1208,\n",
       "    266,\n",
       "    1228,\n",
       "    458,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    528,\n",
       "    402,\n",
       "    2456,\n",
       "    626,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    508,\n",
       "    265,\n",
       "    483,\n",
       "    737,\n",
       "    266,\n",
       "    325,\n",
       "    313,\n",
       "    261,\n",
       "    631,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    258,\n",
       "    2489,\n",
       "    528,\n",
       "    708,\n",
       "    285,\n",
       "    704,\n",
       "    365,\n",
       "    561,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    1213,\n",
       "    462,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    586,\n",
       "    2456,\n",
       "    626,\n",
       "    377,\n",
       "    266,\n",
       "    973,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    1402,\n",
       "    817,\n",
       "    313,\n",
       "    261,\n",
       "    527,\n",
       "    634,\n",
       "    285,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    501,\n",
       "    365,\n",
       "    664,\n",
       "    1333,\n",
       "    383,\n",
       "    405,\n",
       "    1455,\n",
       "    297,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    532,\n",
       "    756,\n",
       "    261,\n",
       "    1333,\n",
       "    1455,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    493,\n",
       "    46,\n",
       "    2456,\n",
       "    626,\n",
       "    2241,\n",
       "    776,\n",
       "    261,\n",
       "    501,\n",
       "    266,\n",
       "    1233,\n",
       "    261,\n",
       "    1333,\n",
       "    1455,\n",
       "    354,\n",
       "    475,\n",
       "    46,\n",
       "    316,\n",
       "    703,\n",
       "    266,\n",
       "    322,\n",
       "    626,\n",
       "    263,\n",
       "    340,\n",
       "    2771,\n",
       "    304,\n",
       "    10,\n",
       "    2120,\n",
       "    626,\n",
       "    477,\n",
       "    328,\n",
       "    261,\n",
       "    1455,\n",
       "    297,\n",
       "    1333,\n",
       "    431,\n",
       "    327,\n",
       "    46,\n",
       "    931,\n",
       "    309,\n",
       "    282,\n",
       "    397,\n",
       "    265,\n",
       "    483,\n",
       "    584,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    677,\n",
       "    285,\n",
       "    1341,\n",
       "    673,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    316,\n",
       "    426,\n",
       "    265,\n",
       "    261,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    962,\n",
       "    266,\n",
       "    660,\n",
       "    673,\n",
       "    2489,\n",
       "    268,\n",
       "    117,\n",
       "    417,\n",
       "    46,\n",
       "    1139,\n",
       "    44,\n",
       "    2456,\n",
       "    626,\n",
       "    282,\n",
       "    1367,\n",
       "    265,\n",
       "    483,\n",
       "    737,\n",
       "    266,\n",
       "    325,\n",
       "    601,\n",
       "    261,\n",
       "    988,\n",
       "    327,\n",
       "    46,\n",
       "    710,\n",
       "    2456,\n",
       "    626,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]},\n",
       "  {'input_ids': [516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    779,\n",
       "    402,\n",
       "    1221,\n",
       "    282,\n",
       "    2439,\n",
       "    810,\n",
       "    261,\n",
       "    389,\n",
       "    440,\n",
       "    46,\n",
       "    316,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    1798,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    322,\n",
       "    413,\n",
       "    46,\n",
       "    317,\n",
       "    1090,\n",
       "    44,\n",
       "    337,\n",
       "    743,\n",
       "    1221,\n",
       "    46,\n",
       "    1174,\n",
       "    349,\n",
       "    367,\n",
       "    265,\n",
       "    325,\n",
       "    476,\n",
       "    543,\n",
       "    261,\n",
       "    390,\n",
       "    779,\n",
       "    46,\n",
       "    284,\n",
       "    1798,\n",
       "    506,\n",
       "    450,\n",
       "    1221,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    947,\n",
       "    44,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    367,\n",
       "    265,\n",
       "    325,\n",
       "    46,\n",
       "    337,\n",
       "    743,\n",
       "    1192,\n",
       "    266,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    735,\n",
       "    2916,\n",
       "    505,\n",
       "    10,\n",
       "    1804,\n",
       "    536,\n",
       "    496,\n",
       "    409,\n",
       "    407,\n",
       "    265,\n",
       "    432,\n",
       "    261,\n",
       "    1798,\n",
       "    735,\n",
       "    833,\n",
       "    46,\n",
       "    316,\n",
       "    1588,\n",
       "    583,\n",
       "    266,\n",
       "    578,\n",
       "    371,\n",
       "    258,\n",
       "    1350,\n",
       "    46,\n",
       "    316,\n",
       "    1533,\n",
       "    383,\n",
       "    261,\n",
       "    631,\n",
       "    468,\n",
       "    533,\n",
       "    630,\n",
       "    1061,\n",
       "    46,\n",
       "    707,\n",
       "    44,\n",
       "    1221,\n",
       "    1588,\n",
       "    265,\n",
       "    261,\n",
       "    1306,\n",
       "    371,\n",
       "    261,\n",
       "    686,\n",
       "    266,\n",
       "    1041,\n",
       "    265,\n",
       "    261,\n",
       "    631,\n",
       "    44,\n",
       "    317,\n",
       "    1936,\n",
       "    44,\n",
       "    631,\n",
       "    44,\n",
       "    432,\n",
       "    627,\n",
       "    545,\n",
       "    358,\n",
       "    735,\n",
       "    2916,\n",
       "    266,\n",
       "    364,\n",
       "    1764,\n",
       "    1782,\n",
       "    687,\n",
       "    10,\n",
       "    412,\n",
       "    631,\n",
       "    837,\n",
       "    1221,\n",
       "    384,\n",
       "    946,\n",
       "    266,\n",
       "    389,\n",
       "    512,\n",
       "    832,\n",
       "    1061,\n",
       "    1366,\n",
       "    354,\n",
       "    261,\n",
       "    389,\n",
       "    440,\n",
       "    46,\n",
       "    284,\n",
       "    1798,\n",
       "    548,\n",
       "    265,\n",
       "    735,\n",
       "    833,\n",
       "    266,\n",
       "    364,\n",
       "    391,\n",
       "    1192,\n",
       "    46,\n",
       "    316,\n",
       "    382,\n",
       "    1221,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    848,\n",
       "    349,\n",
       "    44,\n",
       "    390,\n",
       "    779,\n",
       "    44,\n",
       "    387,\n",
       "    1232,\n",
       "    524,\n",
       "    735,\n",
       "    2916,\n",
       "    46,\n",
       "    337,\n",
       "    862,\n",
       "    492,\n",
       "    735,\n",
       "    519,\n",
       "    337,\n",
       "    621,\n",
       "    1764,\n",
       "    1782,\n",
       "    972,\n",
       "    46,\n",
       "    1046,\n",
       "    384,\n",
       "    325,\n",
       "    458,\n",
       "    414,\n",
       "    710,\n",
       "    391,\n",
       "    44,\n",
       "    1221,\n",
       "    266,\n",
       "    261,\n",
       "    1798,\n",
       "    477,\n",
       "    266,\n",
       "    692,\n",
       "    561,\n",
       "    413,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    1492,\n",
       "    1328,\n",
       "    371,\n",
       "    1286,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    380,\n",
       "    496,\n",
       "    708,\n",
       "    309,\n",
       "    449,\n",
       "    364,\n",
       "    500,\n",
       "    671,\n",
       "    413,\n",
       "    46,\n",
       "    1407,\n",
       "    261,\n",
       "    558,\n",
       "    1286,\n",
       "    405,\n",
       "    346,\n",
       "    266,\n",
       "    973,\n",
       "    44,\n",
       "    409,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    563,\n",
       "    266,\n",
       "    2404,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    842,\n",
       "    118,\n",
       "    1245,\n",
       "    371,\n",
       "    261,\n",
       "    346,\n",
       "    1286,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    536,\n",
       "    258,\n",
       "    2578,\n",
       "    302,\n",
       "    313,\n",
       "    832,\n",
       "    2885,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    2922,\n",
       "    861,\n",
       "    46,\n",
       "    284,\n",
       "    861,\n",
       "    741,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    364,\n",
       "    265,\n",
       "    322,\n",
       "    496,\n",
       "    46,\n",
       "    284,\n",
       "    861,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    540,\n",
       "    486,\n",
       "    868,\n",
       "    708,\n",
       "    349,\n",
       "    500,\n",
       "    1304,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    383,\n",
       "    1027,\n",
       "    2526,\n",
       "    398,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    548,\n",
       "    265,\n",
       "    735,\n",
       "    258,\n",
       "    390,\n",
       "    833,\n",
       "    304,\n",
       "    10,\n",
       "    934,\n",
       "    397,\n",
       "    426,\n",
       "    354,\n",
       "    44,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    2025,\n",
       "    673,\n",
       "    266,\n",
       "    673,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    46,\n",
       "    1407,\n",
       "    261,\n",
       "    823,\n",
       "    313,\n",
       "    261,\n",
       "    1492,\n",
       "    552,\n",
       "    265,\n",
       "    711,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    1582,\n",
       "    266,\n",
       "    325,\n",
       "    776,\n",
       "    261,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    282,\n",
       "    377,\n",
       "    708,\n",
       "    309,\n",
       "    365,\n",
       "    664,\n",
       "    413,\n",
       "    972,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    370,\n",
       "    439,\n",
       "    501,\n",
       "    691,\n",
       "    383,\n",
       "    1011,\n",
       "    1314,\n",
       "    469,\n",
       "    322,\n",
       "    258,\n",
       "    561,\n",
       "    1253,\n",
       "    46,\n",
       "    710,\n",
       "    360,\n",
       "    431,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]}],\n",
       " [{'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    46,\n",
       "    406,\n",
       "    532,\n",
       "    265,\n",
       "    1371,\n",
       "    392,\n",
       "    282,\n",
       "    258,\n",
       "    2053,\n",
       "    2184,\n",
       "    2108,\n",
       "    46,\n",
       "    342,\n",
       "    636,\n",
       "    313,\n",
       "    258,\n",
       "    346,\n",
       "    1576,\n",
       "    328,\n",
       "    338,\n",
       "    696,\n",
       "    413,\n",
       "    44,\n",
       "    258,\n",
       "    456,\n",
       "    266,\n",
       "    258,\n",
       "    465,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    1000,\n",
       "    585,\n",
       "    313,\n",
       "    261,\n",
       "    1576,\n",
       "    44,\n",
       "    406,\n",
       "    507,\n",
       "    258,\n",
       "    346,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    46,\n",
       "    284,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    282,\n",
       "    313,\n",
       "    261,\n",
       "    936,\n",
       "    371,\n",
       "    338,\n",
       "    442,\n",
       "    1043,\n",
       "    46,\n",
       "    342,\n",
       "    407,\n",
       "    265,\n",
       "    650,\n",
       "    334,\n",
       "    292,\n",
       "    371,\n",
       "    309,\n",
       "    44,\n",
       "    409,\n",
       "    392,\n",
       "    282,\n",
       "    607,\n",
       "    371,\n",
       "    261,\n",
       "    2212,\n",
       "    383,\n",
       "    636,\n",
       "    401,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    543,\n",
       "    338,\n",
       "    413,\n",
       "    44,\n",
       "    261,\n",
       "    456,\n",
       "    266,\n",
       "    261,\n",
       "    465,\n",
       "    44,\n",
       "    265,\n",
       "    432,\n",
       "    338,\n",
       "    46,\n",
       "    312,\n",
       "    431,\n",
       "    1228,\n",
       "    458,\n",
       "    265,\n",
       "    873,\n",
       "    261,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    46,\n",
       "    284,\n",
       "    2212,\n",
       "    282,\n",
       "    496,\n",
       "    44,\n",
       "    409,\n",
       "    309,\n",
       "    507,\n",
       "    258,\n",
       "    545,\n",
       "    584,\n",
       "    697,\n",
       "    46,\n",
       "    406,\n",
       "    44,\n",
       "    261,\n",
       "    456,\n",
       "    44,\n",
       "    266,\n",
       "    261,\n",
       "    465,\n",
       "    405,\n",
       "    377,\n",
       "    360,\n",
       "    468,\n",
       "    325,\n",
       "    1807,\n",
       "    261,\n",
       "    277,\n",
       "    571,\n",
       "    2066,\n",
       "    98,\n",
       "    313,\n",
       "    261,\n",
       "    936,\n",
       "    46,\n",
       "    710,\n",
       "    360,\n",
       "    431,\n",
       "    636,\n",
       "    992,\n",
       "    933,\n",
       "    886,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    346,\n",
       "    1903,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    46,\n",
       "    284,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    532,\n",
       "    265,\n",
       "    1349,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    431,\n",
       "    327,\n",
       "    734,\n",
       "    46,\n",
       "    421,\n",
       "    282,\n",
       "    380,\n",
       "    377,\n",
       "    634,\n",
       "    309,\n",
       "    468,\n",
       "    1349,\n",
       "    266,\n",
       "    2290,\n",
       "    313,\n",
       "    261,\n",
       "    1903,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    327,\n",
       "    44,\n",
       "    258,\n",
       "    390,\n",
       "    497,\n",
       "    402,\n",
       "    341,\n",
       "    552,\n",
       "    265,\n",
       "    325,\n",
       "    328,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    46,\n",
       "    341,\n",
       "    266,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    1814,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    458,\n",
       "    46,\n",
       "    312,\n",
       "    703,\n",
       "    266,\n",
       "    365,\n",
       "    258,\n",
       "    653,\n",
       "    371,\n",
       "    442,\n",
       "    46,\n",
       "    284,\n",
       "    631,\n",
       "    282,\n",
       "    1875,\n",
       "    44,\n",
       "    266,\n",
       "    261,\n",
       "    686,\n",
       "    282,\n",
       "    1061,\n",
       "    304,\n",
       "    10,\n",
       "    1260,\n",
       "    258,\n",
       "    1000,\n",
       "    44,\n",
       "    309,\n",
       "    282,\n",
       "    397,\n",
       "    387,\n",
       "    341,\n",
       "    265,\n",
       "    483,\n",
       "    584,\n",
       "    46,\n",
       "    316,\n",
       "    323,\n",
       "    1418,\n",
       "    265,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    266,\n",
       "    760,\n",
       "    309,\n",
       "    258,\n",
       "    346,\n",
       "    695,\n",
       "    46,\n",
       "    284,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    282,\n",
       "    496,\n",
       "    265,\n",
       "    547,\n",
       "    341,\n",
       "    483,\n",
       "    44,\n",
       "    409,\n",
       "    309,\n",
       "    677,\n",
       "    360,\n",
       "    529,\n",
       "    325,\n",
       "    458,\n",
       "    601,\n",
       "    1105,\n",
       "    46,\n",
       "    707,\n",
       "    44,\n",
       "    261,\n",
       "    1800,\n",
       "    399,\n",
       "    279,\n",
       "    881,\n",
       "    1102,\n",
       "    1349,\n",
       "    297,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    44,\n",
       "    2425,\n",
       "    387,\n",
       "    261,\n",
       "    988,\n",
       "    442,\n",
       "    327,\n",
       "    328,\n",
       "    341,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    563,\n",
       "    1020,\n",
       "    44,\n",
       "    401,\n",
       "    282,\n",
       "    258,\n",
       "    1974,\n",
       "    2887,\n",
       "    390,\n",
       "    479,\n",
       "    402,\n",
       "    406,\n",
       "    46,\n",
       "    342,\n",
       "    282,\n",
       "    704,\n",
       "    496,\n",
       "    708,\n",
       "    392,\n",
       "    1036,\n",
       "    338,\n",
       "    1226,\n",
       "    446,\n",
       "    44,\n",
       "    258,\n",
       "    2963,\n",
       "    302,\n",
       "    46,\n",
       "    342,\n",
       "    506,\n",
       "    1577,\n",
       "    313,\n",
       "    338,\n",
       "    605,\n",
       "    409,\n",
       "    468,\n",
       "    364,\n",
       "    572,\n",
       "    309,\n",
       "    304,\n",
       "    10,\n",
       "    516,\n",
       "    1163,\n",
       "    327,\n",
       "    44,\n",
       "    406,\n",
       "    426,\n",
       "    265,\n",
       "    261,\n",
       "    527,\n",
       "    265,\n",
       "    325,\n",
       "    46,\n",
       "    342,\n",
       "    382,\n",
       "    258,\n",
       "    346,\n",
       "    2010,\n",
       "    371,\n",
       "    686,\n",
       "    266,\n",
       "    578,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    1446,\n",
       "    322,\n",
       "    401,\n",
       "    46,\n",
       "    342,\n",
       "    587,\n",
       "    338,\n",
       "    789,\n",
       "    313,\n",
       "    261,\n",
       "    686,\n",
       "    265,\n",
       "    391,\n",
       "    881,\n",
       "    309,\n",
       "    266,\n",
       "    506,\n",
       "    387,\n",
       "    338,\n",
       "    446,\n",
       "    46,\n",
       "    342,\n",
       "    536,\n",
       "    575,\n",
       "    450,\n",
       "    261,\n",
       "    1554,\n",
       "    283,\n",
       "    371,\n",
       "    261,\n",
       "    2010,\n",
       "    304,\n",
       "    10,\n",
       "    670,\n",
       "    1487,\n",
       "    309,\n",
       "    485,\n",
       "    266,\n",
       "    382,\n",
       "    383,\n",
       "    309,\n",
       "    282,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    33,\n",
       "    342,\n",
       "    282,\n",
       "    391,\n",
       "    377,\n",
       "    383,\n",
       "    392,\n",
       "    507,\n",
       "    309,\n",
       "    46,\n",
       "    939,\n",
       "    383,\n",
       "    327,\n",
       "    354,\n",
       "    44,\n",
       "    406,\n",
       "    282,\n",
       "    858,\n",
       "    1974,\n",
       "    2887,\n",
       "    601,\n",
       "    46,\n",
       "    342,\n",
       "    477,\n",
       "    328,\n",
       "    338,\n",
       "    2963,\n",
       "    302,\n",
       "    589,\n",
       "    327,\n",
       "    266,\n",
       "    704,\n",
       "    1102,\n",
       "    309,\n",
       "    1372,\n",
       "    265,\n",
       "    338,\n",
       "    46,\n",
       "    710,\n",
       "    634,\n",
       "    392,\n",
       "    382,\n",
       "    1851,\n",
       "    1120,\n",
       "    44,\n",
       "    392,\n",
       "    529,\n",
       "    963,\n",
       "    266,\n",
       "    1195,\n",
       "    756,\n",
       "    392,\n",
       "    507,\n",
       "    338,\n",
       "    446,\n",
       "    46]},\n",
       "  {'input_ids': [763,\n",
       "    438,\n",
       "    258,\n",
       "    397,\n",
       "    44,\n",
       "    313,\n",
       "    258,\n",
       "    2770,\n",
       "    1020,\n",
       "    44,\n",
       "    401,\n",
       "    636,\n",
       "    258,\n",
       "    390,\n",
       "    497,\n",
       "    402,\n",
       "    341,\n",
       "    46,\n",
       "    341,\n",
       "    508,\n",
       "    265,\n",
       "    740,\n",
       "    266,\n",
       "    325,\n",
       "    697,\n",
       "    46,\n",
       "    451,\n",
       "    327,\n",
       "    44,\n",
       "    341,\n",
       "    382,\n",
       "    258,\n",
       "    1178,\n",
       "    313,\n",
       "    261,\n",
       "    527,\n",
       "    46,\n",
       "    316,\n",
       "    282,\n",
       "    729,\n",
       "    266,\n",
       "    407,\n",
       "    265,\n",
       "    1528,\n",
       "    261,\n",
       "    1178,\n",
       "    304,\n",
       "    10,\n",
       "    645,\n",
       "    426,\n",
       "    265,\n",
       "    340,\n",
       "    358,\n",
       "    44,\n",
       "    1669,\n",
       "    44,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    914,\n",
       "    384,\n",
       "    1922,\n",
       "    261,\n",
       "    1178,\n",
       "    414,\n",
       "    1669,\n",
       "    565,\n",
       "    266,\n",
       "    323,\n",
       "    44,\n",
       "    317,\n",
       "    732,\n",
       "    44,\n",
       "    781,\n",
       "    384,\n",
       "    483,\n",
       "    414,\n",
       "    312,\n",
       "    275,\n",
       "    1879,\n",
       "    368,\n",
       "    328,\n",
       "    261,\n",
       "    558,\n",
       "    1006,\n",
       "    266,\n",
       "    1564,\n",
       "    387,\n",
       "    261,\n",
       "    1178,\n",
       "    265,\n",
       "    322,\n",
       "    103,\n",
       "    270,\n",
       "    46,\n",
       "    931,\n",
       "    360,\n",
       "    837,\n",
       "    261,\n",
       "    1504,\n",
       "    317,\n",
       "    71,\n",
       "    111,\n",
       "    414,\n",
       "    44,\n",
       "    360,\n",
       "    548,\n",
       "    1374,\n",
       "    445,\n",
       "    737,\n",
       "    445,\n",
       "    360,\n",
       "    468,\n",
       "    304,\n",
       "    10,\n",
       "    645,\n",
       "    266,\n",
       "    1669,\n",
       "    598,\n",
       "    328,\n",
       "    431,\n",
       "    470,\n",
       "    2310,\n",
       "    263,\n",
       "    44,\n",
       "    1847,\n",
       "    266,\n",
       "    1111,\n",
       "    442,\n",
       "    46,\n",
       "    312,\n",
       "    468,\n",
       "    735,\n",
       "    261,\n",
       "    861,\n",
       "    313,\n",
       "    470,\n",
       "    1309,\n",
       "    445,\n",
       "    360,\n",
       "    334,\n",
       "    2490,\n",
       "    265,\n",
       "    261,\n",
       "    2317,\n",
       "    2315,\n",
       "    46,\n",
       "    964,\n",
       "    261,\n",
       "    882,\n",
       "    44,\n",
       "    341,\n",
       "    1583,\n",
       "    261,\n",
       "    1178,\n",
       "    266,\n",
       "    1669,\n",
       "    552,\n",
       "    313,\n",
       "    459,\n",
       "    1324,\n",
       "    262,\n",
       "    46,\n",
       "    312,\n",
       "    405,\n",
       "    722,\n",
       "    391,\n",
       "    377,\n",
       "    266,\n",
       "    900,\n",
       "    371,\n",
       "    493,\n",
       "    2933,\n",
       "    46,\n",
       "    312,\n",
       "    2798,\n",
       "    1771,\n",
       "    328,\n",
       "    470,\n",
       "    413,\n",
       "    266,\n",
       "    365,\n",
       "    258,\n",
       "    968,\n",
       "    327,\n",
       "    450,\n",
       "    261,\n",
       "    527,\n",
       "    46]}]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batches[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c607d",
   "metadata": {},
   "source": [
    "### Sequence packer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "from composer.core.types import Batch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "import math\n",
    "from composer.core import Time\n",
    "\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences for causal attention.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": torch.from_numpy(np.append(batch[:, 1:], self.pad_token_id)),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            yield yieldval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95469fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_batches = list(batched(xds, bs))\n",
    "d = {\n",
    "    \"src_iterable\": input_batches,\n",
    "    \"src_batch_size\": bs,\n",
    "    \"src_max_seq_len\": 1,\n",
    "    \"out_batch_size\": 1,\n",
    "    \"out_pseq_len\": 170,\n",
    "    \"buffer_size\": 5,\n",
    "    \"pad_token_id\": -1,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e285d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 516,  327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,\n",
       "           313,  338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,\n",
       "           309,  708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,\n",
       "           302,  328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,\n",
       "           354,  338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,\n",
       "            44,  317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,\n",
       "           850,  309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,\n",
       "           565,  266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,\n",
       "           261,  775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360,\n",
       "          1208,  261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,\n",
       "           384, 2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405,\n",
       "          1714,  266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,\n",
       "           338,  386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,\n",
       "            46,  312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,\n",
       "            46,   -1]]),\n",
       " 'labels': tensor([ 327,   44,  258,  390,  479,  402,  406,  507,  258,  775,  302,  313,\n",
       "          338,  720,   46,  342,  677,  309,  282, 2876,  265,  325,  328,  309,\n",
       "          708,  309,  282, 2073,   46,  406,  407,  265,  850,  261,  775,  302,\n",
       "          328,  338,  386,   44,  391,  392,  468,  459,  119,  258, 1674,  354,\n",
       "          338, 2377,  304,   10,  670,  426,  265,  338,  386,  266,  323,   44,\n",
       "          317,  844,   44,  337,  507,  733,  775,  302,   46, 1127,  349,  850,\n",
       "          309,  328,  524,  266,  459,  119,  627, 2377,  476,  937,  386,  565,\n",
       "          266,  323,   44,  317,  732,   44,  406,   44,  363,  469,  850,  261,\n",
       "          775,  302,  266, 1125,  629, 2377,  505,   10, 2826,   44,  360, 1208,\n",
       "          261,  775,  302,  266,  459,  119,  263,  261, 1674,  354,  406,  384,\n",
       "         2377,   46,  421,  282,  364, 2876,  387,  493,  708,  360,  405, 1714,\n",
       "          266, 1398,  766,  558,   46, 1559,  360, 1699,   44,  406,  943,  338,\n",
       "          386,  387, 1714,  261,  775,  302,  266, 1125,  297,  338, 2377,   46,\n",
       "          312,  722,  536,  377,  708,  360,  365, 1208,  266, 1228,  458,   46,\n",
       "           -1,   -1]),\n",
       " 'cu_seqlens': [tensor([  0, 169, 170], dtype=torch.int32)],\n",
       " 'max_seqlen': [169]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches = next(iter(CausalSequencePacker(**d)))\n",
    "out_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### FlashCausalAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Flash Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    Flash Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with Flash Attention implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - Check if Flash Attention is available (FLASH_ATTN_AVAILABLE)\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [total_seq_len, hidden_dim].\n",
    "            cu_seqlens: Cumulative sequence lengths tensor of shape [batch_size + 1]\n",
    "                    Used instead of an attention mask for both masking and\n",
    "                    variable-length sequences. Example:\n",
    "                        cu_seqlens = torch.tensor([0, 10, 30, 60])\n",
    "                    This means there are three sequences in the batch:\n",
    "                        - First sequence has 10 tokens\n",
    "                        - Second sequence has 20 tokens\n",
    "                        - Third sequence has 30 tokens\n",
    "            max_seqlen: Maximum sequence length in the batch. In the example above,\n",
    "                        the maximum sequence length is 30.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [total_seq_len, hidden_dim] after attention.\n",
    "        \"\"\"\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            raise ImportError(\"Flash Attention is not available. Please install it with `pip install flash-attn`\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x) # [batch_size, seq_len, d_out]\n",
    "\n",
    "        k_reshaped = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        q_reshaped = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v_reshaped = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q_reshaped,\n",
    "            k_reshaped,\n",
    "            v_reshaped,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 170])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLASH_ATTN_AVAILABLE = False\n",
    "try:\n",
    "    from flash_attn import flash_attn_varlen_func\n",
    "\n",
    "    FLASH_ATTN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    # Flash Attention is not available\n",
    "    pass\n",
    "\n",
    "with torch.no_grad(), torch.amp.autocast(device_type=def_device, dtype=torch.bfloat16):\n",
    "    mha = FlashCausalAttentionBlock(hidden_dim=170, num_heads=1).to(def_device)\n",
    "    output = mha(out_batches['input_ids'].to(torch.float32).to(def_device), out_batches['cu_seqlens'][0].to(def_device), out_batches['max_seqlen'][0])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim, bias=False)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "    The Gated Linear Unit has two parallel linear transforms: one for the gate and one for the value.\n",
    "    Apply the activation only to the gate, then multiply elementwise with the value, followed by a\n",
    "    final linear projection and optional dropout.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        intermediate_dim: int,\n",
    "        act: nn.Module = nn.GELU,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a GLU.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            intermediate_dim: Dimension of each intermediate branch\n",
    "                              Often set to 2/3 * 4 * hidden_dim to maintain similar parameter\n",
    "                              count to a standard MLP with 4x expansion\n",
    "            activation: Activation function to use, defaults to GELU\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Wv = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.Wg = nn.Linear(hidden_dim, intermediate_dim)\n",
    "        self.act = act\n",
    "        self.Wo = nn.Linear(intermediate_dim, hidden_dim)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim] or [total_seq_len, hidden_dim]\n",
    "        \"\"\"\n",
    "        gate = self.act(self.Wg(x))\n",
    "        val = self.Wv(x)\n",
    "        out = self.Wo(gate * val)\n",
    "        return self.do(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.ln3 = nn.LayerNorm(emb_dim)\n",
    "        self.ln4 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = FlashCausalAttentionBlock(hidden_dim=emb_dim, num_heads=n_head, dropout=drop_out)\n",
    "        self.ff = GLU(emb_dim, int(emb_dim*ff_mult), act=act)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.ln2(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln3(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.ln4(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias'], cfg['act']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:].to(def_device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d440d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, dtype=torch.bfloat16):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def before_fit(self, learn): self.scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    def before_batch(self, learn):\n",
    "        self.autocast = torch.autocast(\"cuda\", dtype=self.dtype)\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_loss(self, learn): self.autocast.__exit__(None, None, None)\n",
    "        \n",
    "    def backward(self, learn): self.scaler.scale(learn.loss).backward()\n",
    "\n",
    "    def step(self, learn):\n",
    "        self.scaler.step(learn.opt)\n",
    "        self.scaler.update()\n",
    "\n",
    "class AccelerateCB(TrainCB):\n",
    "    order = DeviceCB.order+10\n",
    "    def __init__(self, n_inp=1, mixed_precision=\"fp16\"):\n",
    "        super().__init__(n_inp=n_inp)\n",
    "        self.acc = Accelerator(mixed_precision=mixed_precision)\n",
    "        \n",
    "    def before_fit(self, learn):\n",
    "        learn.model,learn.opt,learn.dls.train,learn.dls.valid = self.acc.prepare(\n",
    "            learn.model, learn.opt, learn.dls.train, learn.dls.valid)\n",
    "    \n",
    "    def after_fit(self, learn): learn.model = self.acc.unwrap_model(learn.model)\n",
    "    def backward(self, learn): self.acc.backward(learn.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6ef94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 12,    # num transformer blocks\n",
    "    'vocab_sz': 3008,\n",
    "    'emb_dim': 384,\n",
    "    'ctx_len': ctx_len,\n",
    "    'n_head': 12,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 2/3 * 4,\n",
    "    'qkv_bias': False,\n",
    "    'act': nn.GELU(),   # activation function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72d6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 24,787,136\n",
      "Total size: 94.56 MB\n"
     ]
    }
   ],
   "source": [
    "model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        super().__init__(*ms, **metrics)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = learn.batch\n",
    "        for m in self.metrics.values(): m.update(learn.preds.flatten(0, 1), y.flatten())\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), DeviceCB(), MixedPrecision()]\n\u001b[32m      3\u001b[39m learn = Learner(model, out_batches, loss_func=loss_fn, cbs=cbs, opt_func=opt)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:462\u001b[39m, in \u001b[36mlr_find\u001b[39m\u001b[34m(self, gamma, max_mult, start_lr, max_epochs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;129m@fc\u001b[39m.patch\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m:Learner, gamma=\u001b[32m1.3\u001b[39m, max_mult=\u001b[32m3\u001b[39m, start_lr=\u001b[32m1e-5\u001b[39m, max_epochs=\u001b[32m10\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLRFinderCB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:264\u001b[39m, in \u001b[36mLearner.fit\u001b[39m\u001b[34m(self, n_epochs, train, valid, cbs, lr)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lr = \u001b[38;5;28mself\u001b[39m.lr\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_func: \u001b[38;5;28mself\u001b[39m.opt = \u001b[38;5;28mself\u001b[39m.opt_func(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m cbs: \u001b[38;5;28mself\u001b[39m.cbs.remove(cb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:249\u001b[39m, in \u001b[36mLearner._fit\u001b[39m\u001b[34m(self, train, valid)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train, valid):\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_dl = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_sz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.train_dl = CycleDL(\u001b[38;5;28mself\u001b[39m.train_dl, \u001b[38;5;28mself\u001b[39m.epoch_sz)\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epochs:\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'train'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGhCAYAAACd/5VtAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG01JREFUeJzt3X+sV/Vh//HXvSDQVe9l/PDeXbyMdiEFq4MEvJdrltiVm91ZM0eLGSWuUkZqtijT4tqCtZAtXcjWuFonLTHL0hlLJLjNrMTR0aurTbgVBNsNK6RLOqGQe5E57sXrvCD3fv9o/DT3KyBYPpfLm8cjOSH3fN7nc97vK8l5cj4/rBkaGhoKAEAhai/2BAAALiRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFGXuxJ3AxDA4O5vDhw7nqqqtSU1NzsacDAJyDoaGhHD9+PE1NTamtPfP9mcsybg4fPpzm5uaLPQ0A4D04ePBgrrnmmjM+flnGzVVXXZXk57+curq6izwbAOBc9PX1pbm5uXIdP5PLMm7efimqrq5O3ADAJebd3lLiDcUAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEUZkbjZsGFDZsyYkQkTJqS1tTU7d+486/gtW7Zk1qxZmTBhQq6//vo8/fTTZxz7x3/8x6mpqclDDz10gWcNAFyKqh43mzdvzqpVq7Ju3brs2bMnc+bMSUdHR44cOXLa8Tt27MjSpUuzYsWKvPjii1m0aFEWLVqUvXv3vmPsP//zP+cHP/hBmpqaqr0MAOASUTM0NDRUzRO0trbmhhtuyCOPPJIkGRwcTHNzc1auXJnVq1e/Y/ySJUvS39+frVu3VvYtWLAgc+fOzcaNGyv7Dh06lNbW1nznO9/JLbfcknvvvTf33nvvaecwMDCQgYGBys99fX1pbm5Ob29v6urqLtBKAYBq6uvrS319/btev6t65+bEiRPZvXt32tvbf3HC2tq0t7enq6vrtMd0dXUNG58kHR0dw8YPDg7mU5/6VD73uc/lwx/+8LvOY/369amvr69szc3N73FFAMBoV9W4OXr0aE6dOpWGhoZh+xsaGtLd3X3aY7q7u991/F/91V9l7Nix+dM//dNzmseaNWvS29tb2Q4ePHieKwEALhVjL/YEztfu3bvzta99LXv27ElNTc05HTN+/PiMHz++yjMDAEaDqt65mTJlSsaMGZOenp5h+3t6etLY2HjaYxobG886/vvf/36OHDmS6dOnZ+zYsRk7dmxeeeWV3HfffZkxY0ZV1gEAXDqqGjfjxo3LvHnz0tnZWdk3ODiYzs7OtLW1nfaYtra2YeOTZPv27ZXxn/rUp/If//Ef+eEPf1jZmpqa8rnPfS7f+c53qrcYAOCSUPWXpVatWpVly5Zl/vz5aWlpyUMPPZT+/v4sX748SXLHHXdk2rRpWb9+fZLknnvuyU033ZQHH3wwt9xyS5544om88MILefTRR5MkkydPzuTJk4ed44orrkhjY2M+9KEPVXs5AMAoV/W4WbJkSV599dWsXbs23d3dmTt3brZt21Z50/CBAwdSW/uLG0g33nhjNm3alAceeCD3339/Zs6cmaeeeirXXXddtacKABSg6t9zMxqd6+fkAYDRY1R8zw0AwEgTNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRlROJmw4YNmTFjRiZMmJDW1tbs3LnzrOO3bNmSWbNmZcKECbn++uvz9NNPVx47efJkvvCFL+T666/P+9///jQ1NeWOO+7I4cOHq70MAOASUPW42bx5c1atWpV169Zlz549mTNnTjo6OnLkyJHTjt+xY0eWLl2aFStW5MUXX8yiRYuyaNGi7N27N0nyxhtvZM+ePfnSl76UPXv25J/+6Z+yf//+3HrrrdVeCgBwCagZGhoaquYJWltbc8MNN+SRRx5JkgwODqa5uTkrV67M6tWr3zF+yZIl6e/vz9atWyv7FixYkLlz52bjxo2nPceuXbvS0tKSV155JdOnT3/XOfX19aW+vj69vb2pq6t7jysDAEbSuV6/q3rn5sSJE9m9e3fa29t/ccLa2rS3t6erq+u0x3R1dQ0bnyQdHR1nHJ8kvb29qampycSJE0/7+MDAQPr6+oZtAECZqho3R48ezalTp9LQ0DBsf0NDQ7q7u097THd393mNf/PNN/OFL3whS5cuPWPFrV+/PvX19ZWtubn5PawGALgUXNKfljp58mT+4A/+IENDQ/nGN75xxnFr1qxJb29vZTt48OAIzhIAGEljq/nkU6ZMyZgxY9LT0zNsf09PTxobG097TGNj4zmNfztsXnnllTzzzDNnfe1t/PjxGT9+/HtcBQBwKanqnZtx48Zl3rx56ezsrOwbHBxMZ2dn2traTntMW1vbsPFJsn379mHj3w6bn/zkJ/nud7+byZMnV2cBAMAlp6p3bpJk1apVWbZsWebPn5+WlpY89NBD6e/vz/Lly5Mkd9xxR6ZNm5b169cnSe65557cdNNNefDBB3PLLbfkiSeeyAsvvJBHH300yc/D5rbbbsuePXuydevWnDp1qvJ+nEmTJmXcuHHVXhIAMIpVPW6WLFmSV199NWvXrk13d3fmzp2bbdu2Vd40fODAgdTW/uIG0o033phNmzblgQceyP3335+ZM2fmqaeeynXXXZckOXToUP7lX/4lSTJ37txh53r22WfzkY98pNpLAgBGsap/z81o5HtuAODSMyq+5wYAYKSJGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoyInGzYcOGzJgxIxMmTEhra2t27tx51vFbtmzJrFmzMmHChFx//fV5+umnhz0+NDSUtWvX5td+7dfyvve9L+3t7fnJT35SzSUAAJeIqsfN5s2bs2rVqqxbty579uzJnDlz0tHRkSNHjpx2/I4dO7J06dKsWLEiL774YhYtWpRFixZl7969lTF//dd/nYcffjgbN27M888/n/e///3p6OjIm2++We3lAACjXM3Q0NBQNU/Q2tqaG264IY888kiSZHBwMM3NzVm5cmVWr179jvFLlixJf39/tm7dWtm3YMGCzJ07Nxs3bszQ0FCamppy33335c/+7M+SJL29vWloaMg3v/nNfPKTn3zXOfX19aW+vj69vb2pq6u7QCsFAKrpXK/fVb1zc+LEiezevTvt7e2/OGFtbdrb29PV1XXaY7q6uoaNT5KOjo7K+J/+9Kfp7u4eNqa+vj6tra1nfM6BgYH09fUN2wCAMlU1bo4ePZpTp06loaFh2P6GhoZ0d3ef9pju7u6zjn/7z/N5zvXr16e+vr6yNTc3v6f1AACj32Xxaak1a9akt7e3sh08ePBiTwkAqJKqxs2UKVMyZsyY9PT0DNvf09OTxsbG0x7T2Nh41vFv/3k+zzl+/PjU1dUN2wCAMlU1bsaNG5d58+als7Ozsm9wcDCdnZ1pa2s77TFtbW3DxifJ9u3bK+M/8IEPpLGxcdiYvr6+PP/882d8TgDg8jG22idYtWpVli1blvnz56elpSUPPfRQ+vv7s3z58iTJHXfckWnTpmX9+vVJknvuuSc33XRTHnzwwdxyyy154okn8sILL+TRRx9NktTU1OTee+/Nl7/85cycOTMf+MAH8qUvfSlNTU1ZtGhRtZcDAIxyVY+bJUuW5NVXX83atWvT3d2duXPnZtu2bZU3BB84cCC1tb+4gXTjjTdm06ZNeeCBB3L//fdn5syZeeqpp3LddddVxnz+859Pf39/7rzzzhw7diy/9Vu/lW3btmXChAnVXg4AMMpV/XtuRiPfcwMAl55R8T03AAAjTdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFEXcAABFETcAQFHEDQBQFHEDABSlanHz2muv5fbbb09dXV0mTpyYFStW5PXXXz/rMW+++WbuuuuuTJ48OVdeeWUWL16cnp6eyuM/+tGPsnTp0jQ3N+d973tfZs+ena997WvVWgIAcAmqWtzcfvvteemll7J9+/Zs3bo1zz33XO68886zHvPZz3423/72t7Nly5Z873vfy+HDh/OJT3yi8vju3btz9dVX5/HHH89LL72UL37xi1mzZk0eeeSRai0DALjE1AwNDQ1d6Cd9+eWXc+2112bXrl2ZP39+kmTbtm352Mc+lp/97Gdpamp6xzG9vb2ZOnVqNm3alNtuuy1Jsm/fvsyePTtdXV1ZsGDBac9111135eWXX84zzzxzxvkMDAxkYGCg8nNfX1+am5vT29uburq6X2apAMAI6evrS319/btev6ty56arqysTJ06shE2StLe3p7a2Ns8///xpj9m9e3dOnjyZ9vb2yr5Zs2Zl+vTp6erqOuO5ent7M2nSpLPOZ/369amvr69szc3N57kiAOBSUZW46e7uztVXXz1s39ixYzNp0qR0d3ef8Zhx48Zl4sSJw/Y3NDSc8ZgdO3Zk8+bN7/py15o1a9Lb21vZDh48eO6LAQAuKecVN6tXr05NTc1Zt3379lVrrsPs3bs3v//7v59169bld37nd846dvz48amrqxu2AQBlGns+g++77758+tOfPuuYD37wg2lsbMyRI0eG7X/rrbfy2muvpbGx8bTHNTY25sSJEzl27Niwuzc9PT3vOObHP/5xFi5cmDvvvDMPPPDA+SwBACjcecXN1KlTM3Xq1Hcd19bWlmPHjmX37t2ZN29ekuSZZ57J4OBgWltbT3vMvHnzcsUVV6SzszOLFy9Okuzfvz8HDhxIW1tbZdxLL72Uj370o1m2bFn+8i//8nymDwBcBqryaakkufnmm9PT05ONGzfm5MmTWb58eebPn59NmzYlSQ4dOpSFCxfmscceS0tLS5LkT/7kT/L000/nm9/8Zurq6rJy5cokP39vTfLzl6I++tGPpqOjI1/5ylcq5xozZsw5RdfbzvXd1gDA6HGu1+/zunNzPr71rW/l7rvvzsKFC1NbW5vFixfn4Ycfrjx+8uTJ7N+/P2+88UZl31e/+tXK2IGBgXR0dOTrX/965fEnn3wyr776ah5//PE8/vjjlf2//uu/nv/+7/+u1lIAgEtI1e7cjGbu3ADApeeifs8NAMDFIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoStXi5rXXXsvtt9+eurq6TJw4MStWrMjrr79+1mPefPPN3HXXXZk8eXKuvPLKLF68OD09Pacd+z//8z+55pprUlNTk2PHjlVhBQDApahqcXP77bfnpZdeyvbt27N169Y899xzufPOO896zGc/+9l8+9vfzpYtW/K9730vhw8fzic+8YnTjl2xYkV+8zd/sxpTBwAuYTVDQ0NDF/pJX3755Vx77bXZtWtX5s+fnyTZtm1bPvaxj+VnP/tZmpqa3nFMb29vpk6dmk2bNuW2225Lkuzbty+zZ89OV1dXFixYUBn7jW98I5s3b87atWuzcOHC/O///m8mTpx4xvkMDAxkYGCg8nNfX1+am5vT29uburq6C7RqAKCa+vr6Ul9f/67X76rcuenq6srEiRMrYZMk7e3tqa2tzfPPP3/aY3bv3p2TJ0+mvb29sm/WrFmZPn16urq6Kvt+/OMf5y/+4i/y2GOPpbb23Ka/fv361NfXV7bm5ub3uDIAYLSrStx0d3fn6quvHrZv7NixmTRpUrq7u894zLhx495xB6ahoaFyzMDAQJYuXZqvfOUrmT59+jnPZ82aNent7a1sBw8ePL8FAQCXjPOKm9WrV6empuas2759+6o116xZsyazZ8/OH/7hH57XcePHj09dXd2wDQAo09jzGXzffffl05/+9FnHfPCDH0xjY2OOHDkybP9bb72V1157LY2Njac9rrGxMSdOnMixY8eG3b3p6empHPPMM8/kP//zP/Pkk08mSd5+u9CUKVPyxS9+MX/+539+PssBAAp0XnEzderUTJ069V3HtbW15dixY9m9e3fmzZuX5OdhMjg4mNbW1tMeM2/evFxxxRXp7OzM4sWLkyT79+/PgQMH0tbWliT5x3/8x/zf//1f5Zhdu3blj/7oj/L9738/v/Ebv3E+SwEACnVecXOuZs+end/93d/NZz7zmWzcuDEnT57M3XffnU9+8pOVT0odOnQoCxcuzGOPPZaWlpbU19dnxYoVWbVqVSZNmpS6urqsXLkybW1tlU9K/f8Bc/To0cr5zvZpKQDg8lGVuEmSb33rW7n77ruzcOHC1NbWZvHixXn44Ycrj588eTL79+/PG2+8Udn31a9+tTJ2YGAgHR0d+frXv16tKQIABarK99yMduf6OXkAYPS4qN9zAwBwsYgbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKOIGACiKuAEAiiJuAICiiBsAoCjiBgAoirgBAIoibgCAoogbAKAo4gYAKIq4AQCKIm4AgKKIGwCgKGMv9gQuhqGhoSRJX1/fRZ4JAHCu3r5uv30dP5PLMm6OHz+eJGlubr7IMwEAztfx48dTX19/xsdrht4tfwo0ODiYw4cP56qrrkpLS0t27dp1wZ67r68vzc3NOXjwYOrq6i7Y81KGG2644YL+fbvclP77u9TWN5rmezHnMlLnruZ5LuRzV/M6ODQ0lOPHj6epqSm1tWd+Z81leeemtrY211xzTZJkzJgxVYmQuro6ccM7VOvv2+Wi9N/fpba+0TTfizmXkTp3Nc9Tjeeu1nXwbHds3nbZv6H4rrvuuthT4DLi79svp/Tf36W2vtE034s5l5E6dzXPM5r+W14Il+XLUtXU19eX+vr69Pb2jpp/0QDASBkN18HL/s7NhTZ+/PisW7cu48ePv9hTAYARNxqug+7cAABFcecGACiKuAEAiiJuAICiiBsAoCjiBgAoirgZQVu3bs2HPvShzJw5M3/3d393sacDACPu4x//eH71V381t912W9XO4aPgI+Stt97Ktddem2effTb19fWZN29eduzYkcmTJ1/sqQHAiPn3f//3HD9+PP/wD/+QJ598sirncOdmhOzcuTMf/vCHM23atFx55ZW5+eab82//9m8Xe1oAMKI+8pGP5KqrrqrqOcTNOXruuefye7/3e2lqakpNTU2eeuqpd4zZsGFDZsyYkQkTJqS1tTU7d+6sPHb48OFMmzat8vO0adNy6NChkZg6AFwQv+y1cKSIm3PU39+fOXPmZMOGDad9fPPmzVm1alXWrVuXPXv2ZM6cOeno6MiRI0dGeKYAUB2XyrVQ3Jyjm2++OV/+8pfz8Y9//LSP/83f/E0+85nPZPny5bn22muzcePG/Mqv/Er+/u//PknS1NQ07E7NoUOH0tTUNCJzB4AL4Ze9Fo4UcXMBnDhxIrt37057e3tlX21tbdrb29PV1ZUkaWlpyd69e3Po0KG8/vrr+dd//dd0dHRcrCkDwAV1LtfCkTJ2RM9WqKNHj+bUqVNpaGgYtr+hoSH79u1LkowdOzYPPvhgfvu3fzuDg4P5/Oc/75NSABTjXK6FSdLe3p4f/ehH6e/vzzXXXJMtW7akra3tgs5F3IygW2+9NbfeeuvFngYAXDTf/e53q34OL0tdAFOmTMmYMWPS09MzbH9PT08aGxsv0qwAYOSMpmuhuLkAxo0bl3nz5qWzs7Oyb3BwMJ2dnRf8VhsAjEaj6VroZalz9Prrr+e//uu/Kj//9Kc/zQ9/+MNMmjQp06dPz6pVq7Js2bLMnz8/LS0teeihh9Lf35/ly5dfxFkDwIVzyVwLhzgnzz777FCSd2zLli2rjPnbv/3boenTpw+NGzduqKWlZegHP/jBxZswAFxgl8q10P9bCgAoivfcAABFETcAQFHEDQBQFHEDABRF3AAARRE3AEBRxA0AUBRxAwAURdwAAEURNwBAUcQNAFAUcQMAFOX/Af9VA20vQ1+6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = torch.optim.AdamW\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), DeviceCB(), MixedPrecision()]\n",
    "learn = Learner(model, out_batches, loss_func=loss_fn, cbs=cbs, opt_func=opt)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8295f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, epochs = 1e-4, 2\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "xtra = [BatchSchedCB(sched)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b11ddac",
   "metadata": {},
   "source": [
    "GLU for ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639085e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.436</td>\n",
       "      <td>2.730</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>1:02:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.568</td>\n",
       "      <td>1.761</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.596</td>\n",
       "      <td>1.605</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>1:06:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.599</td>\n",
       "      <td>1.592</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = torch.compile(GPTModel(cfg).to(def_device), mode=\"reduce-overhead\")\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(plot=True), DeviceCB(), MixedPrecision()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs+xtra, opt_func=opt)\n",
    "learn.fit(epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could not run or get enough food, so she couldn't even call for help!\n",
      "\n",
      "Lucy was so sad she didn't feel like when it was getting late. She regretted not getting to see the festival. Eventually, it was time to go to the festival. Lucy was sad that she had to leave.\n",
      "\n",
      "Then, Lucy had an idea. She decided to take a special nap under her favourite dress and put it in her dresser. After that, when she saw the festival, she realized that she had been weak for a moment. But, it made her feel better. She was relieved to see that all the way to the festival had paid off, but she learned that it must be okay to have an even worthier way. \n",
      "\n",
      "Lucy was happy and excited to have re\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "model.eval()\n",
    "token_ids = generate(\n",
    "    model=model.eval(),\n",
    "    idx=text_to_token_ids(\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\", tokenizer).to(def_device),\n",
    "    max_new_tokens=180,\n",
    "    context_size=cfg[\"ctx_len\"],\n",
    "    top_k=25,\n",
    "    temperature=1.1\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f52dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97909f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "1  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "2  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "3  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "4  Once upon a time, there lived a bunny in a fie...         NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('evaluation_prompts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, model, tokenizer, max_tokens=180, context_size=cfg[\"ctx_len\"], \n",
    "                top_k=25, temperature=1.3):\n",
    "    # Tokenize the prompt\n",
    "    toks = text_to_token_ids(row['prompt'], tokenizer)\n",
    "    \n",
    "    # Generate completion\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=toks.to(def_device),\n",
    "        max_new_tokens=max_tokens,\n",
    "        context_size=context_size,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract only the generated part (not the original prompt)\n",
    "    completion = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['completion'] = df.apply(lambda row: process_row(row, model, tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b414bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"0401_init.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12510bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not move.\\n\\nLucy saw that Lucy was tired and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not breathe.\\n\\nLuckily, Lucy heard a loud ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>not find her way back home. Lucy's mommy was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>barely breathe again!\\n\\nLucy knew her friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>hard out.\\n\\nAt home, Lucy's mom gave her a w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   not move.\\n\\nLucy saw that Lucy was tired and...  \n",
       "1   not breathe.\\n\\nLuckily, Lucy heard a loud ye...  \n",
       "2   not find her way back home. Lucy's mommy was ...  \n",
       "3   barely breathe again!\\n\\nLucy knew her friend...  \n",
       "4   hard out.\\n\\nAt home, Lucy's mom gave her a w...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"0401_init.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. minbpe\n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019726f",
   "metadata": {},
   "source": [
    "Use triton cross entropy loss or compile nn.crosstropyloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a80a",
   "metadata": {},
   "source": [
    "Add view(-1,...) before flash attention and remove view(-1,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eba64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
