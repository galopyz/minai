{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 onwards Answer.AI, LightOn, and contributors\n",
    "# License: Apache-2.0\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from typing import Generic, Iterable, NamedTuple, Optional, TypeVar, Any, Union, Sequence\n",
    "from composer.core.types import Batch\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from numba import njit\n",
    "\n",
    "\n",
    "import math\n",
    "from composer.core import Time\n",
    "\n",
    "\n",
    "class BatchSizeWarmupScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_batch_size: int,\n",
    "        max_batch_size: int,\n",
    "        warmup_tokens: Union[str, Time, int],\n",
    "        world_size: int,\n",
    "    ):\n",
    "        self.min_batch_size = min_batch_size\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "        if isinstance(warmup_tokens, str):\n",
    "            self.warmup_tokens = Time.from_timestring(warmup_tokens).value\n",
    "        elif isinstance(warmup_tokens, Time):\n",
    "            self.warmup_tokens = warmup_tokens.value\n",
    "        else:\n",
    "            self.warmup_tokens = warmup_tokens\n",
    "        self.warmup_tokens = math.ceil(self.warmup_tokens / world_size)\n",
    "        self._step_thresholds = self._calculate_step_thresholds()\n",
    "\n",
    "    def _calculate_step_thresholds(self):\n",
    "        total_batch_sizes = sum(range(self.min_batch_size, self.max_batch_size))\n",
    "        steps_per_unit = self.warmup_tokens / total_batch_sizes\n",
    "\n",
    "        thresholds = []\n",
    "        cumsum = 0\n",
    "        for batch_size in range(self.min_batch_size, self.max_batch_size):\n",
    "            cumsum += batch_size\n",
    "            steps = math.ceil(steps_per_unit * cumsum)\n",
    "            thresholds.append(steps)\n",
    "        return thresholds\n",
    "\n",
    "    def __call__(self, current_step: int) -> int:\n",
    "        if current_step >= self.warmup_tokens:\n",
    "            return self.max_batch_size\n",
    "\n",
    "        for i, threshold in enumerate(self._step_thresholds):\n",
    "            if current_step < threshold:\n",
    "                return self.min_batch_size + i\n",
    "\n",
    "        # should never hit this, but just in case\n",
    "        return self.max_batch_size\n",
    "\n",
    "\n",
    "class SequencePackerBatchOutputTuple(NamedTuple):\n",
    "    masked_pseqs: torch.Tensor\n",
    "    labels: Optional[torch.Tensor]\n",
    "    cu_seq_lens: list[torch.Tensor]\n",
    "    max_cu_seq_len: list[torch.Tensor]\n",
    "\n",
    "\n",
    "class SequencePacker(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # params defining the incoming batches of seqs\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        src_batch_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        # params defining outgoing batches of pseqs\n",
    "        out_batch_size: int,\n",
    "        out_pseq_len: int,\n",
    "        # params defining internal behavior\n",
    "        buffer_size: int,\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        seed=42,\n",
    "        suppress_masking: bool = False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes batches of unpacked, unpadded sequences (seqs) to batches of packed and padded sequences (pseqs).\n",
    "\n",
    "        Every input batch must be a list[list[int]], a list of variable-length sequences of tokens.\n",
    "\n",
    "        Every output batch is a tuple (masked_inputs:Tensor, labels:Tensor, seq_starts_and_end:list).\n",
    "\n",
    "        It performs this streamwise, taking an iterable as the source of incoming batches, and\n",
    "        presents itself as an iterable of outgoing batches.\n",
    "\n",
    "        Args:\n",
    "            src_iterable: An iterable (e.g., a DataLoader), whose iterator yields one incoming batch,\n",
    "                        where a batch is a list of unpadded, variable-length Sequences of token\n",
    "                        IDs. Since this only needs to be an Iterable, it could also be a generator object\n",
    "                         like the result of `itertools.batched(dataset_list,batch_size))`\n",
    "\n",
    "            src_batch_size:  This is the INCOMING batch size, the number of seqs in one batch yielded\n",
    "                          from `src_iterable`'s iterator.\n",
    "\n",
    "            src_max_seq_len: The maximum number of tokens in a seq within an incoming batch.\n",
    "\n",
    "            out_batch_size: the number of pseqs (packed seqs) in one outgoing batch\n",
    "\n",
    "            out_pseq_len: the number of tokens per packed seq, in every outgoing batch\n",
    "\n",
    "            buffer_size: The maximum number of seqs which may be buffered internally.\n",
    "\n",
    "            pad_token_id: The token ID used for padding the space which cannot be filled to reach out_pseq_len.\n",
    "\n",
    "            mask_token_id: The token ID used for masking tokens in the input sequence.\n",
    "\n",
    "            ignore_token_id: The token ID used to ignore tokens. Expected to be applied to every non-masked token, so the model only trains on predictions of masked tokens.\n",
    "\n",
    "            suppress_masking: If True, the sequence packer will not perform masked language modeling.\n",
    "\n",
    "            batch_size_warmup_min_size: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "                                    batch_size_warmup_min_size must be a multiple of micro_batch_size.\n",
    "\n",
    "            batch_size_warmup_tokens: If not None, the sequence packer will gradually increase the batch size from batch_size_warmup_min_size to out_batch_size over the course of the warmup_tokens.\n",
    "\n",
    "            world_size: The number of processes participating in this training run. batch_size_warmup_min_size is divided by this number.\n",
    "        \"\"\"\n",
    "        assert buffer_size >= out_batch_size, f\"required that {buffer_size=} >= {out_batch_size=}\"\n",
    "        self.src_dataloader_len = len(src_iterable)\n",
    "        self.src_iterable = src_iterable\n",
    "        self.src_batch_size = src_batch_size\n",
    "        self.out_batch_size = out_batch_size\n",
    "        self.out_pseq_len = out_pseq_len\n",
    "        self.buffer_size = buffer_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.mask_token_id = mask_token_id\n",
    "        self.ignore_token_id = ignore_token_id\n",
    "        self.mask_prob = mask_prob\n",
    "        self.suppress_masking = suppress_masking\n",
    "        # internals\n",
    "        self.buffer = deque()  # internal buffer holds individual seqs, as tensors.\n",
    "        # for stats to report packing efficiency.\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        # Set random seed\n",
    "        self.seed = seed\n",
    "        self.epoch = -1\n",
    "        self._token_count = 0\n",
    "        self.batch_size_scheduler = None\n",
    "        if batch_size_warmup_min_size is not None and batch_size_warmup_tokens is not None:\n",
    "            self.batch_size_scheduler = BatchSizeWarmupScheduler(\n",
    "                batch_size_warmup_min_size, out_batch_size, batch_size_warmup_tokens, world_size\n",
    "            )\n",
    "        else:\n",
    "            self.batch_size_scheduler = None\n",
    "\n",
    "    @property\n",
    "    def seqs_emitted(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been emitted in OUTGOING batches.\"\n",
    "        return self._seqs_emitted\n",
    "\n",
    "    @property\n",
    "    def seqs_consumed(self):\n",
    "        \"Number of seqs, incoming from src_iterable, which have been consumed.\"\n",
    "        return self._seqs_consumed\n",
    "\n",
    "    def _reset_state(self):\n",
    "        self.epoch += 1\n",
    "        self.buffer.clear()\n",
    "        self._seqs_consumed = 0\n",
    "        self._seqs_emitted = 0\n",
    "        self.np_rng = np.random.default_rng(self.epoch + self.seed)\n",
    "\n",
    "        # Update the epoch for the sampler\n",
    "        if isinstance(self.src_iterable, torch.utils.data.dataloader.DataLoader):\n",
    "            if isinstance(self.src_iterable.sampler, torch.utils.data.distributed.DistributedSampler):\n",
    "                self.src_iterable.sampler.set_epoch(self.epoch)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._reset_state()\n",
    "        self.src_iterator = iter(self.src_iterable)\n",
    "        return self._generate_batches()\n",
    "\n",
    "    def __len__(self):\n",
    "        # rather than estimate the packed length of the dataset, we rely on Composer's ability\n",
    "        # to schedule training the using the number of batches or tokens instead of epochs.\n",
    "        return None\n",
    "\n",
    "    def _fill_buffer(self, max_items_to_add=float(\"inf\")) -> int:\n",
    "        \"\"\"\n",
    "        Refills the internal buffer.\n",
    "\n",
    "        - max_items_to_add: an amount less than or equal to the number of items to add\n",
    "\n",
    "        Returns: the number of items actually added.\n",
    "\n",
    "        The default implementation of this simply extends to src.buffer, which is\n",
    "        initialized as a list in __init__. Subclasses which want to use a different data\n",
    "        structure for internal buffering should override this method and also add\n",
    "        code in __init__ to initialize src.buffer appropriately.\n",
    "\n",
    "        Any implementation of this MUST never place more than self.buffer_size items\n",
    "        in the internal buffer.\n",
    "        \"\"\"\n",
    "        items_added = 0\n",
    "        # NOTE: this should be >=, kept as is to match model training code\n",
    "        # TODO: change if training a new model\n",
    "        while (self.buffer_size - len(self.buffer)) > self.src_batch_size:\n",
    "            try:\n",
    "                # if pulling another batch would fetch more than the requested max, stop\n",
    "                if max_items_to_add < float(\"inf\"):\n",
    "                    if (items_added + self.src_batch_size) > max_items_to_add:\n",
    "                        # print(\"Not adding, because of max_items_to_fetch\")\n",
    "                        break\n",
    "                incoming_batch = next(self.src_iterator)\n",
    "                assert (\n",
    "                    len(incoming_batch) <= self.src_batch_size\n",
    "                ), f\"expected {len(incoming_batch)=} <= {self.src_batch_size=}\"\n",
    "                for item in incoming_batch:\n",
    "                    if len(item[\"input_ids\"]) > 0:  # ignore empty sequences\n",
    "                        self.buffer.append(item[\"input_ids\"])\n",
    "                        items_added += 1\n",
    "                        self._seqs_consumed += 1\n",
    "            except StopIteration:\n",
    "                break\n",
    "        return items_added\n",
    "\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": None,\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            # # assert isinstance(yieldval[0], torch.Tensor), f\"Unexpected {type(yieldval[0])=}\"\n",
    "            # if not self.suppress_masking:\n",
    "            #     assert isinstance(yieldval[1], torch.Tensor), f\"Unexpected {type(yieldval[1])=}\"\n",
    "            # assert isinstance(yieldval[2], list), f\"Unexpected {type(yieldval[2])=}\"\n",
    "            # if yieldval[2]:\n",
    "            #     assert isinstance(yieldval[2][0], torch.Tensor), f\"Unexpected {type(yieldval[2][0])=}\"\n",
    "            yield yieldval\n",
    "\n",
    "    @staticmethod\n",
    "    def mlm_masking(\n",
    "        seq: np.ndarray,\n",
    "        mask_prob: float,\n",
    "        mask_token: int,\n",
    "        pad_token: int = -1,\n",
    "        ignore_index: int = -100,\n",
    "        np_rng=np.random.default_rng(),\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "\n",
    "        This is exactly a numpy version of transformers' `DataCollatorForLanguageModeling.torch_mask_tokens`\n",
    "        https://github.com/huggingface/transformers/blob/main/src/transformers/data/data_collator.py#L827\n",
    "\n",
    "        It performs masking in a way that produces on expectation the following masked inputs:\n",
    "         - (1-mask_prob) of the original positions will be untouched.\n",
    "         - mask_prob * 80%  of the original positions get replaced with a mask token\n",
    "         - mask_prob * 10%  of the original positions get replaced with a random token\n",
    "         - mask_prob * 10%  of the original positions also remain untouched.\n",
    "        This generates the masked_inputs.\n",
    "\n",
    "        It also generates a labels array, which has ignore tokens in the (1-mask_prob) positions\n",
    "\n",
    "        These proportions are expectation values since the random transformation is performed\n",
    "        independently per element. (This is why it is agnostic wrt shape.)\n",
    "\n",
    "        Args:\n",
    "          seq (np.ndarray): the input token IDs (e.g., a sequence, or batch of seqs)\n",
    "          mask_prob (float): probability of initially masking a token, in the first \"wave\" of masking\n",
    "          mask_token (int): token to use for masking\n",
    "          ignore_index (int): the token indicating that position should be ignored during training. We call it `ignore_index` to conform to the API of the cross entropy loss function.\n",
    "\n",
    "        Returns:\n",
    "            tuple[np.array,np.array]: (masked_seq, labels)\n",
    "                masked_seq: the input seq with some tokens replaced by `mask_token`\n",
    "                labels: the original input seq with non-masked tokens replaced by `ignore_index`\n",
    "        \"\"\"\n",
    "        # Create labels\n",
    "        labels = np.where(seq == pad_token, ignore_index, seq)\n",
    "\n",
    "        # Create a single mask\n",
    "        rand = np_rng.random(seq.shape)\n",
    "\n",
    "        # Partition the probability space appropriately using a single mask\n",
    "        # 80% of the time, we mask the token\n",
    "        mask_mask = rand < mask_prob * 0.8\n",
    "        # 10% of the time, we replace the token with a random token\n",
    "        random_mask = (rand >= mask_prob * 0.8) & (rand < mask_prob * 0.9)\n",
    "        # 10% of the time, we keep the token the same\n",
    "        keep_mask = (rand >= mask_prob * 0.9) & (rand < mask_prob)\n",
    "\n",
    "        # We only compute loss over the tokens marked for masking\n",
    "        labels = np.where(mask_mask | random_mask | keep_mask, labels, ignore_index)\n",
    "\n",
    "        # Apply masking\n",
    "        seq = np.where(mask_mask, mask_token, seq)\n",
    "\n",
    "        # Apply random replacement\n",
    "        random_words = np_rng.integers(0, np.max(seq) + 1, size=seq.shape)\n",
    "        seq = np.where(random_mask, random_words, seq)\n",
    "\n",
    "        return seq, labels\n",
    "\n",
    "    @abstractmethod\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        \"\"\"\n",
    "        Returns a batch of packed sequences with its cumulative seq length information.\n",
    "\n",
    "        Or else, returns None if it cannot build a full outgoing batch.\n",
    "\n",
    "        Must mutate self.buffer to remove the sequences that are packed into the batch.\n",
    "\n",
    "        Returns:\n",
    "            (out_batch,cumulative_seq_len):tuple[torch.tensor, list[list[int]]]\n",
    "            where:\n",
    "                - out_batch is a tensor of shape (out_batch_size, out_pseq_len);\n",
    "                - cum_seq_lens is a list of lists, where the outer list is of len out_batch_size,\n",
    "                    and each inner list is of varying length, and contains the start positions of\n",
    "                    every seq in the pseq, and the end position of the last seq in the pseq. This end\n",
    "                    position is necessary to communicate if any padding tokens were added.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "@njit\n",
    "def find_best_fit(remaining_spaces, seq_len):\n",
    "    valid_spaces = seq_len <= remaining_spaces\n",
    "    if np.any(valid_spaces):\n",
    "        valid_space_sizes = remaining_spaces[valid_spaces]\n",
    "        best_fit_idx = np.argmin(valid_space_sizes)\n",
    "        return np.arange(len(remaining_spaces))[valid_spaces][best_fit_idx]\n",
    "    return -1\n",
    "\n",
    "\n",
    "class GreedyBestFitSequencePacker(SequencePacker):\n",
    "    @classmethod\n",
    "    def from_composer(\n",
    "        cls,\n",
    "        src_iterable: Iterable[list[list[int]]],\n",
    "        batch_size: int = 512,\n",
    "        micro_batch_size: int = 32,\n",
    "        max_seq_len: int = 1024,\n",
    "        buffer_size: int = 5120,\n",
    "        # token values\n",
    "        pad_token_id: int = -1,\n",
    "        mask_token_id: int = 0,\n",
    "        ignore_token_id: int = -100,\n",
    "        mask_prob: float = 0.3,\n",
    "        # transform values\n",
    "        seed=42,\n",
    "        suppress_masking=False,\n",
    "        batch_size_warmup_min_size: Optional[int] = None,\n",
    "        batch_size_warmup_tokens: Optional[Union[str, Time]] = None,\n",
    "        world_size: int = 1,\n",
    "    ) -> \"GreedyBestFitSequencePacker\":\n",
    "        if batch_size_warmup_min_size is not None:\n",
    "            if batch_size_warmup_min_size % micro_batch_size != 0:\n",
    "                raise ValueError(f\"{batch_size_warmup_min_size=} must be a multiple of {micro_batch_size=}\")\n",
    "            batch_size_warmup_min_size = int(batch_size_warmup_min_size / micro_batch_size)\n",
    "        return cls(\n",
    "            # input shape\n",
    "            src_iterable=src_iterable,\n",
    "            src_batch_size=batch_size,\n",
    "            src_max_seq_len=max_seq_len,\n",
    "            # output shape\n",
    "            out_batch_size=int(batch_size / micro_batch_size),\n",
    "            out_pseq_len=int(micro_batch_size * max_seq_len),\n",
    "            # internal\n",
    "            buffer_size=buffer_size,\n",
    "            # transformation\n",
    "            pad_token_id=pad_token_id,\n",
    "            mask_token_id=mask_token_id,\n",
    "            ignore_token_id=ignore_token_id,\n",
    "            mask_prob=mask_prob,\n",
    "            seed=seed,\n",
    "            suppress_masking=suppress_masking,\n",
    "            batch_size_warmup_min_size=batch_size_warmup_min_size,\n",
    "            batch_size_warmup_tokens=batch_size_warmup_tokens,\n",
    "            world_size=world_size,\n",
    "        )\n",
    "\n",
    "    def _create_batch(self) -> Optional[tuple[np.ndarray, list[list[int]]]]:\n",
    "        if self.batch_size_scheduler:\n",
    "            self.out_batch_size = self.batch_size_scheduler(self._token_count)\n",
    "\n",
    "        batch = np.full(\n",
    "            (self.out_batch_size, self.out_pseq_len), self.pad_token_id, dtype=np.int64\n",
    "        )  # the pseqs being constructed\n",
    "        seq_counts = np.zeros(self.out_batch_size, dtype=np.int32)  # the count of seqs per pseq\n",
    "        cum_seq_lens = [[0] for _ in range(self.out_batch_size)]\n",
    "        remaining_spaces = np.full(\n",
    "            (self.out_batch_size,), self.out_pseq_len, dtype=np.int32\n",
    "        )  # the space remaining per pseq\n",
    "        temp_buffer = []\n",
    "\n",
    "        while True:\n",
    "            # Check if buffer has more items, and if not replenish\n",
    "            if not self.buffer:\n",
    "                items_to_fetch = self.buffer_size - len(temp_buffer)\n",
    "                items_added = self._fill_buffer(items_to_fetch)\n",
    "                if items_added == 0:\n",
    "                    break\n",
    "\n",
    "            seq = self.buffer.popleft()\n",
    "            seq_len = len(seq)\n",
    "\n",
    "            # Find the best fit (smallest space that can accommodate the sequence)\n",
    "            best_fit_idx = find_best_fit(remaining_spaces, seq_len)\n",
    "            if best_fit_idx != -1:\n",
    "                end_pos = self.out_pseq_len - remaining_spaces[best_fit_idx]\n",
    "                batch[best_fit_idx, end_pos : end_pos + seq_len] = seq\n",
    "                seq_counts[best_fit_idx] += 1\n",
    "                remaining_spaces[best_fit_idx] -= seq_len\n",
    "                cum_seq_lens[best_fit_idx].append(cum_seq_lens[best_fit_idx][-1] + seq_len)\n",
    "            else:\n",
    "                # Can't fit the sequence, save for next batch\n",
    "                temp_buffer.append(seq)\n",
    "\n",
    "        # Add any sequences we skipped back to the start of the buffer\n",
    "        self.buffer.extendleft(temp_buffer)\n",
    "\n",
    "        if np.all(seq_counts > 0):\n",
    "            self._seqs_emitted += np.sum(seq_counts)\n",
    "            for x in cum_seq_lens:\n",
    "                if x[-1] != self.out_pseq_len:\n",
    "                    x.append(self.out_pseq_len)\n",
    "            return batch, cum_seq_lens\n",
    "        else:\n",
    "            # If we can't form a full batch, we return None to signal the end\n",
    "            return None\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class BufferedIterable(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          - iterable: an object which generates a fresh iterator on iter() and which implements len()\n",
    "        \"\"\"\n",
    "        self.iterable = iterable\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return BufferedIterator(self.iterable, self.buffer_size)\n",
    "\n",
    "\n",
    "class BufferedIterator(Generic[T]):\n",
    "    def __init__(self, iterable: Iterable[T], buffer_size: int):\n",
    "        self.iterator = iter(iterable)\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.lock = threading.Lock()\n",
    "        self.exhausted = False\n",
    "        self.filler_thread = threading.Thread(target=self._background_fill, daemon=True)\n",
    "        self.filler_thread.start()\n",
    "\n",
    "    def _background_fill(self):\n",
    "        # Fill up the buffer, whenever possible, in the background\n",
    "        while not self.exhausted:\n",
    "            if len(self.buffer) < self.buffer_size:\n",
    "                try:\n",
    "                    item = next(self.iterator)\n",
    "                    with self.lock:\n",
    "                        self.buffer.append(item)\n",
    "                except StopIteration:\n",
    "                    self.exhausted = True\n",
    "                    break\n",
    "            else:\n",
    "                time.sleep(0.01)  # Sleep for a bit to avoid busy waiting\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> T:\n",
    "        while True:\n",
    "            if not self.buffer:\n",
    "                if self.exhausted:\n",
    "                    # We've exhausted the iterator and the buffer so we're done\n",
    "                    raise StopIteration\n",
    "                else:\n",
    "                    # The buffer is empty but the iterator is not exhausted yet.\n",
    "                    # Let's give the filler thread a chance to add items to the buffer\n",
    "                    time.sleep(0.01)\n",
    "            else:\n",
    "                with self.lock:\n",
    "                    return self.buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb01eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, itertools\n",
    "max_seq_len = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1e39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "  Input shape: torch.Size([8, 512])\n",
      "  Labels shape: torch.Size([8, 512])\n",
      "  Number of sequences: 8\n",
      "Batch 1:\n",
      "  Input shape: torch.Size([8, 512])\n",
      "  Labels shape: torch.Size([8, 512])\n",
      "  Number of sequences: 8\n",
      "Batch 2:\n",
      "  Input shape: torch.Size([8, 512])\n",
      "  Labels shape: torch.Size([8, 512])\n",
      "  Number of sequences: 8\n"
     ]
    }
   ],
   "source": [
    "# Create a simple source of sequences\n",
    "sample_data = [[{\"input_ids\": torch.randint(0, 1000, (random.randint(10, 100),))} for _ in range(32)] for _ in range(10)]\n",
    "\n",
    "# Initialize the packer\n",
    "packer = GreedyBestFitSequencePacker.from_composer(\n",
    "    src_iterable=sample_data,\n",
    "    batch_size=32,            # Size of incoming batches\n",
    "    micro_batch_size=4,       # Output will have batch_size/micro_batch_size items\n",
    "    max_seq_len=128,          # Maximum sequence length\n",
    "    buffer_size=256,          # Buffer size for efficient packing\n",
    "    pad_token_id=0,           # Token ID for padding\n",
    "    mask_token_id=1,          # Token ID for masking (for MLM)\n",
    "    ignore_token_id=-100      # Token ID for positions to ignore\n",
    ")\n",
    "\n",
    "# Iterate through packed batches\n",
    "for batch_idx, batch in enumerate(packer):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Input shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape if batch['labels'] is not None else None}\")\n",
    "    print(f\"  Number of sequences: {len(batch['cu_seqlens'])}\")\n",
    "    \n",
    "    # Stop after a few batches for this example\n",
    "    if batch_idx >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched(iterable, n):\n",
    "    # batched('ABCDEFG', 3) → ABC DEF G\n",
    "    if n < 1:\n",
    "        raise ValueError(\"n must be at least one\")\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(itertools.islice(iterator, n)):\n",
    "        yield batch\n",
    "\n",
    "\n",
    "xs = [\n",
    "    [0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    [3, 3, 3],\n",
    "    [4, 4, 4, 4, 4, 4],\n",
    "    [5, 5, 5, 5, 5, 5],\n",
    "    [6, 6, 6, 6, 6, 6, 6],\n",
    "    [7, 7, 7],\n",
    "    [8, 8],\n",
    "    [],\n",
    "    [10, 10, 10, 10, 10, 10, 10],\n",
    "    [11, 11, 11, 11],\n",
    "    [12, 12, 12, 12],\n",
    "    [13, 13, 13, 13, 13, 13, 13, 13],\n",
    "    [14, 14, 14],\n",
    "    [15, 15, 15, 15, 15, 15, 15],\n",
    "    [16, 16, 16, 16, 16, 16, 16],\n",
    "    [17, 17, 17, 17, 17, 17, 17, 17],\n",
    "    [18],\n",
    "    [19, 19, 19, 19],\n",
    "    [20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
    "    [21, 21, 21, 21],\n",
    "    [22, 22, 22, 22, 22, 22, 22, 22],\n",
    "    [23, 23],\n",
    "    [24, 24, 24, 24, 24, 24, 24, 24, 24, 24],\n",
    "    [25, 25, 25, 25, 25, 25, 25],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92168b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch_size = 4\n",
    "xds = [{\"input_ids\": seq} for seq in xs]\n",
    "input_batches = list(batched(xds, input_batch_size))\n",
    "d = {\n",
    "    \"src_iterable\": input_batches,\n",
    "    \"src_batch_size\": input_batch_size,\n",
    "    \"src_max_seq_len\": max_seq_len,\n",
    "    \"out_batch_size\": 5,\n",
    "    \"out_pseq_len\": 15,\n",
    "    \"buffer_size\": 5,\n",
    "    \"pad_token_id\": -1,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09bf8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 0,  0,  0,  0,  1,  1,  1,  1,  1,  4,  4,  4,  4,  4,  4],\n",
       "          [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  8,  8],\n",
       "          [ 5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6, -1, -1],\n",
       "          [ 7,  7,  7, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 18],\n",
       "          [12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14]]),\n",
       "  'labels': None,\n",
       "  'cu_seqlens': [tensor([ 0,  4,  9, 15], dtype=torch.int32),\n",
       "   tensor([ 0, 10, 13, 15], dtype=torch.int32),\n",
       "   tensor([ 0,  6, 13, 15], dtype=torch.int32),\n",
       "   tensor([ 0,  3, 10, 14, 15], dtype=torch.int32),\n",
       "   tensor([ 0,  4, 12, 15], dtype=torch.int32)],\n",
       "  'max_seqlen': [6, 10, 7, 7, 8]},\n",
       " {'input_ids': tensor([[19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 17, 17, 23, 23, -1],\n",
       "          [16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, -1],\n",
       "          [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, -1],\n",
       "          [22, 22, 22, 22, 22, 22, 22, 22, 25, 25, 25, 25, 25, 25, 25],\n",
       "          [24, 24, 24, 24, 24, 24, 24, 24, 24, 24, -1, -1, -1, -1, -1]]),\n",
       "  'labels': None,\n",
       "  'cu_seqlens': [tensor([ 0,  4, 12, 14, 15], dtype=torch.int32),\n",
       "   tensor([ 0,  7, 14, 15], dtype=torch.int32),\n",
       "   tensor([ 0, 10, 14, 15], dtype=torch.int32),\n",
       "   tensor([ 0,  8, 15], dtype=torch.int32),\n",
       "   tensor([ 0, 10, 15], dtype=torch.int32)],\n",
       "  'max_seqlen': [8, 7, 10, 8, 10]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches = list(GreedyBestFitSequencePacker(**d))\n",
    "out_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"src_iterable\": input_batches,\n",
    "    \"src_batch_size\": input_batch_size,\n",
    "    \"src_max_seq_len\": max_seq_len,\n",
    "    \"out_batch_size\": 1,\n",
    "    \"out_pseq_len\": 15*5,\n",
    "    \"buffer_size\": 5,\n",
    "    \"pad_token_id\": -1,\n",
    "    \"mask_token_id\": -2,\n",
    "    \"ignore_token_id\": -3,\n",
    "    \"mask_prob\": 0.0,\n",
    "    \"seed\": 42,\n",
    "    \"suppress_masking\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568df3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 0,  0,  0,  0,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "            2,  3,  3,  3,  4,  4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  6,  6,\n",
       "            6,  6,  6,  6,  6,  7,  7,  7,  8,  8, 10, 10, 10, 10, 10, 10, 10, 11,\n",
       "           11, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14,\n",
       "           18, -1, -1]]),\n",
       "  'labels': None,\n",
       "  'cu_seqlens': [tensor([ 0,  4,  9, 19, 22, 28, 34, 41, 44, 46, 53, 57, 61, 69, 72, 73, 75],\n",
       "          dtype=torch.int32)],\n",
       "  'max_seqlen': [10]},\n",
       " {'input_ids': tensor([[19, 19, 19, 19, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16,\n",
       "           16, 15, 15, 15, 15, 15, 15, 15, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "           21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 24, 24, 24, 24,\n",
       "           24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, -1, -1, -1, -1, -1,\n",
       "           -1, -1, -1]]),\n",
       "  'labels': None,\n",
       "  'cu_seqlens': [tensor([ 0,  4, 12, 19, 26, 36, 40, 48, 50, 60, 67, 75], dtype=torch.int32)],\n",
       "  'max_seqlen': [10]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_batches = list(GreedyBestFitSequencePacker(**d))\n",
    "out_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d69d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSequencePacker(GreedyBestFitSequencePacker):\n",
    "    def _generate_batches(self):\n",
    "        \"\"\"\n",
    "        Generates batches of packed sequences.\n",
    "\n",
    "        The returned generator's iterator will always, when next() is called on it, either:\n",
    "         - return a valid tuple batch (masked_batch, labels, cu_seq_lens,max_seq_lens)\n",
    "         - raise StopIteration\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            retval = self._create_batch()\n",
    "            if retval is None:\n",
    "                break\n",
    "            batch, lst_cu_seq_lens = retval\n",
    "\n",
    "            assert isinstance(retval, tuple), f\"Unexpected {type(retval)=}\"\n",
    "            assert isinstance(retval[0], np.ndarray), f\"Unexpected {type(retval[0])=}\"\n",
    "            assert isinstance(retval[1], list), f\"Unexpected {type(retval[1])=}\"\n",
    "\n",
    "            cu_seq_lens = [torch.tensor(x, dtype=torch.int32) for x in lst_cu_seq_lens]\n",
    "            max_seq_lens = [torch.max(x[1:] - x[:-1]).item() for x in cu_seq_lens]\n",
    "            assert isinstance(cu_seq_lens, list), f\"Unexpected {type(cu_seq_lens)=}\"\n",
    "            if self.suppress_masking:\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(batch),\n",
    "                    \"labels\": torch.from_numpy(batch[:, 1:]).append(self.pad_token_id),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                }\n",
    "            else:\n",
    "                (masked_batch, labels) = SequencePacker.mlm_masking(\n",
    "                    batch, self.mask_prob, self.mask_token_id, self.pad_token_id, self.ignore_token_id, self.np_rng\n",
    "                )\n",
    "                yieldval = {\n",
    "                    \"input_ids\": torch.from_numpy(masked_batch),\n",
    "                    \"labels\": torch.from_numpy(labels),\n",
    "                    \"cu_seqlens\": cu_seq_lens,\n",
    "                    \"max_seqlen\": max_seq_lens,\n",
    "                    \"attention_mask\": torch.from_numpy(np.where(batch == self.pad_token_id, 0, 1)),\n",
    "                }\n",
    "                self._token_count += yieldval[\"attention_mask\"].sum().item()\n",
    "            yield yieldval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81acd44",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out_batches = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCausalSequencePacker\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m out_batches\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mCausalSequencePacker._generate_batches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cu_seq_lens, \u001b[38;5;28mlist\u001b[39m), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(cu_seq_lens)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_masking:\n\u001b[32m     24\u001b[39m     yieldval = {\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: torch.from_numpy(batch),\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m(\u001b[38;5;28mself\u001b[39m.pad_token_id),\n\u001b[32m     27\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcu_seqlens\u001b[39m\u001b[33m\"\u001b[39m: cu_seq_lens,\n\u001b[32m     28\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax_seqlen\u001b[39m\u001b[33m\"\u001b[39m: max_seq_lens,\n\u001b[32m     29\u001b[39m     }\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     31\u001b[39m     (masked_batch, labels) = SequencePacker.mlm_masking(\n\u001b[32m     32\u001b[39m         batch, \u001b[38;5;28mself\u001b[39m.mask_prob, \u001b[38;5;28mself\u001b[39m.mask_token_id, \u001b[38;5;28mself\u001b[39m.pad_token_id, \u001b[38;5;28mself\u001b[39m.ignore_token_id, \u001b[38;5;28mself\u001b[39m.np_rng\n\u001b[32m     33\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Tensor' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "out_batches = list(CausalSequencePacker(**d))\n",
    "out_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashCausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    Flash Attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with Flash Attention implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - Check if Flash Attention is available (FLASH_ATTN_AVAILABLE)\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, cu_seqlens: Tensor, max_seqlen: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [total_seq_len, hidden_dim].\n",
    "            cu_seqlens: Cumulative sequence lengths tensor of shape [batch_size + 1]\n",
    "                    Used instead of an attention mask for both masking and\n",
    "                    variable-length sequences. Example:\n",
    "                        cu_seqlens = torch.tensor([0, 10, 30, 60])\n",
    "                    This means there are three sequences in the batch:\n",
    "                        - First sequence has 10 tokens\n",
    "                        - Second sequence has 20 tokens\n",
    "                        - Third sequence has 30 tokens\n",
    "            max_seqlen: Maximum sequence length in the batch. In the example above,\n",
    "                        the maximum sequence length is 30.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [total_seq_len, hidden_dim] after attention.\n",
    "        \"\"\"\n",
    "        if not FLASH_ATTN_AVAILABLE:\n",
    "            raise ImportError(\"Flash Attention is not available. Please install it with `pip install flash-attn`\")\n",
    "        \n",
    "        total_seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x) # [batch_size, seq_len, d_out]\n",
    "\n",
    "        k_reshaped = k.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        q_reshaped = q.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        v_reshaped = v.view(total_seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Call Flash Attention\n",
    "        output = flash_attn_varlen_func(\n",
    "            q_reshaped,\n",
    "            k_reshaped,\n",
    "            v_reshaped,\n",
    "            cu_seqlens_q=cu_seqlens,\n",
    "            cu_seqlens_k=cu_seqlens,\n",
    "            max_seqlen_q=max_seqlen,\n",
    "            max_seqlen_k=max_seqlen,\n",
    "            causal=True\n",
    "        )\n",
    "\n",
    "        return self.dropout(self.Wo(output.reshape(total_seq_len, hidden_dim)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
