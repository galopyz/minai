{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcc2b6d",
   "metadata": {},
   "source": [
    "TODO: Clean dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371da66",
   "metadata": {},
   "source": [
    "## TinyStories Hackathon Rules\n",
    "This hackathon is intended to be a fun competition to give ourselves practice pretraining LLMs on consumer hardware. We will follow the [TinyStories paper](<https://arxiv.org/abs/2305.07759>) and train small language models on small datasets and hardware.\n",
    "\n",
    "The hackathon will end on April 7th, [AOE](<https://en.wikipedia.org/wiki/AoE>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c70b6a",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. [**TinyStories:**](<https://huggingface.co/datasets/roneneldan/TinyStories>)\n",
    "   Note that the TinyStories dataset is split into two versions both in the HF dataset:\n",
    "     - GPT-3.5 generated TinyStories\n",
    "    - GPT-4 generated TinyStories\n",
    "   The tar file appears to have the cleanest versions with the least number of duplicates.\n",
    "2. **[Simple Wikipedia](<https://huggingface.co/datasets/lsb/simplewiki2023>)** (optional)\n",
    "   This dataset can be used to give your model more world knowledge than from just the TinyStories dataset. But be careful that \n",
    "it doesn't cause your model to use words which a typical 3 to 4-year-olds doesn't understand. It may need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1528f9",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Models will be evaluated by LLM-as-a-judge following the methodology outlined in the TinyStories paper. More details including how to submit your model's outputs early next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15400",
   "metadata": {},
   "source": [
    "### Model Size Limits\n",
    "Participants will be slotted into one of the following categories based on their hardware:\n",
    "- **Small**: Up to 30M parameters. Low-to-mid range laptop GPUs and Apple Silicon.\n",
    "- **Medium**: Up to 60M parameters. Mid-range GPUs (including high-end laptop GPUs and Apple Silicon)\n",
    "- **Large**: Up to 120M parameters. High-end GPUs and multi-GPU systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1a81",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "While you must train your model from scratch, you are welcome to use any pre-trained tokenizer or train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbdaa",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "You are welcome to use any model architecture you want provided you stay within the parameter budget of your hardware by following the parameter counting rules below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadc72b",
   "metadata": {},
   "source": [
    "### Parameter Counting\n",
    "The Parameter budget is the number of unique floating-point weights receiving gradient updates:\n",
    "- Unique Weights: Count each distinct floating-point weight stored in the model once.\n",
    "- Reuse Multiplier: For each weight, multiply by the number of distinct times it contributes to forward computation (e.g., due to layer-sharing, layer reuse, or non-standard head-sharing). Weight-tied embedding and decoder weights are the exception and are only counted once. MQA/GQA doesn't count as head-sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec912d",
   "metadata": {},
   "source": [
    "### Teams\n",
    "Teams are limited to a maximum of 2 members and must be formed and declared within the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b91a3",
   "metadata": {},
   "source": [
    "### Training Frameworks\n",
    "You might want to take a look at the following libraries and frameworks and adopt one for pretraining:\n",
    "- [Composer](<https://docs.mosaicml.com/projects/composer/en/stable/index.html>) and optionally [LLM Foundry](<https://github.com/mosaicml/llm-foundry>)\n",
    "- [PyTorch Lightning](<https://lightning.ai/docs/pytorch/stable/>) and optionally [LitGPT](<https://github.com/Lightning-AI/litgpt>)\n",
    "- Hugging Face [Trainer](<https://huggingface.co/docs/transformers/en/main_classes/trainer>), [Accelerate](<https://huggingface.co/docs/accelerate/en/index>), and optionally [Axolotl](<https://axolotl-ai-cloud.github.io/axolotl/>) (a wrapper on top of HF)\n",
    "- [fastai](<https://docs.fast.ai/>) with either [fastxtend](<https://fastxtend.benjaminwarner.dev/text.huggingface.html>)/[blurr](<https://ohmeow.github.io/blurr/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc5ba1bc01a4d1ab971bf284d8468cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d6835e703584be792cdc9ad154c5ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49623110cbae488abb6cbde9e29592fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ad364266843778055dbcb425e08b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1144828db8dd4bc78fa15472aef628c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261ab9248a0a485bae04adf645fb3f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd78ea7632714b91a41c5fd994ded2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f798d21c67547baaf081c332177d828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967927c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f566a",
   "metadata": {},
   "source": [
    "For now, we can just use gpt2 tokenizer to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(txt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b7912",
   "metadata": {},
   "source": [
    "Let's encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50878a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(b):\n",
    "    b['text'] = [tokenizer.encode(o, allowed_special={'<|endoftext|>'}) for o in b['text']]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.with_transform(encode)\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2",
   "metadata": {},
   "source": [
    "Now we have numbers. We have to decode them to read text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee9952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trn[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8462f",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e776a",
   "metadata": {},
   "source": [
    "Let's try to use very small subset of data to get started. Our goal is to add `eot_token` to the end of each text. Then, chop them up into `seq_len` to create each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "chunk_sz = seq_len + 1\n",
    "eot_token = 50256\n",
    "eot_tensor = torch.tensor([eot_token])\n",
    "\n",
    "div_by = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = trn.num_rows // div_by // 3\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a17a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = [torch.tensor(o) for o in trn[:data_len]['text']]\n",
    "seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "          257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "          284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "          538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.cat([torch.cat([s, eot_tensor]) for s in seq_tensor])\n",
    "cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135959])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06b6",
   "metadata": {},
   "source": [
    "Let's create batches with `seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73653862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_complete_segments = cat.size(0) // chunk_sz\n",
    "num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddb9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([529, 257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_segments = cat[:num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "complete_segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29129328",
   "metadata": {},
   "source": [
    "> TODO\n",
    "\n",
    "Looking at the last bit, it is pretty close to a whole `seq_len`. We can pad it and use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([535])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder = cat[num_complete_segments * seq_len:]\n",
    "remainder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4a22",
   "metadata": {},
   "source": [
    "### Dataset (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7b90b",
   "metadata": {},
   "source": [
    "Let's create inputs and targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([529, 256]), torch.Size([529, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = complete_segments[:, :-1]\n",
    "targs = complete_segments[:, 1:]\n",
    "inps.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "          287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e53f6",
   "metadata": {},
   "source": [
    "We can create a dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac4a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "         17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "           284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "          2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "           673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "           198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "           366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "          2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "          1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "           356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "           198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "         19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "           407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "          1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "          1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "          1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "          1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "           257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "           284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "           538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922,\n",
       "          5252,    13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,\n",
       "            13,   198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,\n",
       "           287,   262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,\n",
       "           383,  5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,\n",
       "           538,  8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,\n",
       "           351,   606,    13,  1355,   538, 10357]),\n",
       " tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "           287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284,\n",
       "           711,   351,   340,   780,   340,   373,  7786,    13, 20037,  2227,\n",
       "           284,  2648,   262, 17598,   351,   607,  1995,    11,   523,   673,\n",
       "           714, 34249,   257,  4936,   319,   607, 10147,    13,   198,   198,\n",
       "            43,   813,  1816,   284,   607,  1995,   290,   531,    11,   366,\n",
       "         29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,  2648,\n",
       "           340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,  1995,\n",
       "         13541,   290,   531,    11,   366,  5297,    11, 20037,    11,   356,\n",
       "           460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,   198,\n",
       "           198, 41631,    11,   484,  4888,   262, 17598,   290,   384, 19103,\n",
       "           262,  4936,   319, 20037,   338, 10147,    13,   632,   373,   407,\n",
       "          2408,   329,   606,   780,   484,   547,  7373,   290,  5742,  1123,\n",
       "           584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,  1995,\n",
       "           329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,  1119,\n",
       "          1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,  1978,\n",
       "            13, 50256,  7454,  2402,   257,   640,    11,   612,   373,   257,\n",
       "          1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,   284,\n",
       "           467,  3049,   290,   711,   287,   262,  4252,    13,  1355,   538,\n",
       "           373,   257,  5448,  1097,   780,   339,  1464,   550,   922,  5252,\n",
       "            13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,    13,\n",
       "           198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,   287,\n",
       "           262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,   383,\n",
       "          5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,   538,\n",
       "          8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,   351,\n",
       "           606,    13,  1355,   538, 10357,   739]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds = Dataset(inps, targs)\n",
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68240679",
   "metadata": {},
   "source": [
    "We got the training dataset. Now, we can get the validation dataset with the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_len = val.num_rows // div_by * 10\n",
    "val_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcddecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq_tensor = [torch.tensor(o) for o in val[:val_data_len]['text']]\n",
    "val_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "         1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "         3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "         1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "         1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "          550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "        22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "           13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "        12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "          757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "          371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "        30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cat = torch.cat([torch.cat([s, eot_tensor]) for s in val_seq_tensor])\n",
    "val_cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_num_complete_segments = val_cat.size(0) // chunk_sz\n",
    "val_num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd13d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([167, 257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_complete_segments = val_cat[:val_num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "val_complete_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb46a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([167, 256]), torch.Size([167, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inps = val_complete_segments[:, :-1]\n",
    "val_targs = val_complete_segments[:, 1:]\n",
    "val_inps.shape, val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "           366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "           290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "           345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "           198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "         15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "          1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "          3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "          2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "          1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "          3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "          1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "          1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "           550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "         22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "            13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "         12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "           757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "           371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "         30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257,\n",
       "          1310,  6512,  3706, 15890,    13, 15890,  2497,   326,   371, 23536,\n",
       "           373,  6507,   290,  1965,    11,   366,  5195,   389,   345,  6507,\n",
       "            11,   371, 23536,  1701,   198,   198,    49, 23536,  1297, 15890,\n",
       "           546,   262, 30284, 12788,   290,   703,   673,  3521,   470, 12080,\n",
       "           340,    13, 15890,   531,    11,   366,    40,   423,   281,  2126,\n",
       "             0,  3914,   338,  1064,   617,  1263]),\n",
       " tensor([   13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,   366,\n",
       "         22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,   290,\n",
       "          3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,   345,\n",
       "            11, 15899,    13,   314, 25245,   340,   790,  1110,   526,   198,\n",
       "           198,  3260,  2712,   351,   262,  1097,    11, 21168,   290, 15899,\n",
       "          2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,  1598,\n",
       "          1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,  3772,\n",
       "            13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,  2460,\n",
       "            13, 50256,  7454,  2402,   257,   640,    11,   287,   257,  1263,\n",
       "          8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,  3706,\n",
       "           371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,  1375,\n",
       "         19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,  1110,\n",
       "            11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,   550,\n",
       "          1239,  1775,  1997,   588,   340,   878,    13,   632,   373, 22441,\n",
       "           290,  4692,    11,   290,   673,  2227,   284, 12080,   340,    13,\n",
       "           198,   198,    49, 23536,  3088,   284, 12080,   262, 30284, 12788,\n",
       "            11,   475,   340,   373,   845, 32911,    13,  1375,  3088,   757,\n",
       "           290,   757,    11,   475,   673,  4030,  7463,   866,    13,   371,\n",
       "         23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262, 30284,\n",
       "         12788,   523,   881,    13,  3244,    11,   673,  2497,   257,  1310,\n",
       "          6512,  3706, 15890,    13, 15890,  2497,   326,   371, 23536,   373,\n",
       "          6507,   290,  1965,    11,   366,  5195,   389,   345,  6507,    11,\n",
       "           371, 23536,  1701,   198,   198,    49, 23536,  1297, 15890,   546,\n",
       "           262, 30284, 12788,   290,   703,   673,  3521,   470, 12080,   340,\n",
       "            13, 15890,   531,    11,   366,    40,   423,   281,  2126,     0,\n",
       "          3914,   338,  1064,   617,  1263,  5667]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = Dataset(val_inps, val_targs)\n",
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757872",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409779a",
   "metadata": {},
   "source": [
    "We need a dataloader with the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646800e",
   "metadata": {},
   "source": [
    "TODO: do `drop_last=True` for training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\n",
    "dls = DataLoaders(trn_dl, val_dl)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  835,   284, 14936,  ...,   262,  6729,   287],\n",
       "         [  198,   464,  1310,  ...,   477,   262,   661],\n",
       "         [16218,   287,  2263,  ...,   766,   644,   550],\n",
       "         [ 2576,  2936,   517,  ...,  1869,   373, 14720]]),\n",
       " tensor([[  284, 14936,    30,  ...,  6729,   287,   257],\n",
       "         [  464,  1310,  2576,  ...,   262,   661,  1088],\n",
       "         [  287,  2263,  1337,  ...,   644,   550,   925],\n",
       "         [ 2936,   517,  6776,  ...,   373, 14720,   290]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3adb4",
   "metadata": {},
   "source": [
    "We make the model using transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ctx_len, n_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_head == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // n_head\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones((ctx_len, ctx_len)), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x): \n",
    "        bs, num_tokens, d_in = x.shape\n",
    "        q = self.w_q(x)  # (bs, num_tokens, d_out)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        \n",
    "        q = q.view(bs, num_tokens, self.n_head, self.head_dim)  # (bs, num_tokens, n_head, head_dim)\n",
    "        k = k.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        v = v.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1,2) # (bs, n_head, num_tokens, head_dim)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn_scr = q@k.transpose(2,3) # (bs, n_head, num_tokens, num_tokens)\n",
    "        attn_scr = attn_scr.masked_fill(self.mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_wt = torch.softmax(attn_scr / k.shape[-1]**0.5, -1)\n",
    "        \n",
    "        ctx_vec = attn_wt@v  # (bs, n_head, num_tokens, head_dim)\n",
    "        ctx_vec = ctx_vec.transpose(1,2).reshape(bs, num_tokens, -1) # (bs, num_tokens, d_out)\n",
    "        \n",
    "        # concat\n",
    "        return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0806, -0.0211,  0.1296, -0.0414],\n",
       "          [ 0.2287,  0.1592, -0.0296, -0.1997]],\n",
       " \n",
       "         [[ 0.6081,  0.0087,  0.2989, -0.4998],\n",
       "          [ 0.1026,  0.0422,  0.3105, -0.2781]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([2, 2, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=4, ctx_len=2, n_head=2)\n",
    "mha(x), mha(x).shape  # Outputs (bs, num_tokens, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2928, -0.1471,  0.0123, -0.2592], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(4)\n",
    "ff = FeedForward(4, 4*4)\n",
    "ff(x), ff(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = MultiHeadAttention(emb_dim, emb_dim, ctx_len, n_head, qkv_bias=qkv_bias)\n",
    "        self.do = nn.Dropout(drop_out)\n",
    "        self.ff = FeedForward(emb_dim, emb_dim*ff_mult)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39725b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3595, -1.0631,  0.5676],\n",
       "          [-0.3159, -2.2222,  0.1778]],\n",
       " \n",
       "         [[ 1.7169, -1.7565,  1.2721],\n",
       "          [-0.3908, -0.2502,  0.3274]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)\n",
    "tb = TransformerBlock(emb_dim=3, ctx_len=2, n_head=1)\n",
    "tb(x), tb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 48,\n",
    "    'ctx_len': seq_len,\n",
    "    'n_head': 1,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = xb[:3]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ab458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 50257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())\n",
    "total_params = get_total_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 48]), torch.Size([50257, 48]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_emb.weight.shape, model.final_l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 7,313,137\n",
      "Total size: 27.90 MB\n"
     ]
    }
   ],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
    "\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e879d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 38213, 23676,  9929, 29854,  3414, 22988]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2125798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Moves Poor overt DV announcedpeace\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816caf06",
   "metadata": {},
   "source": [
    "For convenience, we create functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16486221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 86, 562, 929, 616, 288, 707,  70]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_token_ids('wassup my dawg', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e130b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wassup my dawg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(text_to_token_ids('wassup my dawg', tokenizer), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1658bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you589bj cartoon regional Islamabad Experimental Cancer straw unin fucked'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836abf4a",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([7, 256]) torch.Size([7, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in trn_dl:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e846d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 135424\n",
      "Validation tokens: 42752\n",
      "All tokens: 178176\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in trn_dl:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dl:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "#     logits = model(input_batch)\n",
    "#     loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "#     total_loss = 0.\n",
    "#     if len(data_loader) == 0:\n",
    "#         return float(\"nan\")\n",
    "#     elif num_batches is None:\n",
    "#         num_batches = len(data_loader)\n",
    "#     else:\n",
    "#         # Reduce the number of batches to match the total number of batches in the data loader\n",
    "#         # if num_batches exceeds the number of batches in the data loader\n",
    "#         num_batches = min(num_batches, len(data_loader))\n",
    "#     for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "#         if i < num_batches:\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             total_loss += loss.item()\n",
    "#         else:\n",
    "#             break\n",
    "#     return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Note:\n",
    "# # Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# # which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# # However, the resulting loss values may be slightly different.\n",
    "\n",
    "# #if torch.cuda.is_available():\n",
    "# #    device = torch.device(\"cuda\")\n",
    "# #elif torch.backends.mps.is_available():\n",
    "# #    device = torch.device(\"mps\")\n",
    "# #else:\n",
    "# #    device = torch.device(\"cpu\")\n",
    "# #\n",
    "# # print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "# model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "# torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "# with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "#     train_loss = calc_loss_loader(trn_dl, model, device)\n",
    "#     val_loss = calc_loss_loader(val_dl, model, device)\n",
    "\n",
    "# print(\"Training loss:\", train_loss)\n",
    "# print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 4,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 96*2,\n",
    "    'ctx_len': seq_len,\n",
    "    'n_head': 4,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662065d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could davidjl alliesopter mishand Free asteroidsBFatmeal lifes Definitive\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        super().__init__(*ms, **metrics)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds.flatten(0, 1)), y.flatten())\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/133 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot params: 30,627,601; MFLOPS: 30.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|MFLOPS|\n",
       "|--|--|--|--|--|\n",
       "|Embedding|(4, 256)|(4, 256, 192)|9,649,344|9.6|\n",
       "|Embedding|(256,)|(256, 192)|9,649,344|9.6|\n",
       "|Dropout|(4, 256, 192)|(4, 256, 192)|0|0.0|\n",
       "|Sequential|(4, 256, 192)|(4, 256, 192)|1,628,928|1.6|\n",
       "|LayerNorm|(4, 256, 192)|(4, 256, 192)|384|0.0|\n",
       "|Linear|(4, 256, 192)|(4, 256, 50257)|9,699,601|9.7|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfba866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='71' class='' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.38% [71/133 00:08&lt;00:07 10.718]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(gamma=1.1, max_mult=2, start_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639085e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.097</td>\n",
       "      <td>7.704</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.080</td>\n",
       "      <td>6.733</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.131</td>\n",
       "      <td>6.372</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.150</td>\n",
       "      <td>6.080</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.153</td>\n",
       "      <td>5.940</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.144</td>\n",
       "      <td>5.835</td>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164</td>\n",
       "      <td>5.675</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.167</td>\n",
       "      <td>5.576</td>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.171</td>\n",
       "      <td>5.485</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.178</td>\n",
       "      <td>5.436</td>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.177</td>\n",
       "      <td>5.330</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.132</td>\n",
       "      <td>5.589</td>\n",
       "      <td>5</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.181</td>\n",
       "      <td>5.210</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164</td>\n",
       "      <td>5.306</td>\n",
       "      <td>6</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.186</td>\n",
       "      <td>5.102</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.185</td>\n",
       "      <td>5.104</td>\n",
       "      <td>7</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.192</td>\n",
       "      <td>5.000</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.192</td>\n",
       "      <td>5.055</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.195</td>\n",
       "      <td>4.912</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>00:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.194</td>\n",
       "      <td>4.972</td>\n",
       "      <td>9</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m token_ids = \u001b[43mgenerate_text_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mctx_len\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mgenerate_text_simple\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[32m      4\u001b[39m     idx_cond = idx[:, -context_size:]  \u001b[38;5;66;03m# Crop current context if it exceeds the supported context size\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# (bs, n_tokens, vocab_sz)\u001b[39;00m\n\u001b[32m      6\u001b[39m     logits = logits[:, -\u001b[32m1\u001b[39m, :]                              \u001b[38;5;66;03m# (bs, vocab_sz)\u001b[39;00m\n\u001b[32m      7\u001b[39m     probas = torch.softmax(logits, dim=-\u001b[32m1\u001b[39m)                 \u001b[38;5;66;03m# (bs, vocab_sz)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     14\u001b[39m     bs, seq_len = x.shape\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     tok = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     pos = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(seq_len, device=x.device))\n\u001b[32m     17\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.do(tok + pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"])\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35956e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab78ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  completion\n",
       "0  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "1  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "2  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "3  Once upon a time, there lived a bunny in a fie...         NaN\n",
       "4  Once upon a time, there lived a bunny in a fie...         NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('evaluation_prompts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d716df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0673b94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day a girl walked into the living room and noticed something very strange. There was a huge cabinet standing in the corner. It looked very old and heavy. She walked over and tried to open it, when suddenly'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[5]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee60c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day a girl walked into the living room and noticed something very strange. There was a huge cabinet standing in the corner. It looked very old and heavy. She walked over and tried to open it, when suddenly'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[9]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24267d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once upon a time, there lived a hamster in the forest. Every day, he would walked around the forest and looking for adventures. One day, he heard someone calling out from behind the bushes.\\nThe hamster listened carefully. He realised that it was a small mouse calling out for help. It got stuck under a heavy log and couldn't get out. The hamster immediately realized that\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7730f75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m token_ids = \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOnce upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms house, she realized she\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ms starting to feel sick. She was so weak she could\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mctx_len\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.3\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mgenerate\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size, temperature, top_k, eos_id)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(model, idx, max_new_tokens, context_size, temperature=\u001b[32m0.0\u001b[39m, top_k=\u001b[38;5;28;01mNone\u001b[39;00m, eos_id=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# For-loop is the same as before: Get logits, and only focus on last time step\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         idx_cond = idx[:, -context_size:].to(\u001b[43mdevice\u001b[49m)\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      7\u001b[39m             logits = model(idx_cond)\n",
      "\u001b[31mNameError\u001b[39m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\", tokenizer),\n",
    "    max_new_tokens=150,\n",
    "    context_size=cfg[\"ctx_len\"],\n",
    "    top_k=25,\n",
    "    temperature=1.3\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3c31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = df.loc[0, 'prompt']\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7454,  2402,   257,   640,    11,   612,  5615,   257, 44915,   287,\n",
       "           257,  2214,    13,  2332,  1438,   373, 22162,    13, 22162,  6151,\n",
       "           284,   423,   730,  5773,   290,  4671,   351,   607, 44915,  2460,\n",
       "            13,  1881,  1110,    11,   618, 22162,   373,   546,   284,  2666,\n",
       "           329,   257, 26951,   379,   257,  1545,   338,  2156,    11,   673,\n",
       "          6939,   673,   338,  3599,   284,  1254,  6639,    13,  1375,   373,\n",
       "           523,  4939,   673,   714]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = text_to_token_ids(prompt, tokenizer)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110df1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7454,  2402,   257,   640,    11,   612,  5615,   257, 44915,   287,\n",
       "           257,  2214,    13,  2332,  1438,   373, 22162,    13, 22162,  6151,\n",
       "           284,   423,   730,  5773,   290,  4671,   351,   607, 44915,  2460,\n",
       "            13,  1881,  1110,    11,   618, 22162,   373,   546,   284,  2666,\n",
       "           329,   257, 26951,   379,   257,  1545,   338,  2156,    11,   673,\n",
       "          6939,   673,   338,  3599,   284,  1254,  6639,    13,  1375,   373,\n",
       "           523,  4939,   673,   714,   262,  4252,  2474,   198,  1537,   511,\n",
       "         22940, 26391,  8151,  2474,   198,   464,  2576,   351,   465, 13373,\n",
       "            11,   484,  2936,  1165,  1110,   607,  1995,   531,    13,   383,\n",
       "          1641,   373,   407,   262,  1310,  2933,  3706,    13,  1375,  2497,\n",
       "           607,  1995,   290,   340,   284,   307,   257,   845,  2041,   788,\n",
       "           262,  3952,   290,   531,   262,  1310,  2576,  1110,   290,   339,\n",
       "           531,   366,    40,   284,   423,   845,   881,   290,   673,  2982,\n",
       "           257,  1310,  2576,  1392,    11,  6184,   373,   257,   640,   286,\n",
       "           329,   607,  1995,   531,   290,   531,   262,  5434,  1110,    11,\n",
       "           262,  8222,     0, 22940, 26391,   129,   241,   607,   835,   262,\n",
       "         11376,    11,   366,  1026,  2041,    11,   290,   531,    11,  6184,\n",
       "           339,   561,  8161,   422,   262,  2156,   673,    13,   366, 29252,\n",
       "           290,   531,   673,   531,    11,   366,    40,  1468,   257,   845,\n",
       "          2497,   257,   890,  1037,   262,  1468,   373,   284,   465,   257,\n",
       "           845,   373,   523,  6568,   284,  1064,  2474,  1375,  3114,   379,\n",
       "           262,  1310,  2933,  1816]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=toks,\n",
    "    max_new_tokens=150,\n",
    "    context_size=cfg[\"ctx_len\"],\n",
    "    top_k=25,\n",
    "    temperature=1.3\n",
    ")\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d3b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could the sun!\"\n",
      "But theirâ€™!\"\n",
      "The girl with his toy, they felt too day her mom said. The family was not the little boy named. She saw her mom and it to be a very special then the park and said the little girl day and he said \"I to have very much and she heard a little girl got, � was a time of for her mom said and said the bug day, the forest!â€œ her way the garden, \"It special, and said, � he would careful from the house she. \"Mom and said she said, \"I old a very saw a long help the old was to his a very was so excited to find!\" She looked at the little boy went\n"
     ]
    }
   ],
   "source": [
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec767aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c9048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the sun!\"\n",
      "But theirâ€™!\"\n",
      "The girl with his toy, they felt too day her mom said. The family was not the little boy named. She saw her mom and it to be a very special then the park and said the little girl day and he said \"I to have very much and she heard a little girl got, � was a time of for her mom said and said the bug day, the forest!â€œ her way the garden, \"It special, and said, � he would careful from the house she. \"Mom and said she said, \"I old a very saw a long help the old was to his a very was so excited to find!\" She looked at the little boy went\n"
     ]
    }
   ],
   "source": [
    "print(token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0deb3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8559/3933177962.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value ' the sun!\"\n",
      "But theirâ€™!\"\n",
      "The girl with his toy, they felt too day her mom said. The family was not the little boy named. She saw her mom and it to be a very special then the park and said the little girl day and he said \"I to have very much and she heard a little girl got, � was a time of for her mom said and said the bug day, the forest!â€œ her way the garden, \"It special, and said, � he would careful from the house she. \"Mom and said she said, \"I old a very saw a long help the old was to his a very was so excited to find!\" She looked at the little boy went' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[0, 'completion'] = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt        Once upon a time, there lived a bunny in a fie...\n",
       "completion     the sun!\"\\nBut theirâ€™!\"\\nThe girl with his ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'completion'] = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1aa8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the sun!\"\\nBut theirâ€™!\"\\nThe girl with his toy, they felt too day her mom said. The family was not the little boy named. She saw her mom and it to be a very special then the park and said the little girl day and he said \"I to have very much and she heard a little girl got, � was a time of for her mom said and said the bug day, the forest!â€œ her way the garden, \"It special, and said, � he would careful from the house she. \"Mom and said she said, \"I old a very saw a long help the old was to his a very was so excited to find!\" She looked at the little boy went'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd516a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row, model, tokenizer, max_tokens=150, context_size=cfg[\"ctx_len\"], \n",
    "                top_k=25, temperature=1.3):\n",
    "    # Tokenize the prompt\n",
    "    toks = text_to_token_ids(row['prompt'], tokenizer)\n",
    "    \n",
    "    # Generate completion\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=toks,\n",
    "        max_new_tokens=max_tokens,\n",
    "        context_size=context_size,\n",
    "        top_k=top_k,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # Extract only the generated part (not the original prompt)\n",
    "    completion = token_ids_to_text(token_ids[:, toks.shape[1]:], tokenizer)\n",
    "    \n",
    "    return completion\n",
    "\n",
    "# Apply the function to each row in the dataframe\n",
    "df['completion'] = df.apply(lambda row: process_row(row, model, tokenizer, max_tokens=64), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b414bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>have help she decided. He looked at him for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>her mom of the end, so happy and they was a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>, â€™ all! It was so excited to take on to his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>a special, but he ran out a bit, and saw a ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>go to play the little girl named Lily! \\nOne ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   have help she decided. He looked at him for t...  \n",
       "1   her mom of the end, so happy and they was a g...  \n",
       "2  , â€™ all! It was so excited to take on to his...  \n",
       "3   a special, but he ran out a bit, and saw a ti...  \n",
       "4   go to play the little girl named Lily! \\nOne ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c81171",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"0401_init.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12510bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>have help she decided. He looked at him for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>her mom of the end, so happy and they was a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>, â€™ all! It was so excited to take on to his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>a special, but he ran out a bit, and saw a ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there lived a bunny in a fie...</td>\n",
       "      <td>go to play the little girl named Lily! \\nOne ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Once upon a time, there lived a bunny in a fie...   \n",
       "1  Once upon a time, there lived a bunny in a fie...   \n",
       "2  Once upon a time, there lived a bunny in a fie...   \n",
       "3  Once upon a time, there lived a bunny in a fie...   \n",
       "4  Once upon a time, there lived a bunny in a fie...   \n",
       "\n",
       "                                          completion  \n",
       "0   have help she decided. He looked at him for t...  \n",
       "1   her mom of the end, so happy and they was a g...  \n",
       "2  , â€™ all! It was so excited to take on to his...  \n",
       "3   a special, but he ran out a bit, and saw a ti...  \n",
       "4   go to play the little girl named Lily! \\nOne ...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"0401_init.csv\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. minbpe\n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
