{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371da66",
   "metadata": {},
   "source": [
    "## TinyStories Hackathon Rules\n",
    "This hackathon is intended to be a fun competition to give ourselves practice pretraining LLMs on consumer hardware. We will follow the [TinyStories paper](<https://arxiv.org/abs/2305.07759>) and train small language models on small datasets and hardware.\n",
    "\n",
    "The hackathon will end on April 7th, [AOE](<https://en.wikipedia.org/wiki/AoE>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c70b6a",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. [**TinyStories:**](<https://huggingface.co/datasets/roneneldan/TinyStories>)\n",
    "   Note that the TinyStories dataset is split into two versions both in the HF dataset:\n",
    "     - GPT-3.5 generated TinyStories\n",
    "    - GPT-4 generated TinyStories\n",
    "   The tar file appears to have the cleanest versions with the least number of duplicates.\n",
    "2. **[Simple Wikipedia](<https://huggingface.co/datasets/lsb/simplewiki2023>)** (optional)\n",
    "   This dataset can be used to give your model more world knowledge than from just the TinyStories dataset. But be careful that \n",
    "it doesn't cause your model to use words which a typical 3 to 4-year-olds doesn't understand. It may need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1528f9",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Models will be evaluated by LLM-as-a-judge following the methodology outlined in the TinyStories paper. More details including how to submit your model's outputs early next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15400",
   "metadata": {},
   "source": [
    "### Model Size Limits\n",
    "Participants will be slotted into one of the following categories based on their hardware:\n",
    "- **Small**: Up to 30M parameters. Low-to-mid range laptop GPUs and Apple Silicon.\n",
    "- **Medium**: Up to 60M parameters. Mid-range GPUs (including high-end laptop GPUs and Apple Silicon)\n",
    "- **Large**: Up to 120M parameters. High-end GPUs and multi-GPU systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1a81",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "While you must train your model from scratch, you are welcome to use any pre-trained tokenizer or train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbdaa",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "You are welcome to use any model architecture you want provided you stay within the parameter budget of your hardware by following the parameter counting rules below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadc72b",
   "metadata": {},
   "source": [
    "### Parameter Counting\n",
    "The Parameter budget is the number of unique floating-point weights receiving gradient updates:\n",
    "- Unique Weights: Count each distinct floating-point weight stored in the model once.\n",
    "- Reuse Multiplier: For each weight, multiply by the number of distinct times it contributes to forward computation (e.g., due to layer-sharing, layer reuse, or non-standard head-sharing). Weight-tied embedding and decoder weights are the exception and are only counted once. MQA/GQA doesn't count as head-sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec912d",
   "metadata": {},
   "source": [
    "### Teams\n",
    "Teams are limited to a maximum of 2 members and must be formed and declared within the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b91a3",
   "metadata": {},
   "source": [
    "### Training Frameworks\n",
    "You might want to take a look at the following libraries and frameworks and adopt one for pretraining:\n",
    "- [Composer](<https://docs.mosaicml.com/projects/composer/en/stable/index.html>) and optionally [LLM Foundry](<https://github.com/mosaicml/llm-foundry>)\n",
    "- [PyTorch Lightning](<https://lightning.ai/docs/pytorch/stable/>) and optionally [LitGPT](<https://github.com/Lightning-AI/litgpt>)\n",
    "- Hugging Face [Trainer](<https://huggingface.co/docs/transformers/en/main_classes/trainer>), [Accelerate](<https://huggingface.co/docs/accelerate/en/index>), and optionally [Axolotl](<https://axolotl-ai-cloud.github.io/axolotl/>) (a wrapper on top of HF)\n",
    "- [fastai](<https://docs.fast.ai/>) with either [fastxtend](<https://fastxtend.benjaminwarner.dev/text.huggingface.html>)/[blurr](<https://ohmeow.github.io/blurr/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f566a",
   "metadata": {},
   "source": [
    "For now, we can just use gpt2 tokenizer to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(txt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b7912",
   "metadata": {},
   "source": [
    "Let's encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50878a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(b):\n",
    "    b['text'] = [tokenizer.encode(o, allowed_special={'<|endoftext|>'}) for o in b['text']]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.with_transform(encode)\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2",
   "metadata": {},
   "source": [
    "Now we have numbers. We have to decode them to read text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee9952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trn[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8462f",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1024\n",
    "chunk_sz = seq_len + 1\n",
    "eot_token = 50256\n",
    "eot_tensor = torch.tensor([eot_token])\n",
    "\n",
    "div_by = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e776a",
   "metadata": {},
   "source": [
    "Let's try to use 1% data to get started. Our goal is to add `eot_token` to the end of each text. Then, chop them up into `seq_len` to create each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = trn.num_rows // div_by // 3\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a17a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = [torch.tensor(o) for o in trn[:data_len]['text']]\n",
    "seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "          257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "          284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "          538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.cat([torch.cat([s, eot_tensor]) for s in seq_tensor])\n",
    "cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12645])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06b6",
   "metadata": {},
   "source": [
    "Let's create batches with `seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73653862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_complete_segments = cat.size(0) // chunk_sz\n",
    "num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddb9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1025])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_segments = cat[:num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "complete_segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29129328",
   "metadata": {},
   "source": [
    "> TODO\n",
    "\n",
    "Looking at the last bit, it is pretty close to a whole `seq_len`. We can pad it and use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([357])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder = cat[num_complete_segments * seq_len:]\n",
    "remainder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4a22",
   "metadata": {},
   "source": [
    "### Dataset (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7b90b",
   "metadata": {},
   "source": [
    "Let's create inputs and targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1024]), torch.Size([12, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = complete_segments[:, :-1]\n",
    "targs = complete_segments[:, 1:]\n",
    "inps.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "          287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e53f6",
   "metadata": {},
   "source": [
    "We can create a dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac4a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3198,  1110,    11,  ..., 24829,   284,   262]),\n",
       " tensor([1110,   11,  257,  ...,  284,  262, 7586]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds = Dataset(inps, targs)\n",
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68240679",
   "metadata": {},
   "source": [
    "We got the training dataset. Now, we can get the validation dataset with the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_len = val.num_rows // div_by * 10\n",
    "val_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcddecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq_tensor = [torch.tensor(o) for o in val[:val_data_len]['text']]\n",
    "val_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "         1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "         3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "         1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "         1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "          550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "        22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "           13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "        12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "          757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "          371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "        30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cat = torch.cat([torch.cat([s, eot_tensor]) for s in val_seq_tensor])\n",
    "val_cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_num_complete_segments = val_cat.size(0) // chunk_sz\n",
    "val_num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd13d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1025])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_complete_segments = val_cat[:val_num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "val_complete_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb46a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1024]), torch.Size([3, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inps = val_complete_segments[:, :-1]\n",
    "val_targs = val_complete_segments[:, 1:]\n",
    "val_inps.shape, val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32565,    13, 15899,  ...,    13,  4186,   373]),\n",
       " tensor([   13, 15899,  2497,  ...,  4186,   373,  3772]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = Dataset(val_inps, val_targs)\n",
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757872",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409779a",
   "metadata": {},
   "source": [
    "We need a dataloader with the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\n",
    "dls = DataLoaders(trn_dl, val_dl)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[34681,   461,   290,  ...,   383, 21613,   373],\n",
       "         [  286,  2842,    13,  ..., 12615,   257, 41236],\n",
       "         [  484,   750,   407,  ...,  1263, 14528,    88],\n",
       "         [  765,   284, 12797,  ...,   640,    11,   612]]),\n",
       " tensor([[  461,   290,  2921,  ..., 21613,   373,  1402],\n",
       "         [ 2842,    13, 50256,  ...,   257, 41236,    13],\n",
       "         [  750,   407,  6654,  ..., 14528,    88, 10649],\n",
       "         [  284, 12797,   262,  ...,    11,   612,   373]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3adb4",
   "metadata": {},
   "source": [
    "We make the model using transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b68844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ctx_len, n_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_head == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // n_head\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones((ctx_len, ctx_len)), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x): \n",
    "        bs, ctx_len, d_in = x.shape\n",
    "        q = self.w_q(x)  # (bs, ctx_len, d_out)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        \n",
    "        q = q.view(bs, ctx_len, self.n_head, self.head_dim)  # (bs, ctx_len, n_head, head_dim)\n",
    "        k = k.view(bs, ctx_len, self.n_head, self.head_dim)\n",
    "        v = v.view(bs, ctx_len, self.n_head, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1,2) # (bs, n_head, ctx_len, head_dim)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn_scr = q@k.transpose(2,3) # (bs, n_head, ctx_len, ctx_len)\n",
    "        \n",
    "        # mask\n",
    "        attn_scr = attn_scr.masked_fill(self.mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        \n",
    "        attn_wt = torch.softmax(attn_scr / k.shape[-1]**0.5, -1)\n",
    "        \n",
    "        ctx_vec = attn_wt@v  # (bs, n_head, ctx_len, head_dim)\n",
    "        ctx_vec = ctx_vec.transpose(1,2).reshape(bs, ctx_len, -1) # (bs, ctx_len, d_out)\n",
    "        \n",
    "        # concat\n",
    "        return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ctx_len, n_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_head == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // n_head\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones((ctx_len, ctx_len)), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x): \n",
    "        bs, num_tokens, d_in = x.shape\n",
    "        q = self.w_q(x)  # (bs, ctx_len, d_out)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        \n",
    "        q = q.view(bs, num_tokens, self.n_head, self.head_dim)  # (bs, ctx_len, n_head, head_dim)\n",
    "        k = k.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        v = v.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1,2) # (bs, n_head, ctx_len, head_dim)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn_scr = q@k.transpose(2,3) # (bs, n_head, ctx_len, ctx_len)\n",
    "        \n",
    "        # mask\n",
    "        attn_scr = attn_scr.masked_fill(self.mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        \n",
    "        attn_wt = torch.softmax(attn_scr / k.shape[-1]**0.5, -1)\n",
    "        \n",
    "        ctx_vec = attn_wt@v  # (bs, n_head, ctx_len, head_dim)\n",
    "        ctx_vec = ctx_vec.transpose(1,2).reshape(bs, num_tokens, -1) # (bs, ctx_len, d_out)\n",
    "        \n",
    "        # concat\n",
    "        return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0806, -0.0211,  0.1296, -0.0414],\n",
       "          [ 0.2287,  0.1592, -0.0296, -0.1997]],\n",
       " \n",
       "         [[ 0.6081,  0.0087,  0.2989, -0.4998],\n",
       "          [ 0.1026,  0.0422,  0.3105, -0.2781]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([2, 2, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=4, ctx_len=2, n_head=2)\n",
    "mha(x), mha(x).shape  # Outputs (bs, ctx_len, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8b039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2928, -0.1471,  0.0123, -0.2592], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(4)\n",
    "ff = FeedForward(4, 4*4)\n",
    "ff(x), ff(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = MultiHeadAttention(emb_dim, emb_dim, ctx_len, n_head, qkv_bias=qkv_bias)\n",
    "        self.do = nn.Dropout(drop_out)\n",
    "        self.ff = FeedForward(emb_dim, emb_dim*ff_mult)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39725b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3595, -1.0631,  0.5676],\n",
       "          [-0.3159, -2.2222,  0.1778]],\n",
       " \n",
       "         [[ 1.7169, -1.7565,  1.2721],\n",
       "          [-0.3908, -0.2502,  0.3274]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)\n",
    "tb = TransformerBlock(emb_dim=3, ctx_len=2, n_head=1)\n",
    "tb(x), tb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_sz': 3,\n",
    "    'emb_dim': 768,\n",
    "    'ctx_len': 1024,\n",
    "    'n_head': 1,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(x)\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = xb[:3]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108a4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50256)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ab458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 50257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e879d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 32141,  2117,  8258, 42538,  2328,  3717]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2125798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am viabilityosp funnyIDES concern 2009\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1658bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves youbrook manga say ال iPadsategvon Specifications considerably Errors\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76166fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[10762],\n",
      "         [47053],\n",
      "         [23567]],\n",
      "\n",
      "        [[    3],\n",
      "         [23440],\n",
      "         [38813]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49720ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  deemedOSPiggins\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([3, 1024]) torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in trn_dl:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e846d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 12288\n",
      "Validation tokens: 3072\n",
      "All tokens: 15360\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in trn_dl:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dl:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 11.005916913350424\n",
      "Validation loss: 11.005401611328125\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(trn_dl, model, device)\n",
    "    val_loss = calc_loss_loader(val_dl, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MetricsCB(), ProgressCB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7995ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(50257, 768)\n",
       "  (do): Dropout(p=0, inplace=False)\n",
       "  (tb): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (do): Dropout(p=0, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_l): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4866d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [2/10 00:32&lt;02:08]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10.998</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.998</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.998</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[224]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m learn = TrainLearner(model, dls, loss_func=loss_fn, cbs=cbs)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:462\u001b[39m, in \u001b[36mlr_find\u001b[39m\u001b[34m(self, gamma, max_mult, start_lr, max_epochs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;129m@fc\u001b[39m.patch\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m:Learner, gamma=\u001b[32m1.3\u001b[39m, max_mult=\u001b[32m3\u001b[39m, start_lr=\u001b[32m1e-5\u001b[39m, max_epochs=\u001b[32m10\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLRFinderCB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:264\u001b[39m, in \u001b[36mLearner.fit\u001b[39m\u001b[34m(self, n_epochs, train, valid, cbs, lr)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lr = \u001b[38;5;28mself\u001b[39m.lr\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_func: \u001b[38;5;28mself\u001b[39m.opt = \u001b[38;5;28mself\u001b[39m.opt_func(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m cbs: \u001b[38;5;28mself\u001b[39m.cbs.remove(cb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:254\u001b[39m, in \u001b[36mLearner._fit\u001b[39m\u001b[34m(self, train, valid)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train: \u001b[38;5;28mself\u001b[39m.one_epoch(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mone_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:245\u001b[39m, in \u001b[36mLearner.one_epoch\u001b[39m\u001b[34m(self, training)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train(training)\n\u001b[32m    244\u001b[39m \u001b[38;5;28mself\u001b[39m.dl = \u001b[38;5;28mself\u001b[39m.train_dl \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dls.valid\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:240\u001b[39m, in \u001b[36mLearner._one_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter,\u001b[38;5;28mself\u001b[39m.batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.dl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:227\u001b[39m, in \u001b[36mLearner._one_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m'\u001b[39m\u001b[33mafter_predict\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_loss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:298\u001b[39m, in \u001b[36mTrainLearner.predict\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    297\u001b[39m     inps = _get_inp(\u001b[38;5;28mself\u001b[39m.batch, \u001b[38;5;28mself\u001b[39m.n_inp, \u001b[38;5;28mself\u001b[39m.inp_nm)\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28mself\u001b[39m.preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[202]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     17\u001b[39m x = \u001b[38;5;28mself\u001b[39m.tb(x)\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_ln(x)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinal_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = TrainLearner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. \n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
