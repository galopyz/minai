{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371da66",
   "metadata": {},
   "source": [
    "## TinyStories Hackathon Rules\n",
    "This hackathon is intended to be a fun competition to give ourselves practice pretraining LLMs on consumer hardware. We will follow the [TinyStories paper](<https://arxiv.org/abs/2305.07759>) and train small language models on small datasets and hardware.\n",
    "\n",
    "The hackathon will end on April 7th, [AOE](<https://en.wikipedia.org/wiki/AoE>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c70b6a",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. [**TinyStories:**](<https://huggingface.co/datasets/roneneldan/TinyStories>)\n",
    "   Note that the TinyStories dataset is split into two versions both in the HF dataset:\n",
    "     - GPT-3.5 generated TinyStories\n",
    "    - GPT-4 generated TinyStories\n",
    "   The tar file appears to have the cleanest versions with the least number of duplicates.\n",
    "2. **[Simple Wikipedia](<https://huggingface.co/datasets/lsb/simplewiki2023>)** (optional)\n",
    "   This dataset can be used to give your model more world knowledge than from just the TinyStories dataset. But be careful that \n",
    "it doesn't cause your model to use words which a typical 3 to 4-year-olds doesn't understand. It may need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1528f9",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Models will be evaluated by LLM-as-a-judge following the methodology outlined in the TinyStories paper. More details including how to submit your model's outputs early next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15400",
   "metadata": {},
   "source": [
    "### Model Size Limits\n",
    "Participants will be slotted into one of the following categories based on their hardware:\n",
    "- **Small**: Up to 30M parameters. Low-to-mid range laptop GPUs and Apple Silicon.\n",
    "- **Medium**: Up to 60M parameters. Mid-range GPUs (including high-end laptop GPUs and Apple Silicon)\n",
    "- **Large**: Up to 120M parameters. High-end GPUs and multi-GPU systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1a81",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "While you must train your model from scratch, you are welcome to use any pre-trained tokenizer or train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbdaa",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "You are welcome to use any model architecture you want provided you stay within the parameter budget of your hardware by following the parameter counting rules below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadc72b",
   "metadata": {},
   "source": [
    "### Parameter Counting\n",
    "The Parameter budget is the number of unique floating-point weights receiving gradient updates:\n",
    "- Unique Weights: Count each distinct floating-point weight stored in the model once.\n",
    "- Reuse Multiplier: For each weight, multiply by the number of distinct times it contributes to forward computation (e.g., due to layer-sharing, layer reuse, or non-standard head-sharing). Weight-tied embedding and decoder weights are the exception and are only counted once. MQA/GQA doesn't count as head-sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec912d",
   "metadata": {},
   "source": [
    "### Teams\n",
    "Teams are limited to a maximum of 2 members and must be formed and declared within the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b91a3",
   "metadata": {},
   "source": [
    "### Training Frameworks\n",
    "You might want to take a look at the following libraries and frameworks and adopt one for pretraining:\n",
    "- [Composer](<https://docs.mosaicml.com/projects/composer/en/stable/index.html>) and optionally [LLM Foundry](<https://github.com/mosaicml/llm-foundry>)\n",
    "- [PyTorch Lightning](<https://lightning.ai/docs/pytorch/stable/>) and optionally [LitGPT](<https://github.com/Lightning-AI/litgpt>)\n",
    "- Hugging Face [Trainer](<https://huggingface.co/docs/transformers/en/main_classes/trainer>), [Accelerate](<https://huggingface.co/docs/accelerate/en/index>), and optionally [Axolotl](<https://axolotl-ai-cloud.github.io/axolotl/>) (a wrapper on top of HF)\n",
    "- [fastai](<https://docs.fast.ai/>) with either [fastxtend](<https://fastxtend.benjaminwarner.dev/text.huggingface.html>)/[blurr](<https://ohmeow.github.io/blurr/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f566a",
   "metadata": {},
   "source": [
    "For now, we can just use gpt2 tokenizer to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2724526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5cf1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(txt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b7912",
   "metadata": {},
   "source": [
    "Let's encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50878a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(b):\n",
    "    b['text'] = [tokenizer.encode(o, allowed_special={'<|endoftext|>'}) for o in b['text']]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c69ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.with_transform(encode)\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2",
   "metadata": {},
   "source": [
    "Now we have numbers. We have to decode them to read text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ee9952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa90df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trn[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8462f",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03d6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "chunk_sz = seq_len + 1\n",
    "eot_token = 50256\n",
    "eot_tensor = torch.tensor([eot_token])\n",
    "\n",
    "div_by = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e776a",
   "metadata": {},
   "source": [
    "Let's try to use 1% data to get started. Our goal is to add `eot_token` to the end of each text. Then, chop them up into `seq_len` to create each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = trn.num_rows // div_by // 3\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20a17a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = [torch.tensor(o) for o in trn[:data_len]['text']]\n",
    "seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f806d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "          257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "          284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "          538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.cat([torch.cat([s, eot_tensor]) for s in seq_tensor])\n",
    "cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eb5f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12645])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06b6",
   "metadata": {},
   "source": [
    "Let's create batches with `seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73653862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_complete_segments = cat.size(0) // chunk_sz\n",
    "num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ddb9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1025])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_segments = cat[:num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "complete_segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29129328",
   "metadata": {},
   "source": [
    "> TODO\n",
    "\n",
    "Looking at the last bit, it is pretty close to a whole `seq_len`. We can pad it and use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36325d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([357])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder = cat[num_complete_segments * seq_len:]\n",
    "remainder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4a22",
   "metadata": {},
   "source": [
    "### Dataset (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7b90b",
   "metadata": {},
   "source": [
    "Let's create inputs and targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758a6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 1024]), torch.Size([12, 1024]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = complete_segments[:, :-1]\n",
    "targs = complete_segments[:, 1:]\n",
    "inps.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce24dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df3229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "          287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e53f6",
   "metadata": {},
   "source": [
    "We can create a dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ac4a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3198,  1110,    11,  ..., 24829,   284,   262]),\n",
       " tensor([1110,   11,  257,  ...,  284,  262, 7586]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds = Dataset(inps, targs)\n",
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68240679",
   "metadata": {},
   "source": [
    "We got the training dataset. Now, we can get the validation dataset with the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e86d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_len = val.num_rows // div_by * 10\n",
    "val_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bcddecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq_tensor = [torch.tensor(o) for o in val[:val_data_len]['text']]\n",
    "val_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01220591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "         1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "         3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "         1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "         1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "          550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "        22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "           13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "        12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "          757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "          371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "        30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cat = torch.cat([torch.cat([s, eot_tensor]) for s in val_seq_tensor])\n",
    "val_cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "380f87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_num_complete_segments = val_cat.size(0) // chunk_sz\n",
    "val_num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dabd13d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1025])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_complete_segments = val_cat[:val_num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "val_complete_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "faeb46a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1024]), torch.Size([3, 1024]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inps = val_complete_segments[:, :-1]\n",
    "val_targs = val_complete_segments[:, 1:]\n",
    "val_inps.shape, val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bade8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32565,    13, 15899,  ...,    13,  4186,   373]),\n",
       " tensor([   13, 15899,  2497,  ...,  4186,   373,  3772]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = Dataset(val_inps, val_targs)\n",
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757872",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409779a",
   "metadata": {},
   "source": [
    "We need a dataloader with the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f8ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1024]), torch.Size([4, 1024]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\n",
    "dls = DataLoaders(trn_dl, val_dl)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c656d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  290,  3409,    13,  ...,   703,  5802,   340],\n",
       "         [  257,  1310,  2933,  ...,   887,   428,   640],\n",
       "         [  286,  2842,    13,  ..., 12615,   257, 41236],\n",
       "         [34681,   461,   290,  ...,   383, 21613,   373]]),\n",
       " tensor([[ 3409,    13,  1119,  ...,  5802,   340,  3947],\n",
       "         [ 1310,  2933,  3706,  ...,   428,   640,    11],\n",
       "         [ 2842,    13, 50256,  ...,   257, 41236,    13],\n",
       "         [  461,   290,  2921,  ..., 21613,   373,  1402]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3adb4",
   "metadata": {},
   "source": [
    "We make the model using transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b4a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7811bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd5a55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ctx_len, n_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_head == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // n_head\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones((ctx_len, ctx_len)), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x): \n",
    "        bs, num_tokens, d_in = x.shape\n",
    "        q = self.w_q(x)  # (bs, num_tokens, d_out)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        \n",
    "        q = q.view(bs, num_tokens, self.n_head, self.head_dim)  # (bs, num_tokens, n_head, head_dim)\n",
    "        k = k.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        v = v.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1,2) # (bs, n_head, num_tokens, head_dim)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn_scr = q@k.transpose(2,3) # (bs, n_head, num_tokens, num_tokens)\n",
    "        attn_scr = attn_scr.masked_fill(self.mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_wt = torch.softmax(attn_scr / k.shape[-1]**0.5, -1)\n",
    "        \n",
    "        ctx_vec = attn_wt@v  # (bs, n_head, num_tokens, head_dim)\n",
    "        ctx_vec = ctx_vec.transpose(1,2).reshape(bs, num_tokens, -1) # (bs, num_tokens, d_out)\n",
    "        \n",
    "        # concat\n",
    "        return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2907f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1778, -0.1473,  0.1028,  0.0059],\n",
       "          [ 0.1752,  0.0355,  0.1155,  0.3003]],\n",
       " \n",
       "         [[ 1.0137, -0.7234,  0.5223,  0.9915],\n",
       "          [ 0.6409, -0.4204,  0.3602,  0.2310]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([2, 2, 4]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=4, ctx_len=2, n_head=2)\n",
    "mha(x), mha(x).shape  # Outputs (bs, num_tokens, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7aa8b039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "333b6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2928, -0.1471,  0.0123, -0.2592], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(4)\n",
    "ff = FeedForward(4, 4*4)\n",
    "ff(x), ff(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = MultiHeadAttention(emb_dim, emb_dim, ctx_len, n_head, qkv_bias=qkv_bias)\n",
    "        self.do = nn.Dropout(drop_out)\n",
    "        self.ff = FeedForward(emb_dim, emb_dim*ff_mult)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39725b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3595, -1.0631,  0.5676],\n",
       "          [-0.3159, -2.2222,  0.1778]],\n",
       " \n",
       "         [[ 1.7169, -1.7565,  1.2721],\n",
       "          [-0.3908, -0.2502,  0.3274]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)\n",
    "tb = TransformerBlock(emb_dim=3, ctx_len=2, n_head=1)\n",
    "tb(x), tb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c54015cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768 / 2 / 2 / 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "825cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_sz': 3,\n",
    "    'emb_dim': 48,\n",
    "    'ctx_len': seq_len,\n",
    "    'n_head': 1,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4bb2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = xb[:3]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c2ab458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1024, 50257])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7,313,137\n"
     ]
    }
   ],
   "source": [
    "def get_total_params(model): \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters: {total_params:,}\")\n",
    "    return total_params\n",
    "total_params = get_total_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38c5a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 48]), torch.Size([50257, 48]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_emb.weight.shape, model.final_l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc749dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 27.90 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 7,313,137\n",
      "Total size of the model: 27.90 MB\n"
     ]
    }
   ],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n",
    "\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6cfd542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e879d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 38213, 23676,  9929, 29854,  3414, 22988]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e2125798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Moves Poor overt DV announcedpeace\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1658bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you589bj cartoon regional Islamabad Experimental Cancer straw unin fucked\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "403f7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76166fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f1cb5daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[20642],\n",
      "         [38668],\n",
      "         [12967]],\n",
      "\n",
      "        [[49254],\n",
      "         [25070],\n",
      "         [38158]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "49720ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Kurtepisode pig\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f01991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "torch.Size([4, 1024]) torch.Size([4, 1024])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([3, 1024]) torch.Size([3, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in trn_dl:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e846d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 12288\n",
      "Validation tokens: 3072\n",
      "All tokens: 15360\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in trn_dl:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dl:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e49f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6efa9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d823ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.997873306274414\n",
      "Validation loss: 10.994172096252441\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(trn_dl, model, device)\n",
    "    val_loss = calc_loss_loader(val_dl, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [MetricsCB(), ProgressCB()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb7995ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(50257, 768)\n",
       "  (do): Dropout(p=0, inplace=False)\n",
       "  (tb): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (w_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (do): Dropout(p=0, inplace=False)\n",
       "      (ff): FeedForward(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): ReLU()\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (final_l): Linear(in_features=768, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ee4866d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='2' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.00% [2/10 00:19&lt;01:16]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10.998</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10.998</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/3 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m learn = TrainLearner(model, dls, loss_func=loss_fn, cbs=cbs)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlearn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:462\u001b[39m, in \u001b[36mlr_find\u001b[39m\u001b[34m(self, gamma, max_mult, start_lr, max_epochs)\u001b[39m\n\u001b[32m    460\u001b[39m \u001b[38;5;129m@fc\u001b[39m.patch\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlr_find\u001b[39m(\u001b[38;5;28mself\u001b[39m:Learner, gamma=\u001b[32m1.3\u001b[39m, max_mult=\u001b[32m3\u001b[39m, start_lr=\u001b[32m1e-5\u001b[39m, max_epochs=\u001b[32m10\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLRFinderCB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_mult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:264\u001b[39m, in \u001b[36mLearner.fit\u001b[39m\u001b[34m(self, n_epochs, train, valid, cbs, lr)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: lr = \u001b[38;5;28mself\u001b[39m.lr\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.opt_func: \u001b[38;5;28mself\u001b[39m.opt = \u001b[38;5;28mself\u001b[39m.opt_func(\u001b[38;5;28mself\u001b[39m.model.parameters(), lr)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m cbs: \u001b[38;5;28mself\u001b[39m.cbs.remove(cb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:252\u001b[39m, in \u001b[36mLearner._fit\u001b[39m\u001b[34m(self, train, valid)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch_sz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.train_dl = CycleDL(\u001b[38;5;28mself\u001b[39m.train_dl, \u001b[38;5;28mself\u001b[39m.epoch_sz)\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.epochs:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m train: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mone_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(): \u001b[38;5;28mself\u001b[39m.one_epoch(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:245\u001b[39m, in \u001b[36mLearner.one_epoch\u001b[39m\u001b[34m(self, training)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28mself\u001b[39m.model.train(training)\n\u001b[32m    244\u001b[39m \u001b[38;5;28mself\u001b[39m.dl = \u001b[38;5;28mself\u001b[39m.train_dl \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dls.valid\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:240\u001b[39m, in \u001b[36mLearner._one_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;129m@with_cbs\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_one_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter,\u001b[38;5;28mself\u001b[39m.batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.dl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:198\u001b[39m, in \u001b[36mwith_cbs.__call__.<locals>._f\u001b[39m\u001b[34m(o, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    197\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m     o.callback(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mCancel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.nm.title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mException\u001b[39m\u001b[33m'\u001b[39m]: \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:232\u001b[39m, in \u001b[36mLearner._one_batch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m'\u001b[39m\u001b[33mafter_loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.callback(\u001b[33m'\u001b[39m\u001b[33mafter_backward\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    234\u001b[39m     \u001b[38;5;28mself\u001b[39m.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/minai/minai/core.py:305\u001b[39m, in \u001b[36mTrainLearner.backward\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/torch_latest/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = TrainLearner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. \n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
