{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371da66",
   "metadata": {},
   "source": [
    "## TinyStories Hackathon Rules\n",
    "This hackathon is intended to be a fun competition to give ourselves practice pretraining LLMs on consumer hardware. We will follow the [TinyStories paper](<https://arxiv.org/abs/2305.07759>) and train small language models on small datasets and hardware.\n",
    "\n",
    "The hackathon will end on April 7th, [AOE](<https://en.wikipedia.org/wiki/AoE>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c70b6a",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "1. [**TinyStories:**](<https://huggingface.co/datasets/roneneldan/TinyStories>)\n",
    "   Note that the TinyStories dataset is split into two versions both in the HF dataset:\n",
    "     - GPT-3.5 generated TinyStories\n",
    "    - GPT-4 generated TinyStories\n",
    "   The tar file appears to have the cleanest versions with the least number of duplicates.\n",
    "2. **[Simple Wikipedia](<https://huggingface.co/datasets/lsb/simplewiki2023>)** (optional)\n",
    "   This dataset can be used to give your model more world knowledge than from just the TinyStories dataset. But be careful that \n",
    "it doesn't cause your model to use words which a typical 3 to 4-year-olds doesn't understand. It may need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1528f9",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Models will be evaluated by LLM-as-a-judge following the methodology outlined in the TinyStories paper. More details including how to submit your model's outputs early next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15400",
   "metadata": {},
   "source": [
    "### Model Size Limits\n",
    "Participants will be slotted into one of the following categories based on their hardware:\n",
    "- **Small**: Up to 30M parameters. Low-to-mid range laptop GPUs and Apple Silicon.\n",
    "- **Medium**: Up to 60M parameters. Mid-range GPUs (including high-end laptop GPUs and Apple Silicon)\n",
    "- **Large**: Up to 120M parameters. High-end GPUs and multi-GPU systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1a81",
   "metadata": {},
   "source": [
    "### Tokenizers\n",
    "While you must train your model from scratch, you are welcome to use any pre-trained tokenizer or train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbdaa",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "You are welcome to use any model architecture you want provided you stay within the parameter budget of your hardware by following the parameter counting rules below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadc72b",
   "metadata": {},
   "source": [
    "### Parameter Counting\n",
    "The Parameter budget is the number of unique floating-point weights receiving gradient updates:\n",
    "- Unique Weights: Count each distinct floating-point weight stored in the model once.\n",
    "- Reuse Multiplier: For each weight, multiply by the number of distinct times it contributes to forward computation (e.g., due to layer-sharing, layer reuse, or non-standard head-sharing). Weight-tied embedding and decoder weights are the exception and are only counted once. MQA/GQA doesn't count as head-sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec912d",
   "metadata": {},
   "source": [
    "### Teams\n",
    "Teams are limited to a maximum of 2 members and must be formed and declared within the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b91a3",
   "metadata": {},
   "source": [
    "### Training Frameworks\n",
    "You might want to take a look at the following libraries and frameworks and adopt one for pretraining:\n",
    "- [Composer](<https://docs.mosaicml.com/projects/composer/en/stable/index.html>) and optionally [LLM Foundry](<https://github.com/mosaicml/llm-foundry>)\n",
    "- [PyTorch Lightning](<https://lightning.ai/docs/pytorch/stable/>) and optionally [LitGPT](<https://github.com/Lightning-AI/litgpt>)\n",
    "- Hugging Face [Trainer](<https://huggingface.co/docs/transformers/en/main_classes/trainer>), [Accelerate](<https://huggingface.co/docs/accelerate/en/index>), and optionally [Axolotl](<https://axolotl-ai-cloud.github.io/axolotl/>) (a wrapper on top of HF)\n",
    "- [fastai](<https://docs.fast.ai/>) with either [fastxtend](<https://fastxtend.benjaminwarner.dev/text.huggingface.html>)/[blurr](<https://ohmeow.github.io/blurr/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f566a",
   "metadata": {},
   "source": [
    "For now, we can just use gpt2 tokenizer to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2724526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(txt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b7912",
   "metadata": {},
   "source": [
    "Let's encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50878a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(b):\n",
    "    b['text'] = [tokenizer.encode(o, allowed_special={'<|endoftext|>'}) for o in b['text']]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69ff67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.with_transform(encode)\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc3a7c2",
   "metadata": {},
   "source": [
    "Now we have numbers. We have to decode them to read text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee9952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn[0]['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90df89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trn[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8462f",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3e776a",
   "metadata": {},
   "source": [
    "Let's try to use very small subset of data to get started. Our goal is to add `eot_token` to the end of each text. Then, chop them up into `seq_len` to create each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 256\n",
    "chunk_sz = seq_len + 1\n",
    "eot_token = 50256\n",
    "eot_tensor = torch.tensor([eot_token])\n",
    "\n",
    "div_by = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8778d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = trn.num_rows // div_by // 3\n",
    "data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a17a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = [torch.tensor(o) for o in trn[:data_len]['text']]\n",
    "seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806d9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "          284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "         2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "          673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "          198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "          366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "         2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "         1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "          356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "          198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "        19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "          407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "         1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "         1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "         1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "         1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "          257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "          284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "          538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = torch.cat([torch.cat([s, eot_tensor]) for s in seq_tensor])\n",
    "cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb5f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135959])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa06b6",
   "metadata": {},
   "source": [
    "Let's create batches with `seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73653862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "529"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_complete_segments = cat.size(0) // chunk_sz\n",
    "num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddb9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([529, 257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_segments = cat[:num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "complete_segments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29129328",
   "metadata": {},
   "source": [
    "> TODO\n",
    "\n",
    "Looking at the last bit, it is pretty close to a whole `seq_len`. We can pad it and use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36325d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([535])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remainder = cat[num_complete_segments * seq_len:]\n",
    "remainder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4a22",
   "metadata": {},
   "source": [
    "### Dataset (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb7b90b",
   "metadata": {},
   "source": [
    "Let's create inputs and targets for a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758a6335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([529, 256]), torch.Size([529, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = complete_segments[:, :-1]\n",
    "targs = complete_segments[:, 1:]\n",
    "inps.shape, targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24dbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "        17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3229ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "          287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targs[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e53f6",
   "metadata": {},
   "source": [
    "We can create a dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac4a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "         17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "           284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "          2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "           673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "           198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "           366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "          2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "          1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "           356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "           198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "         19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "           407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "          1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "          1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "          1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "          1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "           257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "           284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "           538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922,\n",
       "          5252,    13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,\n",
       "            13,   198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,\n",
       "           287,   262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,\n",
       "           383,  5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,\n",
       "           538,  8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,\n",
       "           351,   606,    13,  1355,   538, 10357]),\n",
       " tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "           287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284,\n",
       "           711,   351,   340,   780,   340,   373,  7786,    13, 20037,  2227,\n",
       "           284,  2648,   262, 17598,   351,   607,  1995,    11,   523,   673,\n",
       "           714, 34249,   257,  4936,   319,   607, 10147,    13,   198,   198,\n",
       "            43,   813,  1816,   284,   607,  1995,   290,   531,    11,   366,\n",
       "         29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,  2648,\n",
       "           340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,  1995,\n",
       "         13541,   290,   531,    11,   366,  5297,    11, 20037,    11,   356,\n",
       "           460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,   198,\n",
       "           198, 41631,    11,   484,  4888,   262, 17598,   290,   384, 19103,\n",
       "           262,  4936,   319, 20037,   338, 10147,    13,   632,   373,   407,\n",
       "          2408,   329,   606,   780,   484,   547,  7373,   290,  5742,  1123,\n",
       "           584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,  1995,\n",
       "           329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,  1119,\n",
       "          1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,  1978,\n",
       "            13, 50256,  7454,  2402,   257,   640,    11,   612,   373,   257,\n",
       "          1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,   284,\n",
       "           467,  3049,   290,   711,   287,   262,  4252,    13,  1355,   538,\n",
       "           373,   257,  5448,  1097,   780,   339,  1464,   550,   922,  5252,\n",
       "            13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,    13,\n",
       "           198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,   287,\n",
       "           262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,   383,\n",
       "          5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,   538,\n",
       "          8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,   351,\n",
       "           606,    13,  1355,   538, 10357,   739]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds = Dataset(inps, targs)\n",
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68240679",
   "metadata": {},
   "source": [
    "We got the training dataset. Now, we can get the validation dataset with the same approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e86d1a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data_len = val.num_rows // div_by * 10\n",
    "val_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcddecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_seq_tensor = [torch.tensor(o) for o in val[:val_data_len]['text']]\n",
    "val_seq_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01220591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "          366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "          290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "          345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "          198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "        15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "         1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "         3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "         2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "         1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "         3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "         1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "         1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "          550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "        22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "           13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "        12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "          757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "          371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "        30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_cat = torch.cat([torch.cat([s, eot_tensor]) for s in val_seq_tensor])\n",
    "val_cat[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f87d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_num_complete_segments = val_cat.size(0) // chunk_sz\n",
    "val_num_complete_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd13d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([167, 257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_complete_segments = val_cat[:val_num_complete_segments * chunk_sz].view(-1, chunk_sz)\n",
    "val_complete_segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb46a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([167, 256]), torch.Size([167, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_inps = val_complete_segments[:, :-1]\n",
    "val_targs = val_complete_segments[:, 1:]\n",
    "val_inps.shape, val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bade8855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([32565,    13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,\n",
       "           366, 22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,\n",
       "           290,  3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,\n",
       "           345,    11, 15899,    13,   314, 25245,   340,   790,  1110,   526,\n",
       "           198,   198,  3260,  2712,   351,   262,  1097,    11, 21168,   290,\n",
       "         15899,  2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,\n",
       "          1598,  1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,\n",
       "          3772,    13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,\n",
       "          2460,    13, 50256,  7454,  2402,   257,   640,    11,   287,   257,\n",
       "          1263,  8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,\n",
       "          3706,   371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,\n",
       "          1375, 19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,\n",
       "          1110,    11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,\n",
       "           550,  1239,  1775,  1997,   588,   340,   878,    13,   632,   373,\n",
       "         22441,   290,  4692,    11,   290,   673,  2227,   284, 12080,   340,\n",
       "            13,   198,   198,    49, 23536,  3088,   284, 12080,   262, 30284,\n",
       "         12788,    11,   475,   340,   373,   845, 32911,    13,  1375,  3088,\n",
       "           757,   290,   757,    11,   475,   673,  4030,  7463,   866,    13,\n",
       "           371, 23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262,\n",
       "         30284, 12788,   523,   881,    13,  3244,    11,   673,  2497,   257,\n",
       "          1310,  6512,  3706, 15890,    13, 15890,  2497,   326,   371, 23536,\n",
       "           373,  6507,   290,  1965,    11,   366,  5195,   389,   345,  6507,\n",
       "            11,   371, 23536,  1701,   198,   198,    49, 23536,  1297, 15890,\n",
       "           546,   262, 30284, 12788,   290,   703,   673,  3521,   470, 12080,\n",
       "           340,    13, 15890,   531,    11,   366,    40,   423,   281,  2126,\n",
       "             0,  3914,   338,  1064,   617,  1263]),\n",
       " tensor([   13, 15899,  2497,   262, 22441,  1097,   290,   531,    11,   366,\n",
       "         22017,    11, 21168,    11,   534,  1097,   318,   523,  6016,   290,\n",
       "          3424,  2474, 21168, 13541,   290,  8712,    11,   366, 10449,   345,\n",
       "            11, 15899,    13,   314, 25245,   340,   790,  1110,   526,   198,\n",
       "           198,  3260,  2712,   351,   262,  1097,    11, 21168,   290, 15899,\n",
       "          2936, 47124,    13,  1119,  1043,   257,  1402, 16723,   351,  1598,\n",
       "          1660,    13,  1119, 24070,   262,  1660,   290,  2936,   845,  3772,\n",
       "            13,  1119,  2826,  1978,   477,  1110,   290,  2627,  1266,  2460,\n",
       "            13, 50256,  7454,  2402,   257,   640,    11,   287,   257,  1263,\n",
       "          8222,    11,   612,  5615,   257,  9529,   259,   420, 27498,  3706,\n",
       "           371, 23536,    13,   371, 23536,  6151,   284, 12080,    13,  1375,\n",
       "         19952,  7150,    11, 12586,    11,   290, 18639,    13,  1881,  1110,\n",
       "            11,   371, 23536,  1043,   281, 30284, 12788,    13,  1375,   550,\n",
       "          1239,  1775,  1997,   588,   340,   878,    13,   632,   373, 22441,\n",
       "           290,  4692,    11,   290,   673,  2227,   284, 12080,   340,    13,\n",
       "           198,   198,    49, 23536,  3088,   284, 12080,   262, 30284, 12788,\n",
       "            11,   475,   340,   373,   845, 32911,    13,  1375,  3088,   757,\n",
       "           290,   757,    11,   475,   673,  4030,  7463,   866,    13,   371,\n",
       "         23536,   373,  6507,    13,  1375,  2227,   284, 12080,   262, 30284,\n",
       "         12788,   523,   881,    13,  3244,    11,   673,  2497,   257,  1310,\n",
       "          6512,  3706, 15890,    13, 15890,  2497,   326,   371, 23536,   373,\n",
       "          6507,   290,  1965,    11,   366,  5195,   389,   345,  6507,    11,\n",
       "           371, 23536,  1701,   198,   198,    49, 23536,  1297, 15890,   546,\n",
       "           262, 30284, 12788,   290,   703,   673,  3521,   470, 12080,   340,\n",
       "            13, 15890,   531,    11,   366,    40,   423,   281,  2126,     0,\n",
       "          3914,   338,  1064,   617,  1263,  5667]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = Dataset(val_inps, val_targs)\n",
    "val_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757872",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409779a",
   "metadata": {},
   "source": [
    "We need a dataloader with the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646800e",
   "metadata": {},
   "source": [
    "TODO: do `drop_last=True` for training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f8ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\n",
    "dls = DataLoaders(trn_dl, val_dl)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5509,   329,  4964,  ...,    13,   632,   925],\n",
       "         [  547,  1266,  2460,  ..., 22746, 45230,   832],\n",
       "         [ 3772,   290,  6568,  ...,  8212,    13, 50256],\n",
       "         [   11,   366,  5195,  ...,  3088,   284,  1037]]),\n",
       " tensor([[  329,  4964,   625,  ...,   632,   925,   607],\n",
       "         [ 1266,  2460,    13,  ..., 45230,   832,   262],\n",
       "         [  290,  6568,    13,  ...,    13, 50256,  7454],\n",
       "         [  366,  5195,   389,  ...,   284,  1037,   262]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3adb4",
   "metadata": {},
   "source": [
    "We make the model using transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, ctx_len, n_head, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % n_head == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.n_head = n_head\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // n_head\n",
    "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones((ctx_len, ctx_len)), diagonal=1).bool())\n",
    "    \n",
    "    def forward(self, x): \n",
    "        bs, num_tokens, d_in = x.shape\n",
    "        q = self.w_q(x)  # (bs, num_tokens, d_out)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        \n",
    "        q = q.view(bs, num_tokens, self.n_head, self.head_dim)  # (bs, num_tokens, n_head, head_dim)\n",
    "        k = k.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        v = v.view(bs, num_tokens, self.n_head, self.head_dim)\n",
    "        \n",
    "        q = q.transpose(1,2) # (bs, n_head, num_tokens, head_dim)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn_scr = q@k.transpose(2,3) # (bs, n_head, num_tokens, num_tokens)\n",
    "        attn_scr = attn_scr.masked_fill(self.mask[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_wt = torch.softmax(attn_scr / k.shape[-1]**0.5, -1)\n",
    "        \n",
    "        ctx_vec = attn_wt@v  # (bs, n_head, num_tokens, head_dim)\n",
    "        ctx_vec = ctx_vec.transpose(1,2).reshape(bs, num_tokens, -1) # (bs, num_tokens, d_out)\n",
    "        \n",
    "        # concat\n",
    "        return ctx_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0806, -0.0211,  0.1296, -0.0414],\n",
       "          [ 0.2287,  0.1592, -0.0296, -0.1997]],\n",
       " \n",
       "         [[ 0.6081,  0.0087,  0.2989, -0.4998],\n",
       "          [ 0.1026,  0.0422,  0.3105, -0.2781]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([2, 2, 4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_in=3, d_out=4, ctx_len=2, n_head=2)\n",
    "mha(x), mha(x).shape  # Outputs (bs, num_tokens, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2928, -0.1471,  0.0123, -0.2592], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(4)\n",
    "ff = FeedForward(4, 4*4)\n",
    "ff(x), ff(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = MultiHeadAttention(emb_dim, emb_dim, ctx_len, n_head, qkv_bias=qkv_bias)\n",
    "        self.do = nn.Dropout(drop_out)\n",
    "        self.ff = FeedForward(emb_dim, emb_dim*ff_mult)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39725b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3595, -1.0631,  0.5676],\n",
       "          [-0.3159, -2.2222,  0.1778]],\n",
       " \n",
       "         [[ 1.7169, -1.7565,  1.2721],\n",
       "          [-0.3908, -0.2502,  0.3274]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)\n",
    "tb = TransformerBlock(emb_dim=3, ctx_len=2, n_head=1)\n",
    "tb(x), tb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 48,\n",
    "    'ctx_len': seq_len,\n",
    "    'n_head': 1,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bb2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = xb[:3]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ab458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 50257])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())\n",
    "total_params = get_total_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 48]), torch.Size([50257, 48]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_emb.weight.shape, model.final_l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 7,313,137\n",
      "Total size: 27.90 MB\n"
     ]
    }
   ],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
    "\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e879d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 38213, 23676,  9929, 29854,  3414, 22988]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2125798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Moves Poor overt DV announcedpeace\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816caf06",
   "metadata": {},
   "source": [
    "For convenience, we create functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16486221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 86, 562, 929, 616, 288, 707,  70]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_token_ids('wassup my dawg', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e130b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wassup my dawg'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(text_to_token_ids('wassup my dawg', tokenizer), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1658bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves you589bj cartoon regional Islamabad Experimental Cancer straw unin fucked'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836abf4a",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f01991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([5, 256]) torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in trn_dl:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e846d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 12544\n",
      "Validation tokens: 3328\n",
      "All tokens: 15872\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in trn_dl:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dl:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e49f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "#     logits = model(input_batch)\n",
    "#     loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "#     total_loss = 0.\n",
    "#     if len(data_loader) == 0:\n",
    "#         return float(\"nan\")\n",
    "#     elif num_batches is None:\n",
    "#         num_batches = len(data_loader)\n",
    "#     else:\n",
    "#         # Reduce the number of batches to match the total number of batches in the data loader\n",
    "#         # if num_batches exceeds the number of batches in the data loader\n",
    "#         num_batches = min(num_batches, len(data_loader))\n",
    "#     for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "#         if i < num_batches:\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             total_loss += loss.item()\n",
    "#         else:\n",
    "#             break\n",
    "#     return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823ed63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.978310144864595\n",
      "Validation loss: 10.996747493743896\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Note:\n",
    "# # Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# # which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# # However, the resulting loss values may be slightly different.\n",
    "\n",
    "# #if torch.cuda.is_available():\n",
    "# #    device = torch.device(\"cuda\")\n",
    "# #elif torch.backends.mps.is_available():\n",
    "# #    device = torch.device(\"mps\")\n",
    "# #else:\n",
    "# #    device = torch.device(\"cpu\")\n",
    "# #\n",
    "# # print(f\"Using {device} device.\")\n",
    "\n",
    "\n",
    "# model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "# torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "# with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "#     train_loss = calc_loss_loader(trn_dl, model, device)\n",
    "#     val_loss = calc_loss_loader(val_dl, model, device)\n",
    "\n",
    "# print(\"Training loss:\", train_loss)\n",
    "# print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 4,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 96*2,\n",
    "    'ctx_len': seq_len,\n",
    "    'n_head': 4,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662065d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could davidjl alliesopter mishand Free asteroidsBFatmeal lifes Definitive\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        super().__init__(*ms, **metrics)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds.flatten(0, 1)), y.flatten())\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/133 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot params: 30,627,601; MFLOPS: 30.6\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|MFLOPS|\n",
       "|--|--|--|--|--|\n",
       "|Embedding|(4, 256)|(4, 256, 192)|9,649,344|9.6|\n",
       "|Embedding|(256,)|(256, 192)|9,649,344|9.6|\n",
       "|Dropout|(4, 256, 192)|(4, 256, 192)|0|0.0|\n",
       "|Sequential|(4, 256, 192)|(4, 256, 192)|1,628,928|1.6|\n",
       "|LayerNorm|(4, 256, 192)|(4, 256, 192)|384|0.0|\n",
       "|Linear|(4, 256, 192)|(4, 256, 50257)|9,699,601|9.7|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfba866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='77' class='' max='133' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      57.89% [77/133 00:32&lt;00:23 15.299]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(gamma=1.1, max_mult=2, start_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639085e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.097</td>\n",
       "      <td>7.704</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.080</td>\n",
       "      <td>6.733</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.131</td>\n",
       "      <td>6.372</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.150</td>\n",
       "      <td>6.080</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.153</td>\n",
       "      <td>5.940</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.144</td>\n",
       "      <td>5.835</td>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164</td>\n",
       "      <td>5.675</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.167</td>\n",
       "      <td>5.576</td>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.171</td>\n",
       "      <td>5.485</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.178</td>\n",
       "      <td>5.436</td>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.177</td>\n",
       "      <td>5.330</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.132</td>\n",
       "      <td>5.589</td>\n",
       "      <td>5</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.181</td>\n",
       "      <td>5.210</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164</td>\n",
       "      <td>5.306</td>\n",
       "      <td>6</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.186</td>\n",
       "      <td>5.102</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.185</td>\n",
       "      <td>5.104</td>\n",
       "      <td>7</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.192</td>\n",
       "      <td>5.000</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.192</td>\n",
       "      <td>5.055</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.195</td>\n",
       "      <td>4.912</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.194</td>\n",
       "      <td>4.972</td>\n",
       "      <td>9</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could a time there was a little girl named Lily.\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"])\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. \n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
