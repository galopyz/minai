{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bbf607",
   "metadata": {},
   "source": [
    "Using sdpa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8bde8",
   "metadata": {},
   "source": [
    "# Tiny Stories Hackathon\n",
    "> From Cluster of stars study group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371da66",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## TinyStories Hackathon Rules\n",
    "This hackathon is intended to be a fun competition to give ourselves practice pretraining LLMs on consumer hardware. We will follow the [TinyStories paper](<https://arxiv.org/abs/2305.07759>) and train small language models on small datasets and hardware.\n",
    "\n",
    "The hackathon will end on April 7th, [AOE](<https://en.wikipedia.org/wiki/AoE>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c70b6a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Datasets\n",
    "1. [**TinyStories:**](<https://huggingface.co/datasets/roneneldan/TinyStories>)\n",
    "   Note that the TinyStories dataset is split into two versions both in the HF dataset:\n",
    "     - GPT-3.5 generated TinyStories\n",
    "    - GPT-4 generated TinyStories\n",
    "   The tar file appears to have the cleanest versions with the least number of duplicates.\n",
    "2. **[Simple Wikipedia](<https://huggingface.co/datasets/lsb/simplewiki2023>)** (optional)\n",
    "   This dataset can be used to give your model more world knowledge than from just the TinyStories dataset. But be careful that \n",
    "it doesn't cause your model to use words which a typical 3 to 4-year-olds doesn't understand. It may need to be cleaned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1528f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluation\n",
    "Models will be evaluated by LLM-as-a-judge following the methodology outlined in the TinyStories paper. More details including how to submit your model's outputs early next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a15400",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model Size Limits\n",
    "Participants will be slotted into one of the following categories based on their hardware:\n",
    "- **Small**: Up to 30M parameters. Low-to-mid range laptop GPUs and Apple Silicon.\n",
    "- **Medium**: Up to 60M parameters. Mid-range GPUs (including high-end laptop GPUs and Apple Silicon)\n",
    "- **Large**: Up to 120M parameters. High-end GPUs and multi-GPU systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1a81",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tokenizers\n",
    "While you must train your model from scratch, you are welcome to use any pre-trained tokenizer or train your own tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dbdaa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Model Architecture\n",
    "You are welcome to use any model architecture you want provided you stay within the parameter budget of your hardware by following the parameter counting rules below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadc72b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parameter Counting\n",
    "The Parameter budget is the number of unique floating-point weights receiving gradient updates:\n",
    "- Unique Weights: Count each distinct floating-point weight stored in the model once.\n",
    "- Reuse Multiplier: For each weight, multiply by the number of distinct times it contributes to forward computation (e.g., due to layer-sharing, layer reuse, or non-standard head-sharing). Weight-tied embedding and decoder weights are the exception and are only counted once. MQA/GQA doesn't count as head-sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec912d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Teams\n",
    "Teams are limited to a maximum of 2 members and must be formed and declared within the first week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b91a3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training Frameworks\n",
    "You might want to take a look at the following libraries and frameworks and adopt one for pretraining:\n",
    "- [Composer](<https://docs.mosaicml.com/projects/composer/en/stable/index.html>) and optionally [LLM Foundry](<https://github.com/mosaicml/llm-foundry>)\n",
    "- [PyTorch Lightning](<https://lightning.ai/docs/pytorch/stable/>) and optionally [LitGPT](<https://github.com/Lightning-AI/litgpt>)\n",
    "- Hugging Face [Trainer](<https://huggingface.co/docs/transformers/en/main_classes/trainer>), [Accelerate](<https://huggingface.co/docs/accelerate/en/index>), and optionally [Axolotl](<https://axolotl-ai-cloud.github.io/axolotl/>) (a wrapper on top of HF)\n",
    "- [fastai](<https://docs.fast.ai/>) with either [fastxtend](<https://fastxtend.benjaminwarner.dev/text.huggingface.html>)/[blurr](<https://ohmeow.github.io/blurr/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82c861",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc14c85",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9e940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, BoolTensor\n",
    "\n",
    "from minai import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4bc327",
   "metadata": {},
   "source": [
    "Grab tiny stories data from hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e7cd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2119719\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('roneneldan/TinyStories')\n",
    "trn = ds['train']\n",
    "val = ds['validation']\n",
    "trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967927c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 21990\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38f566a",
   "metadata": {},
   "source": [
    "For now, we can just use gpt2 tokenizer to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b102ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "txt = trn[0]['text']\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2724526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(txt)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5cf1135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(txt)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d8567f",
   "metadata": {},
   "source": [
    "We can make functions for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ca6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_toks(txt, toker): return toker.encode(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea02d509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = txt_to_toks(txt, tokenizer)[:10]\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a784c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_to_toks(txt, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a08231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toks_to_txt(toks, toker): return toker.decode(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c42f8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_txt(toks, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05946e90",
   "metadata": {},
   "source": [
    "Concatenate all texts with `separator=\"<|endoftext|>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57d73f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.<|endoftext|>Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that wer'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separator=\"<|endoftext|>\"\n",
    "ctx_len = 256\n",
    "num_txts = 100\n",
    "\n",
    "trn_txts = separator.join([trn[i]['text'] for i in range(num_txts)])\n",
    "trn_txts[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d68efb",
   "metadata": {},
   "source": [
    "We create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a27966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from llm from scratch\n",
    "class TinyDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, ctx_len):\n",
    "        self.inp = []\n",
    "        self.targ = []\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        for i in range(0, len(token_ids) - ctx_len, ctx_len):\n",
    "            inp_chunk = token_ids[i:i + ctx_len]\n",
    "            targ_chunk = token_ids[i + 1: i + ctx_len + 1]\n",
    "            self.inp.append(torch.tensor(inp_chunk))\n",
    "            self.targ.append(torch.tensor(targ_chunk))\n",
    "\n",
    "    def __len__(self): return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx): return self.inp[idx], self.targ[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57db7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n",
       "         17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n",
       "           284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n",
       "          2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n",
       "           673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n",
       "           198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n",
       "           366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n",
       "          2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n",
       "          1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n",
       "           356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n",
       "           198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n",
       "         19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n",
       "           407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n",
       "          1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n",
       "          1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n",
       "          1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n",
       "          1978,    13, 50256,  7454,  2402,   257,   640,    11,   612,   373,\n",
       "           257,  1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,\n",
       "           284,   467,  3049,   290,   711,   287,   262,  4252,    13,  1355,\n",
       "           538,   373,   257,  5448,  1097,   780,   339,  1464,   550,   922,\n",
       "          5252,    13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,\n",
       "            13,   198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,\n",
       "           287,   262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,\n",
       "           383,  5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,\n",
       "           538,  8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,\n",
       "           351,   606,    13,  1355,   538, 10357]),\n",
       " tensor([ 1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257, 17598,\n",
       "           287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,   284,\n",
       "           711,   351,   340,   780,   340,   373,  7786,    13, 20037,  2227,\n",
       "           284,  2648,   262, 17598,   351,   607,  1995,    11,   523,   673,\n",
       "           714, 34249,   257,  4936,   319,   607, 10147,    13,   198,   198,\n",
       "            43,   813,  1816,   284,   607,  1995,   290,   531,    11,   366,\n",
       "         29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,  2648,\n",
       "           340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,  1995,\n",
       "         13541,   290,   531,    11,   366,  5297,    11, 20037,    11,   356,\n",
       "           460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,   198,\n",
       "           198, 41631,    11,   484,  4888,   262, 17598,   290,   384, 19103,\n",
       "           262,  4936,   319, 20037,   338, 10147,    13,   632,   373,   407,\n",
       "          2408,   329,   606,   780,   484,   547,  7373,   290,  5742,  1123,\n",
       "           584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,  1995,\n",
       "           329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,  1119,\n",
       "          1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,  1978,\n",
       "            13, 50256,  7454,  2402,   257,   640,    11,   612,   373,   257,\n",
       "          1310,  1097,  3706,  1355,   538,    13,  1355,   538,  6151,   284,\n",
       "           467,  3049,   290,   711,   287,   262,  4252,    13,  1355,   538,\n",
       "           373,   257,  5448,  1097,   780,   339,  1464,   550,   922,  5252,\n",
       "            13,  4599,  5252,   925,  1355,   538,  3772,   290,  1913,    13,\n",
       "           198,   198,  3198,  1110,    11,  1355,   538,   373,  5059,   287,\n",
       "           262,  3952,   618,   339,  2497,   257,  1263,  5509,    13,   383,\n",
       "          5509,   550,   867,  5667,   326,   547,  7463,    13,  1355,   538,\n",
       "          8288,   703,   262,  5667,  2121,   290,  2227,   284,   711,   351,\n",
       "           606,    13,  1355,   538, 10357,   739]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds = TinyDataset(trn_txts, tokenizer, ctx_len)\n",
    "trn_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e8b578c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.<|endoftext|>Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_txt(trn_ds[0][0].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0691e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.<|endoftext|>Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_txt(trn_ds[0][1].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3841d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation set\n",
    "val_txts = separator.join([val[i]['text'] for i in range(num_txts // 1)])\n",
    "val_ds = TinyDataset(val_txts, tokenizer, ctx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85ce2bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\\n\\nAfter playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.<|endoftext|>Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\\n\\nRoxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\\n\\nRoxy told Billy about the icy hill and how she couldn\\'t climb it. Billy said, \"I have an idea! Let\\'s find some big'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks_to_txt(val_ds[0][0].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08757872",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409779a",
   "metadata": {},
   "source": [
    "We need a dataloader with the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63ca5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646800e",
   "metadata": {},
   "source": [
    "TODO: do `drop_last=True` for training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4f8ee24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([4, 256]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 4\n",
    "\n",
    "trn_dl, val_dl = get_dls(trn_ds, val_ds, bs)\n",
    "dls = DataLoaders(trn_dl, val_dl)\n",
    "xb,yb = next(iter(trn_dl))\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c656d492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  550,  3111,   845,  ..., 16453,   477,   546],\n",
       "         [  262,  5417,    13,  ...,    11,   612,  5615],\n",
       "         [  640,    11,   612,  ...,   632,   373,   257],\n",
       "         [ 3198,  1110,    11,  ...,  1355,   538, 10357]]),\n",
       " tensor([[ 3111,   845,  1327,  ...,   477,   546,   852],\n",
       "         [ 5417,    13,   383,  ...,   612,  5615,   257],\n",
       "         [   11,   612,   373,  ...,   373,   257,  5897],\n",
       "         [ 1110,    11,   257,  ...,   538, 10357,   739]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:5], yb[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aec692",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3adb4",
   "metadata": {},
   "source": [
    "We make the model using transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b4a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad50e3",
   "metadata": {},
   "source": [
    "### SDPA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55093be6",
   "metadata": {},
   "source": [
    "Here's the `MultiHeadAttention` with Causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7811bd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde98356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDPACausalAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention block implementing multi-head causal (masked) attention using\n",
    "    PyTorch's scaled_dot_product_attention (SDPA).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the causal attention block with SDPA implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_dim: Dimension of the input and output features\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Output dropout probability (0.0 means no dropout)\n",
    "\n",
    "        Note:\n",
    "            - Make sure to check that hidden_dim is divisible by num_heads\n",
    "            - You'll need to create linear (projection) layers for query, key, and value\n",
    "            - Don't forget the output linear (projection) layer\n",
    "            - Create an output dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if hidden_dim % num_heads != 0: raise Exception(\"hidden_dim not divisible by num_heads\")\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq, self.Wk, self.Wv = nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim), nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.Wo = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: BoolTensor | None = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, hidden_dim].\n",
    "            mask: Optional boolean mask of shape [batch_size, sequence_length]\n",
    "                  where True indicates attended tokens and False masked positions\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape [batch_size, seq_len, hidden_dim] after attention.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        q,k,v = self.Wq(x), self.Wk(x), self.Wv(x) # [batch_size, seq_len, d_out]\n",
    "        attn_mask = mask.view(batch_size, 1, 1, seq_len) if mask is not None else None\n",
    "\n",
    "        sdpa_ctx = torch.nn.functional.scaled_dot_product_attention(\n",
    "            q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2), \n",
    "            k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2), \n",
    "            v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2), \n",
    "            attn_mask=attn_mask, dropout_p=0.0, is_causal=True, scale=None)\n",
    "        sdpa_ctx = sdpa_ctx.transpose(1,2).view(batch_size, seq_len, -1)\n",
    "        return self.dropout(self.Wo(sdpa_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85290c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.5674,  0.6473,  0.0982],\n",
       "          [-0.6393,  0.6418,  0.2148]],\n",
       " \n",
       "         [[-0.5239,  0.4952,  0.3595],\n",
       "          [-0.5557,  0.6072,  0.0868]]], grad_fn=<ViewBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spdaa = SDPACausalAttentionBlock(hidden_dim=3, num_heads=1)\n",
    "spdaa(x), spdaa(x).shape  # Outputs (bs, num_tokens, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2119c",
   "metadata": {},
   "source": [
    "### FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbab0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.act = act\n",
    "        self.l2 = nn.Linear(hidden_dim, in_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "333b6ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.2928, -0.1471,  0.0123, -0.2592], grad_fn=<ViewBackward0>),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(4)\n",
    "ff = FeedForward(4, 4*4)\n",
    "ff(x), ff(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280c0f",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbd61c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, emb_dim, ctx_len, n_head, drop_out=0, ff_mult=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mha = SDPACausalAttentionBlock(hidden_dim=emb_dim, num_heads=n_head, dropout=drop_out)\n",
    "        self.do = nn.Dropout(drop_out)\n",
    "        self.ff = FeedForward(emb_dim, emb_dim*ff_mult)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip1 = x\n",
    "        x = self.ln1(x)\n",
    "        x = self.mha(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip1\n",
    "        \n",
    "        skip2 = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.do(x)\n",
    "        x = x + skip2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39725b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3812,  0.6661,  1.0007],\n",
       "          [-0.5559, -0.5410,  0.3982]],\n",
       " \n",
       "         [[ 1.6740, -0.2601,  1.2948],\n",
       "          [-0.5664,  1.2499,  1.1697]]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2, 2, 3]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn((2, 2, 3)) # (bs, ctx_len, d_in)\n",
    "tb = TransformerBlock(emb_dim=3, ctx_len=2, n_head=1)\n",
    "tb(x), tb(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d7ae",
   "metadata": {},
   "source": [
    "### GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "825cd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 1,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 48,\n",
    "    'ctx_len': ctx_len,\n",
    "    'n_head': 1,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3cf425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['vocab_sz'], cfg['emb_dim'])\n",
    "        self.do = nn.Dropout(cfg['drop_out'])\n",
    "        self.tb = nn.Sequential(\n",
    "            *[TransformerBlock(cfg['emb_dim'], cfg['ctx_len'], cfg['n_head'], cfg['drop_out_tb'],\n",
    "                              cfg['ff_mult'], cfg['qkv_bias']) for _ in range(cfg['n_tb'])])\n",
    "        self.final_ln = nn.LayerNorm(cfg['emb_dim'])\n",
    "        self.final_l  = nn.Linear(cfg['emb_dim'], cfg['vocab_sz'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs, seq_len = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(seq_len, device=x.device))\n",
    "        x = self.do(tok + pos)\n",
    "        x = self.tb(x)\n",
    "        x = self.final_ln(x)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4bb2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = xb[:3]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c2ab458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 50257])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "logits = model(batch)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f2225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_params(model): return sum(p.numel() for p in model.parameters())\n",
    "total_params = get_total_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38c5a418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50257, 48]), torch.Size([50257, 48]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.token_emb.weight.shape, model.final_l.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f5c38f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 7,315,633\n",
      "Total size: 27.91 MB\n"
     ]
    }
   ],
   "source": [
    "def get_total_memory(model):\n",
    "    total_params = get_total_params(model)\n",
    "    total_size_bytes = total_params * 4   # Assuming fp32\n",
    "    # Convert to megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Total size: {total_size_mb:.2f} MB\")\n",
    "\n",
    "get_total_memory(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f022896",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bc54096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]  # Crop current context if it exceeds the supported context size\n",
    "        with torch.no_grad(): logits = model(idx_cond)         # (bs, n_tokens, vocab_sz)\n",
    "        logits = logits[:, -1, :]                              # (bs, vocab_sz)\n",
    "        probas = torch.softmax(logits, dim=-1)                 # (bs, vocab_sz)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (bs, 1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                # (bs, n_tokens+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cfd542d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e879d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 17129, 20210, 35203, 31787, 27943, 32965]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # disable dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2125798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Pure cryptocurrency Booster Caucus mundbugs\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816caf06",
   "metadata": {},
   "source": [
    "For convenience, we create functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "511281b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16486221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 86, 562, 929, 616, 288, 707,  70]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_token_ids('wassup my dawg', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "880ff652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7e130b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wassup my dawg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(text_to_token_ids('wassup my dawg', tokenizer), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1658bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every effort moves youlangPlease chron Caucus mundbugsなreprene Frejump'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836abf4a",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f01991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([4, 256]) torch.Size([4, 256])\n",
      "torch.Size([1, 256]) torch.Size([1, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([8, 256]) torch.Size([8, 256])\n",
      "torch.Size([3, 256]) torch.Size([3, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in trn_dl:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_dl:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e846d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 18688\n",
      "Validation tokens: 17152\n",
      "All tokens: 35840\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in trn_dl:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_dl:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6efa9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4747a7b",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90bca503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import  MulticlassAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69ba3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, targ): return F.cross_entropy(pred.flatten(0, 1), targ.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b83922cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'n_tb': 4,    # num transformer blocks\n",
    "    'vocab_sz': 50257,\n",
    "    'emb_dim': 96*2,\n",
    "    'ctx_len': ctx_len,\n",
    "    'n_head': 4,\n",
    "    'drop_out': 0,\n",
    "    'drop_out_tb': 0,  # dropout within transformer blocks\n",
    "    'ff_mult': 4,\n",
    "    'qkv_bias': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2716b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662065d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "start_context = \"Once upon a time, there lived a bunny in a field. Her name was Lucy. Lucy loved to have feasts and parties with her bunny friends. One day, when Lucy was about to leave for a feast at a friend's house, she realized she's starting to feel sick. She was so weak she could\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer).to(def_device),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68d44a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetricsCB(MetricsCB):\n",
    "    def __init__(self, *ms, **metrics):\n",
    "        super().__init__(*ms, **metrics)\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): m.update(to_cpu(learn.preds.flatten(0, 1)), y.flatten())\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd56e4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/19 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot params: 30,778,129; MFLOPS: 30.8\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "|Module|Input|Output|Num params|MFLOPS|\n",
       "|--|--|--|--|--|\n",
       "|Embedding|(4, 256)|(4, 256, 192)|9,649,344|9.6|\n",
       "|Embedding|(256,)|(256, 192)|9,649,344|9.6|\n",
       "|Dropout|(4, 256, 192)|(4, 256, 192)|0|0.0|\n",
       "|Sequential|(4, 256, 192)|(4, 256, 192)|1,779,456|1.8|\n",
       "|LayerNorm|(4, 256, 192)|(4, 256, 192)|384|0.0|\n",
       "|Linear|(4, 256, 192)|(4, 256, 50257)|9,699,601|9.7|\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3dfba866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='3' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      30.00% [3/10 00:07&lt;00:17]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.044</td>\n",
       "      <td>11.456</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.079</td>\n",
       "      <td>10.543</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.087</td>\n",
       "      <td>9.025</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.26% [1/19 00:00&lt;00:02 9.980]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find(gamma=1.1, max_mult=2, start_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "639085e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>epoch</th>\n",
       "      <th>train</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.043</td>\n",
       "      <td>10.112</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.077</td>\n",
       "      <td>8.755</td>\n",
       "      <td>0</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.084</td>\n",
       "      <td>8.319</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.081</td>\n",
       "      <td>7.922</td>\n",
       "      <td>1</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.093</td>\n",
       "      <td>7.700</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.091</td>\n",
       "      <td>7.408</td>\n",
       "      <td>2</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.102</td>\n",
       "      <td>7.279</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.100</td>\n",
       "      <td>7.068</td>\n",
       "      <td>3</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.109</td>\n",
       "      <td>6.999</td>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.095</td>\n",
       "      <td>6.849</td>\n",
       "      <td>4</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.113</td>\n",
       "      <td>6.789</td>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.089</td>\n",
       "      <td>6.684</td>\n",
       "      <td>5</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.122</td>\n",
       "      <td>6.612</td>\n",
       "      <td>6</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.103</td>\n",
       "      <td>6.543</td>\n",
       "      <td>6</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.125</td>\n",
       "      <td>6.459</td>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.117</td>\n",
       "      <td>6.407</td>\n",
       "      <td>7</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.128</td>\n",
       "      <td>6.323</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.136</td>\n",
       "      <td>6.285</td>\n",
       "      <td>8</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.138</td>\n",
       "      <td>6.202</td>\n",
       "      <td>9</td>\n",
       "      <td>train</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.128</td>\n",
       "      <td>6.190</td>\n",
       "      <td>9</td>\n",
       "      <td>eval</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = GPTModel(cfg)\n",
    "cbs = [LLMMetricsCB(accuracy=MulticlassAccuracy()), ProgressCB(), TrainCB(), DeviceCB()]\n",
    "learn = Learner(model, dls, loss_func=loss_fn, cbs=cbs)\n",
    "learn.fit(10, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e0cf04f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m token_ids = \u001b[43mgenerate_text_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_to_token_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mctx_len\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOutput text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, token_ids_to_text(token_ids, tokenizer))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mgenerate_text_simple\u001b[39m\u001b[34m(model, idx, max_new_tokens, context_size)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[32m      4\u001b[39m     idx_cond = idx[:, -context_size:]  \u001b[38;5;66;03m# Crop current context if it exceeds the supported context size\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_cond\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# (bs, n_tokens, vocab_sz)\u001b[39;00m\n\u001b[32m      6\u001b[39m     logits = logits[:, -\u001b[32m1\u001b[39m, :]                              \u001b[38;5;66;03m# (bs, vocab_sz)\u001b[39;00m\n\u001b[32m      7\u001b[39m     probas = torch.softmax(logits, dim=-\u001b[32m1\u001b[39m)                 \u001b[38;5;66;03m# (bs, vocab_sz)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     14\u001b[39m     bs, seq_len = x.shape\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     tok = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     pos = \u001b[38;5;28mself\u001b[39m.pos_emb(torch.arange(seq_len, device=x.device))\n\u001b[32m     17\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.do(tok + pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/fromscratch/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=cfg[\"ctx_len\"])\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027adbf",
   "metadata": {},
   "source": [
    "Hyperparameters: Learning rate, optimizer: Gradient clipping, batch size: 4k\n",
    "\n",
    "Mixed precision -> weight decay needed. (bfloat16)\n",
    "\n",
    "Distributed data parallel: Split data into 2 and use graident accumulation\n",
    "\n",
    "Fully Sharded data parallel: shard of data into GPUs as layer goes.\n",
    "\n",
    "CPU offload\n",
    "\n",
    "DataLoader: Use for loop.\n",
    "\n",
    "!!!!! Look at the data. !!!!!\n",
    "\n",
    "Eval: next token accuracy, loss\n",
    "\n",
    "Try GLU instead of ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eff7e9",
   "metadata": {},
   "source": [
    "Tips: \n",
    "\n",
    "1. Try simple model.\n",
    "2. Weight Tying.\n",
    "3. Hyperparameter sweep\n",
    "4. minbpe\n",
    "\n",
    "\n",
    "Get sequencing packing to work -> iterate faster\n",
    "flash attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019726f",
   "metadata": {},
   "source": [
    "Use triton cross entropy loss or compile nn.crosstropyloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27a80a",
   "metadata": {},
   "source": [
    "Add view(-1,...) before flash attention and remove view(-1,...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
